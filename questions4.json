[
  {
    "type": "mc",
    "question": "מה הבעיה העיקרית בניסיון לאפיין רק את ה-w במודל StyleGAN כדי לעבור מדומיין כלבים לדומיין חתולים?",
    "options": [
      "ה-w משנה רק את הטקסטורה אך לא את הצבעים",
      "ה-מודל כלל לא למד להפיק חתולים ולכן כל וקטור w יישאר בדומיין הכלבים",
      "ה-w גורם לקריסת Mode Collapse מיידית",
      "ה-w נמצא במרחב Z ולא במרחב W+"
    ],
    "correctAnswerIndex": 1,
    "explanation": "ללא דוגמאות חתולים המודל אינו מכיר את הדומיין, ולכן הזזת w בלבד לא תייצר אף חתול."
  },
  {
    "type": "mc",
    "question": "מדוע אופטימיזציה ישירה של משקולות הגנרטור בלבד לא פתרה את בעיית הגיוון בדומיין החדש?",
    "options": [
      "המשקולות נהפכו לאורתוגונליות מדי",
      "כל וקטור w מופה לאותה תמונה כמעט זהה, ולכן אין שונות",
      "ה-Discriminator סירב להתעדכן במקביל",
      "CLIP דורש וקטור טקסט נוסף בכל צעד"
    ],
    "correctAnswerIndex": 1,
    "explanation": "עדכון המשקולות בלי פיקוח מתאים גרם למודל לקרוס לנקודה אחת ולהפיק חתול יחיד."
  },
  {
    "type": "mc",
    "question": "ב-StyleGAN-NADA משתמשים בשני גנרטורים (G_frozen ו-G_train) כדי…",
    "options": [
      "להפחית צריכת זיכרון בזמן אימון",
      "למדוד כיוון שינוי סמנטי מווקטור לטנט יחיד",
      "להשוות בין תמונת מקור ופלט מאומן במרחב CLIP לצורך כיוון שינוי רצוי",
      "לאפשר אימון על דאטה ר噗שיון בלבד"
    ],
    "correctAnswerIndex": 2,
    "explanation": "ה-frozen מייצר את הדומיין המקורי, המאומן מנסה להגיע לדומיין חדש; כיוון ההבדל מותאם לכיוון הטקסט ב-CLIP."
  },
  {
    "type": "mc",
    "question": "איזה Loss מרכזי מנחה את StyleGAN-NADA להשתנות בכיוון הטקסטואלי הרצוי?",
    "options": [
      "Pixel L2 Loss",
      "CLIP Direction Loss",
      "Identity Loss (ArcFace)",
      "Perceptual LPIPS Loss"
    ],
    "correctAnswerIndex": 1,
    "explanation": "Loss מבוסס CLIP דואג שההפרש בין התמונות יתאים להפרש בין התיאורים בטקסט."
  },
  {
    "type": "mc",
    "question": "Latent Vector Optimization (LVO) מתבצע לרוב במרחב W+ כי…",
    "options": [
      "W+ גוזר משותף לכל השכבות ומפחית פרמטרים",
      "מאפשר 18 וקטורי סטייל נפרדים לשכבות, וכך שחזור מדויק וגמיש יותר",
      "W+ נמנע מכל regularization ולכן מהיר יותר",
      "ב-W אסור להשתמש ב-Perceptual Loss"
    ],
    "correctAnswerIndex": 1,
    "explanation": "הקצאת וקטור לכל שכבה מעניקה חופש תאום מקומי ומשפרת איכות שחזור."
  },
  {
    "type": "mc",
    "question": "Encoder-Based Inversion עדיפה על LVO בעיקר משום ש-…",
    "options": [
      "אינה משתמשת ב-StyleGAN כלל",
      "מספקת קוד לטנט מידי וללא אופטימיזציה איטרטיבית",
      "דורשת רק Loss מסוג L2",
      "מבטלת צורך ב-Regularization Loss"
    ],
    "correctAnswerIndex": 1,
    "explanation": "Encoder מחזיר w+ ישירות בריצת inference אחת, בניגוד ל-LVO שלוקח עשרות איטרציות."
  },
  {
    "type": "mc",
    "question": "ב-pSp Encoder משתמשים בזוגות (Edge Map, Image) כדי…",
    "options": [
      "להפוך Edge Map ישירות לתמונה ריאליסטית דרך StyleGAN",
      "ללמוד קוונטיזציה דיסקרטית ב-codebook",
      "לאמן Discriminator נוסף למרחב ה-w",
      "לשפר CLIP Loss עבור כיווני טקסט"
    ],
    "correctAnswerIndex": 0,
    "explanation": "ה-Encoder ממפה קלט מבני (קווים, סגמנטציה) לווקטורי W+ שהגנרטור מתרגם לתמונה ברורה."
  },
  {
    "type": "mc",
    "question": "ב-Designing an Encoder for Manipulation מוסיפים Discriminator על מרחב W כדי…",
    "options": [
      "לאכוף שהפלט של ה-Encoder קרוב להתפלגות W ולפיכך עריך",
      "לבדוק אם הקלט שייך לדומיין הכלבים",
      "למנוע Mode Collapse בין שכבות W+",
      "להגביר חדות בשחזור"
    ],
    "correctAnswerIndex": 0,
    "explanation": "Discriminator מבחין בין וקטור W אמיתי לזה שמגיע מן ה-Encoder וכך מאלץ קרבה ל-W."
  },
  {
    "type": "mc",
    "question": "Pivotal Tuning (PTI) מעדכן…",
    "options": [
      "רק את הקוד w אחרי inversion",
      "רק את משקולות הגנרטור, תוך שמירת w קבוע",
      "גם w וגם Discriminator",
      "רק את HyperNetwork קטן"
    ],
    "correctAnswerIndex": 1,
    "explanation": "PTI משפר שחזור בכך שמכוונן מקומית את הגנרטור סביב w בודד מבלי לשנותו."
  },
  {
    "type": "mc",
    "question": "ב-HyperStyle, HyperNetwork מפיקה Δ (תוספת) למשקולות הגנרטור כדי…",
    "options": [
      "להבטיח Identity Loss אפסי",
      "לתקן את השחזור בזמן ריצה יחיד בלי Fine-Tuning ארוך",
      "להקטין את גודל הקובץ של הגנרטור",
      "לבנות codebook דיסקרטי חדש"
    ],
    "correctAnswerIndex": 1,
    "explanation": "ה-Δ מחוברת למשקולות הקבועות ומחזירה שחזור משופר ללא אופטימיזציה איטרטיבית."
  },
  {
    "type": "mc",
    "question": "ב-PixelCNN משתמשים ב-Masked Convolution כדי…",
    "options": [
      "לצמצם מספר פרמטרים",
      "למנוע דליפת מידע מפיקסלים עתידיים ולשמור אוטורגרסיביות",
      "לשפר חדות באמצעות GAN",
      "לאפשר Attention לטווח ארוך"
    ],
    "correctAnswerIndex": 1,
    "explanation": "המסכה מאפסת פילטרים שמביטים ימינה/מטה ובכך מבטיחה תלות רק בעבר."
  },
  {
    "type": "mc",
    "question": "חיסרון עיקרי של PixelCNN הוא…",
    "options": [
      "אין יכולת לייצר תמונות מותנות",
      "דגימה איטית ואיכות מוגבלת לרזולוציות נמוכות",
      "חוסר במנגנון Self-Attention",
      "שימוש ברעש לבן בלבד"
    ],
    "correctAnswerIndex": 1,
    "explanation": "ייצור פיקסל-אחר-פיקסל איטי, והתמונות ברזולוציה קטנה נראות מטושטשות."
  },
  {
    "type": "mc",
    "question": "ב-VQ-VAE, ה-Commitment Loss נועד…",
    "options": [
      "לחייב את ה-Encoder להישאר קרוב ל-codebook ולהקטין ריצוד",
      "לשמור על חדות השחזור",
      "לאמן Discriminator נוסף ל-GAN",
      "לכייל את ה-Transformer"
    ],
    "correctAnswerIndex": 0,
    "explanation": "ה-Encoder 'מתחייב' לבחור קודbook קיים ולא להתרחק למקומות שה-Decoder לא מכיר."
  },
  {
    "type": "mc",
    "question": "מה החידוש המרכזי ששיפר VQ-GAN ביחס ל-VQ-VAE?",
    "options": [
      "החלפת Decoder בקונבולוציה ממוסכת",
      "הוספת GAN Loss ו-Transformer על הקודים לשיפור חדות וקשרים גלובליים",
      "הסרת ה-codebook ובחירה רציפה במקום דיסקרטית",
      "עבודה ישירה ברמת פיקסלים ללא קוד דיסקרטי"
    ],
    "correctAnswerIndex": 1,
    "explanation": "GAN Loss מחדד תמונה, Transformer לומד תלות רחוקה בקודים ומגביר קוהרנטיות."
  },
  {
    "type": "mc",
    "question": "Self-Attention בשכבת Transformer מחשב מפה המבוססת על…",
    "options": [
      "מכפלת Query-Key ו-Softmax",
      "L2 pixel distance",
      "Conv 3×3 רקורסיבי",
      "Random Fourier features"
    ],
    "correctAnswerIndex": 0,
    "explanation": "Query×Key יוצר משקלים, Softmax מנרמל ומשמש חלוקת קשב."
  },
  {
    "type": "mc",
    "question": "מדוע Transformer דורש חישובים בריבוע למספר הקודים?",
    "options": [
      "כל קוד משודר פעמיים ל-Decoder",
      "יש צורך להשוות כל Query לכל Key ברצף",
      "Gradient Clipping מתבצע פעמיים",
      "Because Softmax הוא O(n-log-n)"
    ],
    "correctAnswerIndex": 1,
    "explanation": "Self-Attention מחייב חיבורי Query–Key עבור כל זוג ולכן O(n²)."
  },
  {
    "type": "mc",
    "question": "בקוד היררכי של VQ-GAN, שכבת top-level מקודדת…",
    "options": [
      "פרטים טקסטורליים",
      "מבנה גלובלי גס של התמונה",
      "רעש סטוכסטי בלבד",
      "וקטור טקסט CLIP"
    ],
    "correctAnswerIndex": 1,
    "explanation": "top מייצג פריסה כללית; bottom מוסיף חדות ותדר גבוה."
  },
  {
    "type": "open",
    "question": "כיצד CLIP משמש גשר בין טקסט לתמונה בשיטת StyleGAN-NADA?",
    "correctAnswerText": "CLIP ממפה גם תמונות וגם טקסט לאותו מרחב אמבדינג. בנאדה מודדים את וקטור השינוי בין תמונת המקור (מה-G_frozen) לתמונת ה-G_train במרחב זה, ומשווים אותו לכיוון בין הטקסט המקורי לטקסט היעד. המטרה היא ליישר את שני הכיוונים, כך שהשינוי בתמונה ישקף את המשמעות הסמנטית של שינוי הטקסט.",
    "explanation": "הקבלה של כיוון תמונה-תמונה לכיוון טקסט-טקסט הופכת הבדל לשינוי מבוקר."
  },
  {
    "type": "open",
    "question": "תאר Trade-off בין Fidelity לעריכות בסביבת W+ כפי שהוצג בהרצאה.",
    "correctAnswerText": "שחזור מדויק דורש להתרחק מ-W ולהשתמש ב-W+ חופשי; אך עריכה סמנטית עובדת טוב רק לקודים קרובים ל-W. לכן Encoder מצומצם מדי פוגם בשחזור, ואילו Encoder חופשי מדי פוגם בעריכות.",
    "explanation": "צריך לאזן: קרוב מספיק ל-W לקבלת כיווני עריכה, אך עשיר מספיק בשכבות לשחזור."
  },
  {
    "type": "open",
    "question": "למה HyperStyle מסוגל לשמור את כיווני העריכה של StyleGAN גם אחרי שיפור השחזור?",
    "correctAnswerText": "HyperStyle לא משנה את משקולות הגנרטור בסיסית; הוא מוסיף Δ קטנה בזמן הריצה. הכיוונים הסמנטיים במרחב הלטנט חושבו על הגנרטור המקורי ולכן נשארים תקפים, וה-Δ רק משפרת פרטים מקומיים.",
    "explanation": "התוספת זמנית ואינה שוברת את ההתפלגות שהגדירה את הכיוונים המקוריים."
  },
  {
    "type": "open",
    "question": "הסבר כיצד Masked Convolution מבטיחה אוטורגרסיביות ב-PixelCNN.",
    "correctAnswerText": "חוסמים את החלקים במסנן שקולטים פיקסלים מימין או מתחת למיקום הנוכחי. כך כל פיקסל רואה רק 'עבר' לפי סדר הסריקה (למשל שמאל ולמעלה), ולכן לא נחשף למידע עתידי בזמן האימון והדגימה.",
    "explanation": "המסכה היא תנאי הכרחי לשמירה על מודל p(x_i | x_<i)."
  },
  {
    "type": "open",
    "question": "מדוע Transformer על קודים דיסקרטיים מתאים יותר מפיקסלים גולמיים בלמידה אוטורגרסיבית?",
    "correctAnswerText": "קודים דיסקרטיים מסכמים אזור פיקסלים גדול ומכילים מידע סמנטי יציב, בעוד שפיקסלים בודדים רועשים ושינוי קטן נחשב זהה תפיסתית. כך Transformer לומד קשרים משמעותיים במקום פרטים חסרי משמעות.",
    "explanation": "רצף קצר, עשיר במשמעות, חוסך חישוב ומשפר קוהרנטיות גלובלית."
  },
  {
    "type": "open",
    "question": "באילו מצבים PTI עדיף על HyperStyle, וההפך?",
    "correctAnswerText": "PTI מתאים כשצריך התאמת שחזור מדויקת במיוחד לתמונה בודדת ומוכנים להריץ אופטימיזציה; HyperStyle עדיף כשיש צורך בזמן ריצה מהיר על אוסף תמונות מגוון, במחיר דיוק מעט נמוך יותר.",
    "explanation": "PTI = איטי אך מותאם; HyperStyle = מהיר וגמיש אך מסתמך על רשת כללית."
  },
  {
    "type": "mc",
    "question": "ב-Stable Diffusion משתמשים ב-cross-attention כדי…",
    "options": [
      "לסנן רעש לבן מהרשת בזמן השחזור",
      "להחדיר את אמבדינג הטקסט אל תוך תכונות-התמונה בכל שכבת U-Net",
      "לגלות כיווני עריכה סמנטיים במרחב הלטנט",
      "למזער את מספר ה-tokens שה-Transformer רואה"
    ],
    "correctAnswerIndex": 1,
    "explanation": "מודול cross-attention משלב Query של פיצ'רי-התמונה עם Key/Value מן הטקסט וכך מקודד תנאי מילולי מדויק."
  },
  {
    "type": "mc",
    "question": "Classifier-Free Guidance (CFG) בדיפיוז'ן מגדיל…",
    "options": [
      "את המהירות של שלב denoising פי־שתיים",
      "את שליטת המשתמש בתנאי הטקסט על-ידי שקלול פלט מותנה וללא-תנאי",
      "את גודל ה-codebook של VQ-GAN לצורך חדות",
      "את קצב הלמידה בכל אפוק"
    ],
    "correctAnswerIndex": 1,
    "explanation": "CFG מקזז לוג-סבירות בלתי-מותנית מסבירות מותנית כדי לחזק השפעת הטקסט."
  },
  {
    "type": "mc",
    "question": "איזו שיטת דגימה לדיפיוז'ן נחשבת מהירה אך פחות מדויקת לפי ההרצאה?",
    "options": [
      "DDIM 50 צעדים",
      "Euler a (ancestral)",
      "PNDM (Pseudo-Num. DP-ODE)",
      "Sampling ב-ODE Solver ממוטב"
    ],
    "correctAnswerIndex": 0,
    "explanation": "DDIM מקצר מאוד את מספר השלבים אך יוצר פרטים פחות חדים מריצות ארוכות."
  },
  {
    "type": "mc",
    "question": "Memory-Efficient Attention (MEA) שהוזכר בהרצאה חוסך זיכרון על-ידי…",
    "options": [
      "החלפת softmax בליניארי",
      "חישוב בלוק-wise של המטריצה Q·K במקום בבת-אחת",
      "קידוד positional בסגנון סינוסואידלי",
      "שימוש בשכבת Conv1×1 במקום Attention"
    ],
    "correctAnswerIndex": 1,
    "explanation": "עיבוד בבלוקים קטנים מאפשר לשמור רק חלק מהמפתח-המכפלה ולצמצם השימוש VRAM."
  },
  {
    "type": "mc",
    "question": "Latent-Diffusion פועל במרחב…",
    "options": [
      "פיקסלים מלא",
      "קודים דיסקרטיים של VQ-GAN",
      "סמנטיקה של CLIP בלבד",
      "קוונטיזציה ברזולוציית 32×32"
    ],
    "correctAnswerIndex": 1,
    "explanation": "LD דוגם רעש ומנקה אותו במרחב הקומפקטי של VQ-GAN, מה שמאפשר חיסכון גדול בחישוב."
  },
  {
    "type": "open",
    "question": "הסבר בקצרה את תהליך denoising בדיפיוז'ן: מ-ε_t עד x_0.",
    "correctAnswerText": "ה-U-Net מנבא את הרעש ε_t עבור צעד t. לאחר מכן משתמשים במשוואת הרקונסטרוקציה x_{t-1}=ᾱ_t^{-0.5}(x_t-β_t ε_t)+σ_t ε′ כדי לחזור צעד אחורה. חזרה איטרטיבית (או DDIM מקוצר) מביאה ממערבולת רעש לתמונה נקייה x_0.",
    "explanation": "המודל לומד אינטגרטור הפוך לתהליך הדיפיוזיה ומחסיר רעש בהדרגה."
  },
  {
    "type": "open",
    "question": "מדוע Latent-Diffusion מסוגל לייצר תמונות 1024×1024 על כרטיס VRAM בינוני?",
    "correctAnswerText": "הדגימה נעשית על מפות f×h/8×w/8 במקום על הפיקסלים; כך self-attention וחישובי UNet מתבצעים על טנסורים קטנים פי 64, ואז Decoder של VQ-GAN מגדיל חזרה לרזולוציה מלאה.",
    "explanation": "צמצום המרחב בחישוב הדיפיוזי חוסך דרישת חישוב-זיכרון באופן דרסטי."
  },
  {
    "type": "open",
    "question": "מה היתרון של Timestep Embedding (sin/cos) ביחס ל-learnable vector ב-UNet של דיפיוז'ן?",
    "correctAnswerText": "אמבדינג סינוסי מספק רצף אינסופי, אקסטרפולציה חלקה, ושומר על מידע יחסי בין צעדים, בעוד אמבדינג ל-512 צעדים קבועים מגביל את הדגם ולעתים דורש טבלאות גדולות.",
    "explanation": "מחזוריות ויכולת אינטרפולציה תורמות ל-generalization בצעדי דגימה שונים."
  },
  {
    "type": "open",
    "question": "איך CFG (Classifier-Free Guidance) משנה את משוואת הדגימה?",
    "correctAnswerText": "במקום ε=ε_θ(x_t,c), מחשבים ε̂=ε_θ(x_t)+w(ε_θ(x_t,c)-ε_θ(x_t)); הפרש בין מותנה ללא-מותנה מוגדל במשקל w וכך מושך את התמונה לכיוון התנאי.",
    "explanation": "השקלול מחזק את השפעת c מבלי להזדקק למודל-מסווג חיצוני."
  },
  {
    "type": "open",
    "question": "מה הסיכון בהגדלת משקל CFG (w) מעבר לערך 8-10, וכיצד רואים זאת בתמונה?",
    "correctAnswerText": "w גבוה מדי דוחף את המודל לשטח לא-נלמד ויוצר over-saturation או ארטיפקטים מפוספסים; רואים תבניות חדות לא טבעיות והיעלמות פרטים עדינים.",
    "explanation": "הדגם יוצא מעקומת הנתונים שפגש באימון ולכן מפגין חוסר יציבות."
  },
  {
    "type": "open",
    "question": "הסבר בקצרה כיצד p sampling (PNDM) מאיץ דיפיוז'ן לעומת DDPM.",
    "correctAnswerText": "PNDM פותר את משוואת ODE המקבילה לתהליך הדיפיוזי במדרגות גדולות ומשתמש בנגזרות קודמות (multi-step) כמו נוסחת Adams-Bashforth; כך נדרשים ~25 צעדים במקום 100-1000.",
    "explanation": "פתרון ODE חוסך צעדים ע\"י חישוב שיפוע משוער יותר מדויק לכל צעד גדול."
  },
  {
    "type": "open",
    "question": "למה Sparse Attention (Win-size 64) יעיל לדגימת תמונות גדולות אך פחות לטקסט בקוד קצר?",
    "correctAnswerText": "בתמונה יש תלות חזקה מקומית; חלון 64×64 תופס הקשר מספיק. בטקסט, תלות ארוכת טווח בין מילים מרוחקות חשובה, ולכן חיתוך החלון יאבד סמנטיקה.",
    "explanation": "טבע הדאטה קובע אם ניתן להגביל ראייה מקומית מבלי לפגוע באיכות."
  },
  {
    "type": "mc",
    "question": "DreamBooth מנצל זוגות (תמונה, טקסט ממותג) כדי…",
    "options": [
      "לייצב את דגימת הדיפיוז'ן בעזרת PNDM",
      "לדייק את המודל בדמות/אובייקט חדש ללא פגיעה בכיווני טקסט כלליים",
      "להאיץ denoising בשיטת CFG",
      "להמיר קודי VQ-GAN לרזולוציה גבוהה יותר"
    ],
    "correctAnswerIndex": 1,
    "explanation": "המנגנון מוסיף token ייחודי ומכוונן את המודל כך שהטוקן מזוהה עם התמונות שסיפקת."
  },
  {
    "type": "mc",
    "question": "טקסטואל אינברז'ן (Textual Inversion) שונה מדרים־בּוּת בכך ש-…",
    "options": [
      "מעדכן רק טוקן embedding חדש ומשאיר את כל משקולות הדיפיוז'ן קבועות",
      "דורש לפחות 100 תמונות לאובייקט",
      "פועל רק במודלים אוטורגרסיביים",
      "מתבסס על Encoder של CLIP לשינוי וקטור w"
    ],
    "correctAnswerIndex": 0,
    "explanation": "Textual Inversion לומד embedding חדש בשכבת הטקסט בלבד ללא fine-tune על ה-UNet."
  },
  {
    "type": "mc",
    "question": "LoRA (Low-Rank Adaptation) מקטין את מספר הפרמטרים בעדכון דיפיוז'ן על-ידי…",
    "options": [
      "כפילת כל פילטר במטריצה רֵאנדום",
      "הוספת פירוק מדרגה נמוכה (A·B) למשקולות הקיימות בזמן האימון",
      "החלפת כל קונבולוציה ב-Depthwise",
      "שמירת Layers FP16 בלבד"
    ],
    "correctAnswerIndex": 1,
    "explanation": "רק המטריצות הצרות A,B נלמדות; המשקל המקורי נשאר קפוא וחוסך זיכרון."
  },
  {
    "type": "mc",
    "question": "ControlNet מוסיף רשת ענף (branch) צמודה ל-UNet כדי…",
    "options": [
      "לשפר דיוק CFG",
      "להזרים תנאי מבני כמו קווי-מתאר, מפות עומק או פוזה",
      "להחליף את CLIP ב-AutoEncoder",
      "לבצע Safety Checking בזמן אמת"
    ],
    "correctAnswerIndex": 1,
    "explanation": "הרשת המשוכפלת מקבלת פיצ'רים לֹא-רעשים מהתנאי ומדריכה את דיפיוז'ן מבלי להרוס המשקולות המקוריות."
  },
  {
    "type": "mc",
    "question": "Scheduler מסוג K-LMS (LMSD) נחשב…",
    "options": [
      "ODE-based עם ארבעה שלבים ומערב גרדיאנט קודם",
      "שיטת Euler-Maruyama עם רעש גאוסי חופשי",
      "פתרון ליניארי לאחוזת β קבועה בלבד",
      "האיטי ביותר מבין הסמפלרים הנפוצים"
    ],
    "correctAnswerIndex": 0,
    "explanation": "LMS משתמש במדרגה מרובעת (multistep) בדומה ל-PNDM אך עם רעש החזרה שונה."
  },
  {
    "type": "open",
    "question": "מדוע Textual Inversion דורש לבחור *טוקן נדיר* (לדוגמה \"sks\") במקום תיק של מילת קיים?",
    "correctAnswerText": "טוקן נדיר אינו מקושר למשמעות קיימת, ולכן האמבדינג החדש יכול לקודד רק את האובייקט שלך בלי להתנגש עם דפוסים שנלמדו עבור מילים קיימות.",
    "explanation": "הימנעות מקונפליקט במשמעות מאפשרת שליטה מלאה בבקשות הטקסט."
  },
  {
    "type": "open",
    "question": "כיצד LoRA מאפשר לשתף מודלים מותאמים אישית בלי להפר את זכויות ההפצה של Stable Diffusion המקורי?",
    "correctAnswerText": "מפיצים רק את משקלות A,B הזעירים (Δ) ולא את משקולות ה-UNet המלאות; המשתמש טוען את SD המקורי ומחיל את ה-LoRA מעל, כך שאין הפצה חוזרת של הקניין הרוחני המלא.",
    "explanation": "Δ לבדו ≪ 1 MB לכן אינו כולל תוכן המודל הרשמי."
  },
  {
    "type": "open",
    "question": "מה הסכנה ב-CFG גבוה מאוד כאשר משלבים אותו עם סאמפלר DDIM קצר (30 צעדים)?",
    "correctAnswerText": "השילוב יוצר עודף גרדיאנט לכיוון הטקסט בכל צעד גדול, ולכן התמונה יכולה לקרוס לרוויית צבעים, double-edge או פסים גאומטריים.",
    "explanation": "צעד גדול + חיזוק חזק → יציאה מעקומת האימון."
  },
  {
    "type": "open",
    "question": "למה ControlNet מקפיא את משקולות ה-UNet המקורי ומאמן רק את הענף הנוסף?",
    "correctAnswerText": "כך שומרים על יכולת המודל לייצר כל דומיין שקודם ידע, ומונעים 'שיכחה קטסטרופלית' בזמן הכוונת המבנה; הענף המלמד רק מוסיף מידע, לא מוחק.",
    "explanation": "שילוב לטרלי מאפשר תנאי חזק בלי לפגוע בגנרליות."
  },
  {
    "type": "open",
    "question": "הסבר בקצרה מהו \"Noise Offset\" (σ_offset) בסאמפלר Euler-a ומדוע ניתן להקטינו לתמונות של פרצופים.",
    "correctAnswerText": "σ_offset מוסיף רעש מ-N(0,σ²) בשלב early למניעת mode collapse להתכנסות לאפס. פנים מכילות מבנה חזק ומוגדר, לכן מספיק offset קטן כדי לשמור חדות מבלי לפגוע בזיהוי.",
    "explanation": "רעש מופרז יפגע בתווי פנים; רף נמוך מייצר תוצאה נקייה."
  },
  {
    "type": "open",
    "question": "מה היתרון של Fine-Tuning דיפיוז'ן במרחב V-Prediction (v-prediction) לעומת ε-Prediction?",
    "correctAnswerText": "חיזוי v (שילוב ליניארי של ε ו-x_0) מקל על הסמפלרים ODE לסגור מעגל בין צעדים קדימה ואחורה, ומקטין סטריפינג צבעוני; המודל לומד מטרה סימטרית יותר ב-β.",
    "explanation": "מטה-אנרגיה חלקה → גרדיאנט יציב דגימה יעילה."
  }
]
