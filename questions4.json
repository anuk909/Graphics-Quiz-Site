[
  {
    "type": "mc",
    "question": "איזו בעיה מרכזית פותרת StyleGAN-NADA?",
    "options": [
      "הסרת רעש סטוכסטי משכבות הרעש של StyleGAN",
      "התאמת הגנרטור לדומיין חדש בלי תמונות מאותו דומיין",
      "שיפור חדות הפלט בעזרת Discriminator נוסף",
      "המרת תמונות אמיתיות לקוד W+ במהירות"
    ],
    "correctAnswerIndex": 1,
    "explanation": "השיטה מקפיאה עותק מקורי של הגנרטור ומדייקת עותק מאומן כך שיצייר קטגוריה שלא הופיעה באימון, מבלי להחזיק דאטה של אותה קטגוריה"
  },
  {
    "type": "mc",
    "question": "למה אופטימיזציה ישירה של ה-w המקורי לא מספיקה כדי לייצר חתול ממודל שאומן על כלבים?",
    "options": [
      "כי מודל CLIP אינו חלק מהזרימה קדימה",
      "כיוון שהמרחב W חסום למשקולות קפואות וגיוון נמוך",
      "משום שהגנרטור כלל לא מכיר חתולים ולכן כל וקטור w יישאר בדומיין הכלבים",
      "בגלל ש-w תמיד עובר נירמול Demodulation שמגביל סגנון"
    ],
    "correctAnswerIndex": 2,
    "explanation": "המודל לא 'יודע' ליצור חתול; הזזת w בלבד תסתובב בתוך אזורי כלביים של ההתפלגות"
  },
  {
    "type": "mc",
    "question": "באימון StyleGAN-NADA איזה רכיב קפוא (frozen)?",
    "options": [
      "ה-Discriminator של StyleGAN",
      "עותק מקורי של הגנרטור",
      "רשת CLIP הטקסטואלית בלבד",
      "כל שכבות ה-Mapping Network"
    ],
    "correctAnswerIndex": 1,
    "explanation": "משתמשים בשני גנרטורים: G_frozen נשאר ללא עדכון ומייצר את התמונה בדומיין המקורי לצורך חישוב הכיוון ב-CLIP"
  },
  {
    "type": "mc",
    "question": "מהו היתרון הבולט של Inversion מבוסס-Encoder (למשל pSp) לעומת Latent Optimization?",
    "options": [
      "דיוק גבוה יותר במחיר חיפוש איטרטיבי ארוך",
      "מהירות רבה בזכות Pass יחיד קדימה על חשבון דיוק קטן יותר",
      "שמירה מוחלטת על זהות הפנים ללא Fine-Tuning",
      "יכולת לעבוד בדומיין חדש בלי אימון נוסף"
    ],
    "correctAnswerIndex": 1,
    "explanation": "Encoder מאומן מראש מחזיר מיד וקטור W+ ולכן ההיפוך מיידי, אך השחזור פחות מדויק יחסית לאופטימיזציה איטרטיבית"
  },
  {
    "type": "mc",
    "question": "Pivotal Tuning Inversion (PTI) משפר דיוק שחזור ע\"י…",
    "options": [
      "אימון Discriminator נוסף לרזולוציה גבוהה",
      "טאונינג נקודתי של משקולות הגנרטור סביב תמונה מסוימת",
      "החלפת AdaIN במנגנון Mod/Demod",
      "שימוש בקוד לטנטי Z במקום W+"
    ],
    "correctAnswerIndex": 1,
    "explanation": "PTI עושה Fine-Tuning קל למשקולות על בסיס הפלט הראשוני כדי לצמצם שגיאות לוקאליות של התמונה המסוימת"
  },
  {
    "type": "mc",
    "question": "מה מוסיף HyperStyle לגנרטור כדי לשפר את השחזור עבור כל תמונה?",
    "options": [
      "שכבות Self-Attention גלובליות",
      "רשת קטנה שמפיקה תיקון ממוקד למשקולות הגנרטור",
      "קוד רעש דינמי פר-פיקסל",
      "Decoder נוסף לרזולוציה גבוהה"
    ],
    "correctAnswerIndex": 1,
    "explanation": "HyperStyle מוסיף רשת קטנה הלומדת להפיק תיקון מותאם למשקולות הגנרטור, כך שהשחזור יהיה מדויק יותר מבלי לאבד את מבנה העריכה הכללי."
  },
  {
    "type": "mc",
    "question": "מדוע נהוג להשתמש במרחב W+ ולא רק ב-W בתהליך inversion של תמונות ב-StyleGAN?",
    "options": [
      "כי W+ מותאם ל-CLIP ומחזיר טקסט חופשי",
      "כדי לאפשר וקטור שונה בכל שכבה לשחזור פרטים עדינים",
      "כיוון שהוא צורך פחות זיכרון GPU",
      "מפני שהוא מונע Demodulation בשכבות"
    ],
    "correctAnswerIndex": 1,
    "explanation": "מרחב W+ נותן שליטה על כל שכבה בנפרד, מה שמאפשר לשפר דיוק גם בפרטים הקטנים וגם במבנה הכללי של התמונה."
  },
  {
    "type": "mc",
    "question": "איזה שימוש הדגימה ההרצאה ל-Encoder Inversion מעבר ל-\"Invert & Edit\"?",
    "options": [
      "In-painting ללא מסכה",
      "Edge-Map → Image ליצירת תמונות ריאליסטיות מקווי מתאר",
      "דחיסת תמונה לארכיון Codebook",
      "שחזור עומק (Depth) בהפרדה"
    ],
    "correctAnswerIndex": 1,
    "explanation": "כאשר מאמנים את ה-Encoder על זוגות קווי מתאר/תמונה, אפשר להפוך סקיצה לתמונה פוטוריאליסטית בזכות כוחו של StyleGAN"
  },
  {
    "type": "mc",
    "question": "מה העיקרון המרכזי שמאפיין מודלים גנרטיביים אוטורגרסיביים?",
    "options": [
      "יצירת כל התמונה בבת אחת לפי ייצוג כללי בודד",
      "חיזוי כל רכיב (למשל פיקסל) בהתבסס על הרכיבים שנוצרו לפניו",
      "בחירה של תמונה מוכנה מתוך אוסף מוגבל של אפשרויות",
      "שימוש במפלה כדי לדחות דוגמאות לא אמינות"
    ],
    "correctAnswerIndex": 1,
    "explanation": "במודלים אוטורגרסיביים כל רכיב מחושב לפי מה שנוצר עד אותו שלב, מה שמאפשר יצירה הדרגתית של תמונה או רצף."
  },
  {
    "type": "mc",
    "question": "כיצד מפורקת הסתברות התמונה במודל אוטורגרסיבי?",
    "options": [
      "p(x)=Σp(x_i)",
      "p(x)=Π p(x_i | x_{<i})",
      "p(x)=max p(x_i)",
      "p(x)=Mean p(x_i)"
    ],
    "correctAnswerIndex": 1,
    "explanation": "ה-AR מחלק את הקלט לרצף וּמודל את ההסתברות המותנית של כל חלק על קודמיו, כלומר מכפלת תנאים סדרתית"
  },
  {
    "type": "mc",
    "question": "PixelCNN משתמש ב-Masked Convolution בעיקר כדי…",
    "options": [
      "למנוע התפוצצות גרדיאנט ברזולוציה גבוהה",
      "לא לאפשר לפיקסל הנוכחי לראות פיקסלים עתידיים בסריקה",
      "להקטין את מספר הפרמטרים פי ארבעה",
      "לשלב Self-Attention חישובי"
    ],
    "correctAnswerIndex": 1,
    "explanation": "אפסים במסנן מבטיחים שהרשת נשענת רק על פיקסלים שכבר נוצרו וכך משמרת את הסדר האוטורגרסיבי התקף"
  },
  {
    "type": "mc",
    "question": "מגבלה בולטת של PixelCNN שהוזכרה בהרצאה היא…",
    "options": [
      "דגימות מטושטשות ואיטיות מאוד ברזולוציות גבוהות",
      "קושי בלמידה מבוססת CLIP",
      "חוסר יכולת לעבוד עם קוד דיסקרטי",
      "הישענות על תוויות מפוקחות"
    ],
    "correctAnswerIndex": 0,
    "explanation": "המודל יוצר תמונות 32×32-64×64 לא חדות ודגימה פיקסל-אחר-פיקסל גוזלת זמן רב"
  },
  {
    "type": "mc",
    "question": "VQ-VAE מציג ייצוג דיסקרטי בעזרת…",
    "options": [
      "קואנטיזציה של וקטורי הפיצ'ר לקודבוק סופי",
      "החלת Masked Convolution בכל שכבה",
      "Demodulation גלובלי על וקטורים",
      "קרוס-אטנשן בין פאטצ'ים"
    ],
    "correctAnswerIndex": 0,
    "explanation": "Encoder מחפש לכל וקטור את הערך הקרוב ביותר ב-Codebook וכך דוחס את התמונה למפת סמלים קומפקטית"
  },
  {
    "type": "mc",
    "question": "מדוע שחזורי VQ-VAE נוטים להיות מטושטשים?",
    "options": [
      "ה-Decoder חסר שכבות Upsample",
      "אין Loss אדברסריאלי ואיבוד קשרים גלובליים",
      "שימוש בדלתא משקולות מקומיות בלבד",
      "ה-Codebook קטן מדי ומאלץ חזרתיות"
    ],
    "correctAnswerIndex": 1,
    "explanation": "ללא Discriminator נוסף התמונות נשענות רק על Loss שחזור ולכן מאבדות חדות ופרטים גלובליים"
  },
  {
    "type": "mc",
    "question": "VQ-GAN מוסיף ל־VQ-VAE שני מרכיבים עיקריים: Discriminator ו־…",
    "options": [
      "Transformer הלומד את התפלגות הקודים הדיסקרטיים",
      "Masked Convolution נוסף בשכבה האחרונה",
      "שכבת Modulation-Demodulation",
      "שכבת תיקון משקולות המותאמת לכל תמונה"
    ],
    "correctAnswerIndex": 0,
    "explanation": "השילוב של Discriminator משפר את חדות התמונה, וה-Transformer לומד קשרים גלובליים ברצף הקודים כדי לשפר את הקוהרנטיות."
  },
  {
    "type": "mc",
    "question": "Self-Attention במודל אוטורגרסיבי של VQ-GAN מאפשר…",
    "options": [
      "חסכון בזיכרון ליניארי לגודל הקלט",
      "למידת קשרים גלובליים בין אזורים רחוקים בתמונה",
      "הפחתת צורך ב-Codebook",
      "ביטול Masked Convolution"
    ],
    "correctAnswerIndex": 1,
    "explanation": "כל פיקסל 'שואל' על אחרים דרך Q,K,V ומקבל ייצוג עשיר המחבר חלקים מרוחקים של התמונה"
  },
  {
    "type": "mc",
    "question": "מה החיסרון העיקרי של Self-Attention שהודגש?",
    "options": [
      "אובדן חזרה פנימית בתמונה",
      "מורכבות חישובית ריבועית בגודל הקלט",
      "בעיית Mode Collapse מתמשכת",
      "רגישות גבוהה ללמידת יתר"
    ],
    "correctAnswerIndex": 1,
    "explanation": "לכל שכבת Attention עלות O(n²), מה שהופך אותה יקרה במיוחד לתמונות גדולות"
  },
  {
    "type": "mc",
    "question": "במודל VQ-VAE עם מבנה היררכי, מהו התפקיד של שכבות top ו-bottom?",
    "options": [
      "הפרדת מבנה גלובלי מפרטים עדינים לשיפור רזולוציה",
      "דילול משקלים לחיסכון בזיכרון",
      "למידת קוד לטנט רציף במקום דיסקרטי",
      "ביטול הצורך בגנרטור לשלב הפענוח"
    ],
    "correctAnswerIndex": 0,
    "explanation": "שכבת top מקודדת את המבנה הכללי של התמונה, ושכבת bottom מוסיפה טקסטורות ופרטים – כך נשמר איזון בין איכות לבין דחיסה."
  },
  {
    "type": "mc",
    "question": "איזו נוסחת רגולריזציה מוסיפים ב-pSp כדי לייצב את קודי W+?",
    "options": [
      "L1 בין פיקסלים",
      "KL Divergence לנורמלית",
      "Distance Loss אל הקוד הממוצע במרחב W",
      "Orthogonality Loss על משקולות"
    ],
    "correctAnswerIndex": 2,
    "explanation": "Regularization מושך את הקוד לכיוון μ_W ומונע חריגה לאזורים שבהם הגנרטור מפיק תוצאות"
  },
  {
    "type": "mc",
    "question": "ב-HyperStyle, מהי מטרת הרשת הנוספת שנלמדת לצד הגנרטור?",
    "options": [
      "לחזות שינוי בקוד w",
      "לחשב תיקון למשקולות הגנרטור כדי לשפר דיוק שחזור",
      "ליצור מסכת Attention עבור שכבות ה-UNet",
      "לדחוס את קוד Z לרצף טוקנים קצר"
    ],
    "correctAnswerIndex": 1,
    "explanation": "הרשת לומדת להפיק תיקון פר-תמונה למשקולות הגנרטור, כך שהפלט יתאים טוב יותר לתמונה המקורית שנרצה לשחזר."
  },
  {
    "type": "mc",
    "question": "ב-PixelCNN מותנה, כיצד מוזרם המידע על הקטגוריה הרצויה אל המודל?",
    "options": [
      "באמצעות קוד מתוך ה-Codebook",
      "באמצעות תווית שמספקת מידע קטגורי נוסף למודל",
      "באמצעות שכבת רעש סטוכסטי",
      "באמצעות שינוי ישיר במשקולות הרשת"
    ],
    "correctAnswerIndex": 1,
    "explanation": "PixelCNN יכול לקבל תווית חיצונית (למשל ספרה) שמנחה אותו לדגום רק דוגמאות שמתאימות לקטגוריה הזו."
  },
  {
    "type": "mc",
    "question": "מה המאפיין העיקרי של Masked Conv ב-PixelCNN לעומת Conv רגיל?",
    "options": [
      "ליבה בקונבולוציה רדיאלית",
      "איפוס משקלים במסנן שלא שייכים לעבר הפיקסל",
      "הוספת Bias כפול לכל ערוץ",
      "שימוש בקוד Book Lookup"
    ],
    "correctAnswerIndex": 1,
    "explanation": "המסיכה מבטיחה שכל קונבולוציה נשענת רק על פיקסלים שכבר יוצרו ולא על עתידיים, כך נשמרת סיבתיות "
  },
  {
    "type": "mc",
    "question": "Transformer על רצף הקודים ב-VQ-GAN עדיף על PixelCNN כי…",
    "options": [
      "הוא ליניארי בגודל הקלט",
      "הוא לוכד קשרים ארוכי טווח ומשפר קוהרנטיות",
      "הוא אינו דורש שום Mask",
      "הוא מפחית גודל Codebook בחצי"
    ],
    "correctAnswerIndex": 1,
    "explanation": "Self-Attention מקשרת קוד בקצה אחד של התמונה עם אחר בצד השני וכך יוצרת מבנה גלובלי משכנע "
  },
  {
    "type": "mc",
    "question": "ב-StyleGAN-NADA החיווי בין שתי תמונות נמדד ב-CLIP כ…",
    "options": [
      "הפרש וקטורים במרחב הסגנון S",
      "כיוון סמנטי בין אמבדינגי התמונות במרחב CLIP",
      "רמת קשיחות מודולציה ל-w",
      "מרחק L2 ישיר בפיקסלים"
    ],
    "correctAnswerIndex": 1,
    "explanation": "משווים embedding_תמונה_מקור ל-embedding_תמונה_יעד ומיישרים אותו לכיוון בין הטקסטים "
  },
  {
    "type": "mc",
    "question": "מה היתרון המרכזי של שיטת StyleGAN-NADA ביחס לשיטות עריכה מסורתיות?",
    "options": [
      "מאפשרת עריכה על בסיס טקסט ללא דאטה נוסף או fine-tuning לתמונה ספציפית",
      "דורשת אימון discriminators נוספים לכל מחלקה",
      "משתמשת רק בשכבות ה-noise למיקוד בשינויים טקסטורליים",
      "מצריכה תיוגים מפורשים בכל שלב באימון"
    ],
    "correctAnswerIndex": 0,
    "explanation": "StyleGAN-NADA מאפשר שליטה מבוססת טקסט על סגנון התמונה בלי דאטה נוסף, תוך שימוש במודל מאומן מראש ו-CLIP."
  },
  {
    "type": "mc",
    "question": "מדוע דגימה ב-PixelCNN איטית?",
    "options": [
      "ה-Codebook קטן מדי",
      "נדרש חישוב סדרתי פיקסל-אחר-פיקסל",
      "Self-Attention מרובע בזמן",
      "Demodulation בכל שכבה"
    ],
    "correctAnswerIndex": 1,
    "explanation": "כל פיקסל מחושב לאחר הקודמים ולכן אין הקבלה; זה גורם לזמן דגימה ארוך במיוחד "
  },
  {
    "type": "mc",
    "question": "GAN-Loss ב-VQ-GAN מחייב את ה-Decoder…",
    "options": [
      "לסכם פיקסלים ל-1",
      "להפיק תמונות חדות ו“אמיתיות” בעיני Discriminator",
      "לשמור על L2 נמוך בלבד",
      "להפוך Self-Attention לריבועי"
    ],
    "correctAnswerIndex": 1,
    "explanation": "בזכות Discriminator נוסף, הדקודר לומד להחזיר פרטים חדים שנראים אמיתיים למפלה "
  },
  {
    "type": "mc",
    "question": "איזה מנגנון שומר על קשרים ארוכי-טווח במודל Transformer אך דורש משאבים רבים?",
    "options": [
      "Skip-Connections",
      "Self-Attention",
      "Masked Conv",
      "Batch-Norm"
    ],
    "correctAnswerIndex": 1,
    "explanation": "Self-Attention מצריך חישוב מטריצה n×n ולכן יקר אולם מוסיף הבנה גלובלית "
  },
  {
    "type": "mc",
    "question": "ב-HyperStyle, לאחר השחזור ניתן עדיין להשתמש בכיווני עריכה שנלמדו מראש כי…",
    "options": [
      "הרשת משנה רק את ה-Mapping Network",
      "התיקון למשקולות הוא קטן ואינו משנה את מבנה מרחב העריכה",
      "המערכת מבטלת רעש סטוכסטי בין שכבות",
      "המנגנון מחליף את CLIP בתוויות מפוקחות"
    ],
    "correctAnswerIndex": 1,
    "explanation": "התיקונים שמוספים למשקולות הגנרטור הם מקומיים ומדויקים, ולכן נשמרים הכיוונים הסמנטיים של מרחב העריכה המקורי."
  },
  {
    "type": "mc",
    "question": "מהו תפקיד ה-Codebook ב-VQ-VAE?",
    "options": [
      "לאכוף רגולריזציה L2 על Encoder",
      "לאפשר קוונטיזציה של פיצ'רים לווקטורים סופיים ולצמצם ממד",
      "להגדיל חדות בעזרת GAN-Loss",
      "לייצר קשר בין פיקסלים סמוכים"
    ],
    "correctAnswerIndex": 1,
    "explanation": "כל וקטור פיצ'ר מוצמד לכניסה הקרובה ביותר בקודבוק, כך שהיצוג נעשה דיסקרטי ודחוס "
  },
  {
    "type": "mc",
    "question": "למה Transformer ברצף קודים איננו ליניארי בגודל התמונה?",
    "options": [
      "בגלל קוונטיזציה",
      "מפני שמטריצת QK^T היא מסדר n×n",
      "ה-Codebook קטן מדי",
      "Demodulation מוסיף מורכבות"
    ],
    "correctAnswerIndex": 1,
    "explanation": "Self-Attention מחשב את הדמיון בין כל זוג קודים ולכן העלות חישובית ריבועית במספר הקוד n "
  },
  {
    "type": "open",
    "question": "תאר בקצרה את התהליך המרכזי של StyleGAN-NADA.",
    "correctAnswerText": "מעתיקים את הגנרטור, מקפיאים את המקור, ומאמנים את העותק כך שכיוון השינוי בין תמונת-מקור לתמונת-יעד (ב-CLIP) יהיה מקביל לכיוון שנגזר מזוג טקסטים. כך לומדים לעבור לדומיין חדש בלי דאטה מתאים.",
    "explanation": "השוואת כיוונים ב-CLIP מדריכה את המשקולות להתאים דומיין חדש תוך שמירה על מבנה לטנטי "
  },
  {
    "type": "open",
    "question": "למה אופטימיזציה של w בלבד נכשלת כשמנסים לרנדר חתול בגנרטור כלבים?",
    "correctAnswerText": "ה-w שולט רק בתוכן שקיים בדומיין הלמידה. אם הגנרטור מעולם לא ראה חתולים, כלל מרחב W מיפוי לכלבים ולכן אין כיוון שמוביל לחתול.",
    "explanation": "המודל חסר פונקציית ייסוד לדומיין החתולים ולכן w תקוע בתת-מרחב של כלבים "
  },
  {
    "type": "open",
    "question": "הסבר את ה-trade-off בין דיוק למהירות בגישות Latent Optimization לעומת Encoder Inversion.",
    "correctAnswerText": "Latent Optimization מוצא קוד לטנט מותאם אישית בדיוק גבוה אך דורש זמן חישוב רב; Encoder מבצע ניבוי מהיר בפס אחד אך השחזור פחות מדויק כי המודל כללי ואינו מותאם לתמונה הספציפית.",
    "explanation": "Encoder מקריב דיוק עבור מהירות, בעוד Latent Optimization מתאים את הקוד לכל תמונה אך דורש זמן."
  },
  {
    "type": "open",
    "question": "כיצד Pivotal Tuning Inversion (PTI) מצליח לחדד שחזור של תמונה מבלי לפגוע בכיווני העריכה של הגנרטור?",
    "correctAnswerText": "Pivotal Tuning Inversion מתחיל ב-inversion רגיל של התמונה, ואז מבצע Fine-Tuning קטן על משקולות הגנרטור באופן מקומי לתמונה הזו בלבד. כך נשמרים כיווני העריכה, אבל מתקבל שחזור מדויק יותר.",
    "explanation": "העדכון ממוקד סביב נקודת הפיווט (w) ולכן לא מעוות את המבנה הכללי של מרחב העריכה."
  },
  {
    "type": "open",
    "question": "מהו Masked Convolution ב-PixelCNN ולמה הוא קריטי?",
    "correctAnswerText": "זהו פילטר קונבולוציה שבו ערכים שמעבירים מידע מפיקסלים עתידיים מאופסים, וכך הפיקסל הנוכחי תלוי רק בעברו ומקיים אוטורגרסיביות.",
    "explanation": "המסיכה מונעת \"דליפת עתיד\" ומבטיחה שהמודל יכבד את סדר הסריקה "
  },
  {
    "type": "open",
    "question": "כיצד VQ-VAE מייצג תמונה כקוד דיסקרטי?",
    "correctAnswerText": "Encoder מפיק מפה של וקטורים רציפים; כל וקטור מוחלף בכניסה הקרובה ביותר ב-Codebook קבוע, ו-Decoder משחזר מהקוד הבדיד.",
    "explanation": "התהליך קוואנטיזציה → ייצוג קומפקטי 
  },
  {
    "type": "open",
    "question": "תאר את שלבי חישוב Self-Attention כפי שהוסברו בהרצאה.",
    "correctAnswerText": "מחשבים Q, K, V מהקלט. מבצעים מכפלה בין Q ל-K^T כדי למדוד דמיון בין עמדות, מריצים softmax, ואז מכפילים את התוצאה ב-V כדי ליצור ייצוג חדש שמשלב מידע מכל המקומות בתמונה.",
    "explanation": "כך כל מיקום בתמונה מקבל מידע מהמיקומים הדומים לו על פי Q ו-K."
  },
  {
    "type": "open",
    "question": "מדוע Transformer מספק קוהרנטיות טובה יותר מאשר PixelCNN כאשר עובדים על רצף קודים בדגם כמו VQ-GAN?",
    "correctAnswerText": "Transformer משתמש ב-Self-Attention שמאפשר לכל קוד להתחשב גם בקודים רחוקים ברצף. לכן הוא לומד קשרים גלובליים בתמונה, ולא רק תלות מקומית כמו ב-Masked Convolution של PixelCNN.",
    "explanation": "קשרים רחוקים בין חלקים שונים בתמונה חשובים לשמירה על מבנה, סימטריה והגיון חזותי כללי – Transformer תופס אותם טוב יותר."
  },
  {
    "type": "open",
    "question": "כיצד פועלת ההיררכיה של שתי השכבות במבנה המורחב של VQ-VAE?",
    "correctAnswerText": "השכבה העליונה מקודדת את המבנה הגלובלי של התמונה, והשכבה התחתונה מוסיפה פרטים כמו טקסטורות. הדגימה נעשית מהשכבה העליונה כלפי מטה.",
    "explanation": "ההפרדה מאפשרת למודל לתפוס תכונות ברמות שונות ולשלב מבנה עם פירוט חזותי."
  },
  {
    "type": "open",
    "question": "מה העיקרון המרכזי בפירוק אוטורגרסיבי של מודלים גנרטיביים?",
    "correctAnswerText": "במקום לחזות את כל התמונה או הרצף בבת אחת, המודל לומד לחזות כל חלק (כמו פיקסל או מילה) לפי מה שנוצר עד אותו שלב, וכך בונה את הפלט שלב אחר שלב.",
    "explanation": "הפירוק מאפשר למודל ללמוד הסתברויות מותנות לכל רכיב בהתאם למה שקדמה לו."
  },
  {
    "type": "open",
    "question": "כיצד Loss אדברסריאלי ב-VQ-GAN מחדד תמונה?",
    "correctAnswerText": "Discriminator דורש מה-Decoder לייצר פרטים דקים כדי להטעותו; לכן השחזור הופך חד ומציאותי יותר לעומת Loss שחזור בלבד.",
    "explanation": "GAN-Loss מוסיף לחץ לפלט לראות אמיתי בעיני מפלה "
  },
  {
    "type": "open",
    "question": "מה תפקיד Regularization Loss ב-pSp Encoder?",
    "correctAnswerText": "למשוך את הקודים ש-Encoder מפיק לכיוון הממוצע ב-W ובכך למנוע חריגה לאזורים שבהם הגנרטור מפיק תוצאות לא יציבות.",
    "explanation": "שמירה על קודים 'בטוחים' משפרת יציבות שחזור "
  },
  {
    "type": "open",
    "question": "אילו מגבלות מציג PixelCNN כשעוברים לרזולוציות גבוהות?",
    "correctAnswerText": "הדגימה סיבתית פיקסל-אחר-פיקסל נעשית איטית וקשרי לוקאליות גורמים לתמונות מטושטשות וחסרות קוהרנטיות גלובלית.",
    "explanation": "העדר קשר ארוך טווח והחישוב הסדרתי מגבילים "
  },
  {
    "type": "open",
    "question": "כיצד HyperStyle משפר את איכות השחזור של תמונה קיימת ביחס לשיטות אחרות?",
    "correctAnswerText": "HyperStyle מוסיף רשת קטנה שלומדת להפיק תיקון למשקולות הגנרטור בזמן ריצה. התיקון מותאם אישית לכל תמונה, ומחובר למשקולות המקוריות כדי לשפר את השחזור מבלי לאבד את מבנה מרחב העריכה.",
    "explanation": "התיקון מתבצע בצורה מקומית ומבוסס על התמונה, בלי לפגוע בכיווני העריכה הכלליים של המודל."
  },
  {
    "type": "open",
    "question": "כיצד CLIP מדריך את האימון ב-StyleGAN-NADA?",
    "correctAnswerText": "חישוב כיוון בין תמונות (G_frozen, G_train) וכיוון בין טקסטים, והמטרה לגרום לכיוונים להחפף במרחב CLIP – זה מפעיל גרדיאנט על משקולות הגנרטור המאומן.",
    "explanation": "CLIP מספק מרחב משותף תמונה-טקסט לחיווי סמנטי "
  },
  {
    "type": "open",
    "question": "מדוע השימוש במרחב W+ עוזר לשמר פרטים עדינים בתהליך inversion?",
    "correctAnswerText": "מכיוון שכל שכבה מקבלת וקטור נפרד, ניתן לשלוט בדיוק על שכבות נמוכות (מבנה) ועל שכבות גבוהות (פרטים עדינים כמו טקסטורה ונמשים), וכך מתקבל שחזור מדויק יותר.",
    "explanation": "הרחבת דרגת החופש של הקוד מאפשרת לשמור גם על מיקרו-וריאציות ללא פשרה על איכות."
  },
  {
    "type": "open",
    "question": "מהו תהליך הקוונטיזציה ב-VQ-VAE ולמה הוא חשוב?",
    "correctAnswerText": "בכל מיקום בוחרים את הווקטור הקרוב ב-Codebook במקום לשמור ערך רציף; כך מקבלים ייצוג דחוס שמקל על מודל גנרטיבי ללמוד התפלגות על קבוצה סופית.",
    "explanation": "הדיסקרטיזציה מאפשרת שימוש ב-AR או Transformer על רצף קצר ובר-השוואה "
  },
  {
    "type": "open",
    "question": "תאר בקצרה מודל Autoregressive מותנה וכיצד הוא שולט על סוג הדגימות.",
    "correctAnswerText": "מכניסים קוד h שמייצג קטגוריה; במהלך האימון החלקים ברצף לומדים גם מהפיקסלים הקודמים וגם מה-h כך שבדגימה ניתן לבחור h ולקבל רק דוגמאות מאותה קטגוריה.",
    "explanation": "הוספת תנאי מיישרת את הדגימה להתחום הרצוי "
  }
]
