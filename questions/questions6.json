[
  {
    "type": "mc",
    "question": "מהו הרעיון המרכזי שמניע את מודלי הדיפוזיה?",
    "options": [
      "יצירת תמונה על ידי הוספת רעש הדרגתית.",
      "יצירת תמונה על ידי הסרת רעש מדורגת.",
      "המרת תמונות בין דומיינים שונים.",
      "יצירת תמונות מותנות מתיאורי טקסט בלבד."
    ],
    "correctAnswerIndex": 1,
    "explanation": "הרעיון המרכזי שמניע את מודלי הדיפוזיה פשוט: יצירת תמונה דרך תהליך של הסרת רעש מדורגת."
  },
  {
    "type": "mc",
    "question": "מדוע מודלי דיפוזיה החליפו את הדומיננטיות של GANs בתחום עיבוד התמונה?",
    "options": [
      "בגלל תהליך אימון מהיר יותר.",
      "בזכות איכות תמונות נמוכה יותר.",
      "עקב תהליך אימון יציב יותר, איכות תמונות גבוהה, הימנעות מ-mode collapse, ויכולת שליטה מדויקת.",
      "כי הם דורשים פחות משאבי חישוב."
    ],
    "correctAnswerIndex": 2,
    "explanation": "הם הצליחו להחליף את הדומיננטיות של GANS בזכות יתרונות משמעותיים: תהליך אימון יציב יותר, איכות תמונות גבוהה, יכולת להימנע מבעיות כמו mode collapse, וכן אפשרות לשלוט ולכוון את תהליך היצירה בצורה מדויקת."
  },
  {
    "type": "mc",
    "question": "מהי המטרה העיקרית של הרשת המאומנת בתהליך ה-Reverse denoising?",
    "options": [
      "לשחזר את התמונה המקורית ($x_0$).",
      "לנחש מהו הרעש שנוסף לתמונה ($x_t$).",
      "למזער את פונקציית ה-loss של המודל.",
      "להוסיף רעש גאוסי לתמונה."
    ],
    "correctAnswerIndex": 1,
    "explanation": "הרשת מאומנת לקבל את התמונה המורעשת ואת הערך $t$, ולנסות לנחש מהו הרעש שנוסף לתמונה. המטרה של הרשת היא לא לשחזר את התמונה המקורית עצמה, אלא לשחזר רק את הרעש שהתווסף לה - כך שניתן יהיה להסיר אותו."
  },
  {
    "type": "mc",
    "question": "מהו סוג פונקציית ה-loss שמשמשת לאימון מודל DDPM?",
    "options": [
      "Cross-entropy loss",
      "L1 loss",
      "L2 loss (Mean Squared Error)",
      "Kullback-Leibler divergence"
    ],
    "correctAnswerIndex": 2,
    "explanation": "החישוב מתבצע לפי L2 בין הרעש האמיתי לרעש החזוי. זהו ה-loss שבו משתמשים בפועל כדי לאמן את הרשת במודל DDPM."
  },
  {
    "type": "mc",
    "question": "מה תפקידו של הפרמטר $\\beta_t$ בתהליך ה-Forward Diffusion?",
    "options": [
      "לקבוע את איכות התמונה הסופית.",
      "לקבוע את עוצמת הרעש המתווספת בכל שלב.",
      "לשלוט במספר צעדי הדיפוזיה.",
      "להגדיר את קצב למידת המודל."
    ],
    "correctAnswerIndex": 1,
    "explanation": "בכל שלב $t$, התמונה $x_t$ נוצרת מהתמונה הקודמת $x_{t-1}$ על ידי הוספת רעש גאוסי. הרעש שנוסף בכל שלב נקבע לפי פרמטר $\\beta_t$, שמייצג את עוצמת הרעש. זהו מה שנקרא noise schedule."
  },
  {
    "type": "mc",
    "question": "כיצד מוכנס מידע הזמן ($t$) לרשת U-Net במודלי דיפוזיה?",
    "options": [
      "כקלט נוסף לשכבת הפלט של הרשת.",
      "על ידי המרת הסקלר $t$ לווקטור embedding דרך MLP והזרקתו לשכבות הרשת.",
      "באמצעות שינוי ארכיטקטורת הרשת עבור כל צעד זמן.",
      "הזמן אינו מוזן לרשת באופן ישיר."
    ],
    "correctAnswerIndex": 1,
    "explanation": "מאחר והרשת צריכה לדעת באיזה שלב בזמן היא פועלת (כלומר מהו ערך $t$), את המידע הזה יש 'להחדיר' פנימה. הדרך המקובלת לעשות זאת היא: (1) ממירים את הסקלר $t$ לווקטור embedding דרך MLP (2) מזריקים את הווקטור הזה לשכבות הרשת – לרוב לתוך ה־Residual Blocks או על ידי חיבור פשוט (addition) או באמצעות Adaptive Group Normalization (שיטה שמאפשרת לנרמל פיצ'רים תוך התחשבות בזמן)."
  },
  {
    "type": "mc",
    "question": "מהו ההבדל העיקרי בין DDPM ל-DDIM?",
    "options": [
      "DDPM משתמש במודל סיווג, DDIM לא.",
      "DDPM מבוסס על הנחת מרקוביות ודורש יותר צעדים; DDIM מסיר הנחה זו ומאפשר דגימה מהירה יותר.",
      "DDPM מייצר תמונות באיכות גבוהה יותר מ-DDIM.",
      "DDIM הוא מודל ישן יותר מ-DDPM."
    ],
    "correctAnswerIndex": 1,
    "explanation": "במודל DDPM, תהליך הדגימה מבוסס על הנחת מרקוביות – כלומר, כל שלב תלוי רק ב־$x_t$. מודל DDIM מרחיב את הגישה הזו על ידי הסרה של ההנחה המרקובית. למעשה, DDIM מאפשר ביצוע דגימה באיכות גבוהה גם עם מספר צעדים קטן, כמו 50 או 100 בלבד."
  },
  {
    "type": "mc",
    "question": "מה היתרון העיקרי של Latent Diffusion Models (LDM) בהשוואה למודלי דיפוזיה הפועלים במרחב הפיקסלים?",
    "options": [
      "איכות תמונה נמוכה יותר.",
      "זמן אימון ארוך יותר.",
      "חיסכון עצום בזמן חישוב ובדרישות זיכרון.",
      "חוסר יכולת לשלב תנאים חיצוניים (כמו טקסט)."
    ],
    "correctAnswerIndex": 2,
    "explanation": "היתרונות של LDM הינם חיסכון עצום בזמן חישוב ובדרישות זיכרון – מאפשר יצירת תמונות ברזולוציה גבוהה גם על חומרה מוגבלת."
  },
  {
    "type": "open",
    "question": "הסבר בקצרה מהי שיטת Classifier Guidance במודלי דיפוזיה מותנים.",
    "correctAnswerText": "Classifier Guidance משתמשת בנגזרת של מודל סיווג מאומן כדי לכוון את תהליך הדגימה של מודל הדיפוזיה, כך שהתמונה שנוצרת תהיה שייכת למחלקה הרצויה. במהלך הדגימה, מודל סיווג מקבל תמונות רועשות ומנבא את המחלקה, והנגזרת של הלוג-סבירות מתווספת לתהליך הדגימה כדי להטות אותו לכיוון המחלקה הרצויה.",
    "explanation": "השיטה מאפשרת שליטה מדויקת יותר על תוכן התמונה הנוצרת על ידי שילוב מידע ממסווג חיצוני."
  },
  {
    "type": "open",
    "question": "כיצד פועלת שיטת Classifier-Free Guidance במודלי דיפוזיה?",
    "correctAnswerText": "בשיטת Classifier-Free Guidance מאמנים מודל דיפוזיה אחד שמסוגל לעבוד גם עם תנאי (כמו טקסט או תווית) וגם בלעדיו. במהלך האימון, התנאי מסופק לפעמים ולפעמים מושמט. בתהליך הדגימה, מבצעים שני חישובים עבור כל שלב – אחד עם תנאי ואחד בלעדיו – ומשלבים אותם ליניארית כדי להגביר את ההשפעה של התיאור הרצוי על התמונה הסופית, ללא צורך בקלאסיפייר חיצוני.",
    "explanation": "השיטה מאפשרת שליטה על יצירת התמונה על בסיס תנאים חיצוניים (כמו טקסט) מבלי להזדקק למודל סיווג נפרד."
  },
  {
    "type": "mc",
    "question": "מהו 'Forward Diffusion' במודלי דיפוזיה?",
    "options": [
      "התהליך שבו המודל לומד להסיר רעש מהתמונה.",
      "התהליך שבו מוסיפים רעש לתמונה בשלבים עד שהיא הופכת לרעש גאוסי לחלוטין.",
      "התהליך שבו מייצרים תמונה חדשה מרעש טהור.",
      "תהליך אימון הרשת באמצעות L2 loss."
    ],
    "correctAnswerIndex": 1,
    "explanation": "Forward Diffusion שבו מוסיפים רעש לתמונה בשלבים עד שהיא הופכת לרעש גאוסי לחלוטין."
  },
  {
    "type": "mc",
    "question": "מהי מטרת ה-'skip connections' בארכיטקטורת U-Net המשמשת במודלי דיפוזיה?",
    "options": [
      "להפחית את מספר הפרמטרים במודל.",
      "לשפר את מהירות האימון.",
      "לאפשר זרימת גרדיאנטים טובה יותר ושמירה על פרטים.",
      "לצמצם את רזולוציית התמונה."
    ],
    "correctAnswerIndex": 2,
    "explanation": "רשת ה-U-Net בנויה ממבנה של Encoder שמבצע כיווץ (downsampling) ו-Decoder שמבצע הגדלה (upsampling), כאשר ביניהם קיימות קפיצות (skip connections) שמחברות שכבות בהתמרה דומה בין שני הצדדים. קישורים אלו מאפשרים זרימת גרדיאנטים טובה יותר ושמירה על פרטים."
  },
  {
    "type": "open",
    "question": "כיצד תהליך הדגימה במודלי דיפוזיה שונה בשלבים מוקדמים (t גבוה) ובשלבים מאוחרים (t נמוך)?",
    "correctAnswerText": "בשלבים הראשונים של הדגימה (t גבוה, קרוב לרעש מוחלט) הרשת מתמקדת בלימוד תדרים נמוכים, כלומר מבנה גס של התמונה: מיקומים, צבעים כלליים, צורת האובייקטים. בשלבים המאוחרים (t נמוך, קרוב לתמונה) היא מתמקדת בתדרים גבוהים, כלומר שחזור של פרטים חזותיים עדינים כמו טקסטורות, גבולות חדים וקווים קטנים.",
    "explanation": "היכולת הזו של המודל לשלוט על רמות שונות של תוכן לאורך זמן היא תכונה ייחודית של מודלי דיפוזיה."
  },
  {
    "type": "mc",
    "question": "מה מייצגת $\\epsilon_{\\theta}(x_t, t)$ במודל DDPM?",
    "options": [
      "התמונה המקורית $x_0$.",
      "הרעש שנוסף לתמונה $x_t$ ושהרשת מנסה לחזות.",
      "התוחלת של ההתפלגות הגאוסית.",
      "השונות בתהליך הסרת הרעש."
    ],
    "correctAnswerIndex": 1,
    "explanation": "הרשת מקבלת את $x_t$ ואת $t$, ומנסה לנחש את הרעש שנוסף ($\\epsilon$)."
  },
  {
    "type": "mc",
    "question": "כיצד Latent Diffusion Models (LDM) מאפשרים יצירה מותנית (Conditional Generation)?",
    "options": [
      "על ידי אימון מודל נפרד לכל תנאי.",
      "על ידי הוספת שכבה קונבולוציונית נוספת.",
      "התנאי מועבר דרך מקודד ייעודי (domain-specific encoder), והפלט שלו מוזרק לשלבים פנימיים של רשת הדיפוזיה באמצעות Cross-Attention.",
      "על ידי שימוש בארכיטקטורת U-Net בלבד."
    ],
    "correctAnswerIndex": 2,
    "explanation": "כדי לאפשר יצירה מותנית (כמו יצירת תמונה על בסיס טקסט), המודל תומך בתנאים כמו תיאור טקסטואלי, מסכת סגמנטציה, או ייצוג סמנטי אחר. התנאי מועבר דרך מקודד ייעודי (domain-specific encoder), והפלט שלו מוזרק לשלבים פנימיים של רשת הדיפוזיה באמצעות Cross-Attention."
  },
  {
    "type": "open",
    "question": "מהו ההבדל המהותי בין Self-Attention ל-Cross-Attention ב-Stable Diffusion?",
    "correctAnswerText": "ב-Self-Attention, השאילתות (Q), המפתחות (K) והערכים (V) מגיעים כולם מאותה מפת מאפיינים של התמונה – כלומר, המודל מסתכל פנימה על עצמו כדי למצוא קשרים פנימיים. ב-Cross-Attention, השאילתות (Q) נובעות מהתמונה, אך המפתחות (K) והערכים (V) נלקחים מהתנאי (למשל טקסט). זה מאפשר למודל להבין אילו מילים מהטקסט רלוונטיות לכל אזור בתמונה.",
    "explanation": "שניהם מנגנוני קשב חשובים בטרנספורמרים, אך הם משמשים למטרות שונות ביחס למקור המידע."
  },
  {
    "type": "mc",
    "question": "מדוע מודל DiT (Diffusion Transformer) משתמש בארכיטקטורת טרנספורמר במקום U-Net?",
    "options": [
      "כדי להפחית את מספר צעדי הדגימה.",
      "בגלל שהטרנספורמרים טובים יותר בטיפול בתמונות ברזולוציה נמוכה.",
      "בזכות היכולת להתרחב (scalability) ולטפל בקשרים רחוקים בצורה יעילה, כפי שהוכח ב-NLP ובראייה ממוחשבת.",
      "כדי להקל על תהליך ההרעשה הקדמי."
    ],
    "correctAnswerIndex": 2,
    "explanation": "המוטיבציה לשימוש בטרנספורמרים מגיעה מההצלחה הגדולה שלהם הן ב-NLP והן במשימות של ראייה ממוחשבת, במיוחד בזכות היכולת להתרחב (scalability) ולטפל בקשרים רחוקים בצורה יעילה."
  },
  {
    "type": "open",
    "question": "ציין שני יתרונות מרכזיים של מודלי DDIM בהשוואה ל-DDPM.",
    "correctAnswerText": "שני יתרונות מרכזיים של DDIM הם: (1) דגימה מהירה יותר פי 10 עד פי 100 בהשוואה ל-DDPM. (2) אפשרות לדגימה דטרמיניסטית (כאשר השונות שווה לאפס).",
    "explanation": "יתרונות אלו הופכים את DDIM ליעיל וגמיש יותר ליצירת תמונות."
  }
]
