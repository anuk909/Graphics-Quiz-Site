[
  {
    "type": "mc",
    "question": "מהי המטרה העיקרית של שיטת TokenFlow בעריכת וידאו?",
    "options": [
      "לייצר סרטונים חדשים מאפס על בסיס טקסט בלבד.",
      "לשמור על עקביות (consistency) בין פריימים ערוכים על ידי מעקב אחר פיצ'רים.",
      "להאיץ את תהליך ה-denoising במודלי דיפוזיה.",
      "ללמוד תנועה דינמית מאוסף סרטונים גדול."
    ],
    "correctAnswerIndex": 1,
    "explanation": "הבעיה המרכזית בעריכת וידאו פריים-אחר-פריים היא חוסר עקביות. TokenFlow פותר זאת על ידי חישוב שדה השכנים הקרובים (NN field) כדי לעקוב אחר פיצ'רים (tokens) בין פריימים, ולהבטיח שהעריכה המבוצעת על פריים מפתח תופץ באופן עקבי לשאר הפריימים."
  },
  {
    "type": "mc",
    "question": "במודל Tune-A-Video, מהי המשמעות של \"ניפוח\" (inflating) רשת UNet?",
    "options": [
      "הגדלת הרזולוציה המרחבית של התמונות.",
      "הוספת שכבות טמפורליות למודל תמונה קיים כדי שיוכל לעבד וידאו.",
      "הכפלת מספר הפרמטרים בכל שכבת קונבולוציה.",
      "שימוש ביותר רעש בתהליך הדיפוזיה."
    ],
    "correctAnswerIndex": 1,
    "explanation": "הרעיון ב-Tune-A-Video הוא לקחת מודל טקסט-לתמונה (T2I) מאומן מראש, ו\"לנפח\" אותו על ידי הוספת שכבות חדשות (Temporal Attention) שמאפשרות לו לעבד את המימד השלישי - זמן. כך, מודל תמונה הופך למודל וידאו."
  },
  {
    "type": "mc",
    "question": "מהי אחת המגבלות של שיטת Temporal Attention הפשוטה המוצגת ב-Tune-A-Video?",
    "options": [
      "היא אינה מאפשרת עריכה מבוססת טקסט.",
      "היא דורשת אימון מחדש של כל המודל.",
      "היא מניחה שכל פיקסל צריך לשים לב רק לעצמו בפריימים אחרים, מה שלא תמיד נכון בתנועה מורכבת.",
      "היא עובדת רק על סרטונים בשחור-לבן."
    ],
    "correctAnswerIndex": 2,
    "explanation": "המימוש הפשוט של Temporal Attention גורם לכל פיקסל במיקום (x,y) לשים לב רק לפיקסלים באותו מיקום (x,y) בפריימים אחרים. זו הנחה מגבילה, מכיוון שאובייקטים זזים, והתקווה היא שמנגנון ה-self-attention המרחבי יפצה על כך."
  },
  {
    "type": "mc",
    "question": "מהי מטרת האימון הדו-שלבי במודל AnimateDiff?",
    "options": [
      "לאמן תחילה על תמונות ואז על טקסטים.",
      "להפריד בין לימוד המראה הכללי (שמושפע לרעה מפריימים של וידאו) ולימוד התנועה.",
      "לייצר קודם פריימים ברזולוציה נמוכה ואז גבוהה.",
      "לאמן תחילה את ה-Encoder ואז את ה-Decoder."
    ],
    "correctAnswerIndex": 1,
    "explanation": "השלב הראשון מתאים את מודל התמונה לדאטה של פריימים מתוך וידאו (באמצעות LoRA), מה שעלול להכניס טשטוש. השלב השני מוסיף מודול תנועה. בזמן inference, משתמשים רק במודול התנועה ללא התאמת ה-LoRA, כדי ללמוד את התנועה אך להימנע מהירידה באיכות (טשטוש) שהוכנסה בשלב הראשון."
  },
  {
    "type": "mc",
    "question": "כיצד ארכיטקטורת STUNet במודל Lumiere שונה ממודלי UNet קודמים לעיבוד וידאו?",
    "options": [
      "היא משתמשת ב-Transformers במקום קונבולוציות.",
      "היא מעבדת כל פריים בנפרד לחלוטין.",
      "היא אינה משתמשת ב-skip connections.",
      "היא מצמצמת את מספר הפריימים (מימד הזמן) ככל שהרשת נעשית עמוקה יותר."
    ],
    "correctAnswerIndex": 3,
    "explanation": "בעוד שמודלים קודמים שמרו על מספר פריימים קבוע לאורך כל רשת ה-UNet, מודל Lumiere מציג את STUNet (Space-Time UNet) שמבצע דגימה מחדש (resizing) גם במימד הזמן, ומפחית את מספר הפריימים בשכבות העמוקות יותר. זה מאפשר עיבוד יעיל יותר של תנועה גלובלית."
  },
  {
    "type": "mc",
    "question": "מה הבעיה המרכזית ש-VideoJam מנסה לפתור?",
    "options": [
      "זמני ריצה ארוכים ביצירת וידאו.",
      "קושי בשימור זהות של אובייקט לאורך זמן.",
      "מודלי דיפוזיה נוטים להתמקד בטקסטורה וצבע, ולא תמיד לומדים תנועה רציפה וזורמת.",
      "איכות נמוכה של פריימים בודדים."
    ],
    "correctAnswerIndex": 2,
    "explanation": "התצפית המרכזית ב-VideoJam הייתה שמודל דיפוזיה סטנדרטי משיג loss דומה על וידאו רגיל ועל וידאו שהפריימים שלו עורבבו. זה מצביע על כך שהמודל לא נותן מספיק דגש לרצף התנועה. VideoJam מטפל בכך על ידי אילוץ המודל לחזות גם את הזרימה האופטית (Optical Flow)."
  },
  {
    "type": "mc",
    "question": "במודל Generative Omnimate, מה מייצג כל צבע ב-Trimask?",
    "options": [
      "לבן: אובייקט שיש להסיר; שחור: אובייקט שיש לשמר; אפור: רקע.",
      "לבן: אובייקט שיש לשמר; שחור: אובייקט שיש להסיר; אפור: רקע שהרשת מחליטה לגביו.",
      "לבן: רקע; שחור: אובייקט; אפור: צל.",
      "לבן: אזור ללא שינוי; שחור: אזור לעריכה; אפור: אזור למחיקה."
    ],
    "correctAnswerIndex": 1,
    "explanation": "ה-Trimask משמשת להנחיית המודל. לבן (ערך 1) מסמן אובייקט שחובה לשמר בסרטון הפלט. שחור (ערך 0) מסמן אובייקט שיש להסיר. אפור (ערך 0.5) מסמן את הרקע, והרשת צריכה ללמוד כיצד להשלים אותו (למשל, למלא חורים שנוצרו מהסרת אובייקטים)."
  },
  {
    "type": "mc",
    "question": "מהי המטרה של שלב ה-Omnimatte optimization ב-Generative Omnimate?",
    "options": [
      "לאמן את מודל הדיפוזיה הראשוני.",
      "לייצר את ה-Trimasks באופן אוטומטי.",
      "להפריד את האובייקטים מהרקע שלהם ולקבל שכבת RGBA נקייה עבור כל אובייקט.",
      "לשפר את הרזולוציה של סרטון הפלט."
    ],
    "correctAnswerIndex": 2,
    "explanation": "לאחר השלב הראשון שמייצר סרטונים של כל אובייקט בנפרד על רקע מלא (Solo videos), שלב האופטימיזציה נועד לחלץ מכל סרטון כזה את שכבת ה-RGBA של האובייקט (כלומר, הפיקסלים של האובייקט עצמו עם ערוץ שקיפות), על ידי פתרון משוואת שחזור שמפרידה בין האובייקט (foreground) לרקע הנקי (background)."
  },
  {
    "type": "mc",
    "question": "בפרסונליזציה של וידאו (Still Moving), מדוע שימוש ישיר במודל T2I שעבר פרסונליזציה בתוך מודל וידאו כמו Lumiere נכשל?",
    "options": [
      "Lumiere אינו תומך במודלים שעברו fine-tuning.",
      "מודל ה-T2I לא יודע לייצר תנועה.",
      "המשקולות הטמפורליות של מודל הווידאו לא אומנו על תמונות מהתפלגות הנתונים החדשה (של האובייקט שעבר פרסונליזציה).",
      "ישנה אי-התאמה ברזולוציות בין המודלים."
    ],
    "correctAnswerIndex": 2,
    "explanation": "המשקולות הטמפורליות (שאחראיות על התנועה) במודל הווידאו הורגלו לראות פיצ'רים מהתפלגות הנתונים המקורית. כאשר מחליפים את מודל התמונה במודל שעבר פרסונליזציה (למשל על כלב מסוים), הפיצ'רים שהוא מייצר שונים, והמשקולות הטמפורליות אינן יודעות כיצד ליצור מהם תנועה קוהרנטית, מה שמוביל לתוצאות גרועות."
  },
  {
    "type": "mc",
    "question": "בשיטת 'Still Moving', מה תפקידו של הפרמטר 'a'?",
    "options": [
      "לקבוע את אורך הסרטון.",
      "לקבוע את רמת הרעש ההתחלתית.",
      "לשלוט אם המודל צריך לייצר סרטון סטטי (עם תנועה קפואה) או סרטון דינמי.",
      "לווסת את מהירות התנועה בסרטון."
    ],
    "correctAnswerIndex": 2,
    "explanation": "הפרמטר 'a' משמש כבורר. כאשר a=1, המודל מאומן לייצר פלט סטטי (כל הפריימים זהים לתמונת הקלט). כאשר a=0, המודל משתמש ביכולות התנועה הרגילות שלו. זה מאפשר לאמן את המודל על תמונות מההתפלגות החדשה (במצב סטטי) מבלי לאבד את יכולת יצירת התנועה הכללית שלו."
  },
  {
    "type": "mc",
    "question": "מה ההבדל המרכזי בין Static Concepts ו-Dynamic Concepts בפרסונליזציה של וידאו?",
    "options": [
      "Static מתייחס לתמונות ו-Dynamic לסרטונים.",
      "Static הוא פרסונליזציה של מראה (appearance), בעוד ש-Dynamic הוא פרסונליזציה של מראה ותנועה גם יחד.",
      "Static משתמש ב-GANs ו-Dynamic ב-Diffusion Models.",
      "Static עובד רק על רקעים ו-Dynamic רק על אובייקטים."
    ],
    "correctAnswerIndex": 1,
    "explanation": "פרסונליזציה של קונספט סטטי עוסקת בלימוד המראה של אובייקט או סגנון מסוים. פרסונליזציה של קונספט דינמי היא משימה מורכבת יותר, שמטרתה ללמוד לא רק את המראה, אלא גם תנועה ספציפית (למשל, סגנון ריצה או ריקוד מסוים)."
  },
  {
    "type": "mc",
    "question": "בפרסונליזציה של Dynamic Concepts, מה המטרה של LoRA Set Encoding?",
    "options": [
      "ללמוד את סדר הפריימים הנכון.",
      "ללמוד את המראה (appearance) של האובייקט ממספר פריימים ללא התחשבות בסדר שלהם.",
      "לקודד את הטקסט המלווה לסרטון.",
      "ללמוד את התנועה (motion) של האובייקט."
    ],
    "correctAnswerIndex": 1,
    "explanation": "בשלב הראשון, LoRA Set Encoding, המודל לומד את המראה של האובייקט על ידי אימון על קבוצה (set) של פריימים מהסרטון, תוך התעלמות מהסדר שלהם. זה מאפשר למודל ללכוד את המאפיינים הוויזואליים של האובייקט מנקודות מבט שונות."
  },
  {
    "type": "mc",
    "question": "מהו תהליך DDIM Inversion כפי שהוא משמש ב-TokenFlow?",
    "options": [
      "תהליך שהופך טקסט לתמונה.",
      "תהליך שלוקח תמונה נקייה ומייצר סדרת רעשים שמתוכם ניתן לשחזר אותה.",
      "תהליך שמזהה אובייקטים בתמונה.",
      "תהליך שמגדיל את רזולוציית התמונה."
    ],
    "correctAnswerIndex": 1,
    "explanation": "תהליך הדיפוזיה הסטנדרטי מתחיל מרעש ומסיר אותו כדי לייצר תמונה. DDIM Inversion הוא התהליך ההפוך: הוא לוקח תמונה קיימת (פריים מהווידאו) ומחשב באופן דטרמיניסטי את וקטור הרעש הספציפי שיוביל לאותה תמונה בדיוק בתהליך ה-denoising. זהו צעד חיוני כדי 'להפוך' את הווידאו לרעשים שניתן לערוך."
  },
  {
    "type": "mc",
    "question": "במודל Tune-A-Video, מהו תפקיד ה-Q, K, V בשכבת ה-Temporal Attention?",
    "options": [
      "Q מהפריים הראשון, K,V מהפריים הנוכחי.",
      "Q, K, V כולם מהפריים הנוכחי.",
      "Q מהפריים הנוכחי, ו-K,V מהפריים הראשון והפריים הקודם.",
      "Q, K, V נלקחים באופן אקראי מכל הפריימים."
    ],
    "correctAnswerIndex": 2,
    "explanation": "כדי לשמור גם על המבנה של הפריים הנוכחי וגם על עקביות המראה מההתחלה, נבחר מבנה א-סימטרי: ה-Query (Q) נלקח מהפריים הנוכחי, בעוד שה-Key (K) וה-Value (V) נלקחים מהפריים הראשון (לשמירת מראה כללי) ומהפריים הקודם (לשמירת רצף תנועה)."
  },
  {
    "type": "mc",
    "question": "מהי הטענה המרכזית לגבי מודל Sora של OpenAI כפי שהוצגה בהרצאה?",
    "options": [
      "הוא משתמש בארכיטקטורת UNet משופרת.",
      "הוא מתמקד בפירוק סצנות לשכבות.",
      "הוא מבוסס על Diffusion Transformers בסקייל ענק, ללא ייעולים מיוחדים.",
      "הוא דורש אימון על סרטון בודד בלבד."
    ],
    "correctAnswerIndex": 2,
    "explanation": "ההרצאה מציינת בקצרה ש-Sora הוא דוגמה לגישה של 'סקייל' (scale). במקום להסתמך על ייעולים ארכיטקטוניים מורכבים, הוא משיג את התוצאות המרשימות שלו על ידי שימוש בארכיטקטורת Diffusion Transformers והרצתה על כמות עצומה של דאטה ומשאבי חישוב."
  },
  {
    "type": "mc",
    "question": "ב-AnimateDiff, מהי מטרת ה-Motion Module?",
    "options": [
      "להחליף לחלוטין את שכבות הקונבולוציה.",
      "להוסיף יכולות עיבוד טקסט למודל.",
      "להזריק יכולות של הבנת תנועה למודל תמונה קיים.",
      "לדחוס את הווידאו לגודל קטן יותר."
    ],
    "correctAnswerIndex": 2,
    "explanation": "ה-Motion Module, שהוא למעשה Temporal Transformer, הוא הרכיב ש-AnimateDiff מוסיף לארכיטקטורת מודל התמונה. מטרתו היא ללמוד 'motion priors' מדאטה-סט גדול של סרטונים, כך שניתן יהיה להרכיב אותו על כל מודל תמונה ולהפוך אותו למודל וידאו."
  },
  {
    "type": "mc",
    "question": "ב-VideoJam, כיצד הזרימה האופטית (Optical Flow) מקודדת כתמונה?",
    "options": [
      "הצבע מייצג את מהירות התנועה והשקיפות את הכיוון.",
      "הצבע מייצג את זווית התנועה (כיוון) והשקיפות/בהירות את גודל המהירות.",
      "הערוץ האדום מקודד תנועה בציר X והערוץ הירוק מקודד תנועה בציר Y.",
      "כל פיקסל מקבל ערך בינארי - 1 אם יש תנועה, 0 אם אין."
    ],
    "correctAnswerIndex": 1,
    "explanation": "הזרימה האופטית מקודדת באופן ויזואלי כך שהצבע בכל פיקסל מייצג את כיוון התנועה (זווית המהירות), והשקיפות (opacity) או הבהירות מייצגת את גודל וקטור המהירות (norm). ככל שהתנועה מהירה יותר, כך האזור כהה/שקוף פחות."
  },
  {
    "type": "mc",
    "question": "מהי מטרת שלב ה-(I) Joint editing ב-TokenFlow?",
    "options": [
      "לערוך את כל פריימי הווידאו בו-זמנית.",
      "לחשב את הזרימה האופטית בין הפריימים.",
      "לערוך מספר פריימי מפתח נבחרים תוך שימוש ב-attention משותף כדי לשמור על דמיון ביניהם.",
      "לבצע DDIM Inversion על פריימי המפתח."
    ],
    "correctAnswerIndex": 2,
    "explanation": "בשלב זה, לא עורכים את כל הסרטון. דוגמים מספר פריימי מפתח (keyframes), ועורכים רק אותם על פי הטקסט (למשל, \"colourful painting\"). כדי שהעריכה תהיה עקבית בין פריימי המפתח עצמם, משתמשים במנגנון attention מורחב/משותף (Extended Attention)."
  },
  {
    "type": "mc",
    "question": "מהי טכניקת ה-LoRA Sequence Encoding בפרסונליזציה של Dynamic Concepts?",
    "options": [
      "לימוד המראה הכללי של האובייקט.",
      "לימוד התנועה הספציפית של האובייקט על ידי אימון על רצף הפריימים.",
      "דחיסת רצף הפריימים לקובץ קטן יותר.",
      "קידוד של כל פריים בנפרד ללא קשר לאחרים."
    ],
    "correctAnswerIndex": 1,
    "explanation": "לאחר לימוד המראה (Set Encoding), שלב ה-Sequence Encoding מאמן LoRA נוסף על רצף הפריימים המלא. שלב זה נועד ללכוד את התנועה והדינמיקה הספציפית של האובייקט, כפי שהיא באה לידי ביטוי בסדר הכרונולוגי של הפריימים."
  },
  {
    "type": "mc",
    "question": "בארכיטקטורת Lumiere (STUNet), מהו תפקיד ה-Temporal Resizing?",
    "options": [
      "לשנות את קצב הפריימים (fps) של הסרטון הסופי.",
      "להקטין את מספר הפריימים במורד הרשת (downsampling) ולהגדיל אותו במעלה הרשת (upsampling).",
      "להתאים את גודל הסרטון לגודל ה-batch.",
      "לשנות את משך הזמן של הסרטון שנוצר."
    ],
    "correctAnswerIndex": 1,
    "explanation": "בדיוק כמו ש-Spatial Resizing (pooling) מקטין את הרזולוציה המרחבית, Temporal Resizing מקטין את הרזולוציה בזמן, כלומר את מספר הפריימים. פעולה זו מתבצעת בשכבות העמוקות של ה-UNet כדי לעבד תנועה ארוכת טווח באופן יעיל, והופכת לפעולת upsampling בזמן בצד המפענח."
  },
  {
    "type": "open",
    "question": "הסבר מדוע עריכת כל פריים בווידאו בנפרד באמצעות מודל דיפוזיה לתמונות היא גישה בעייתית, ומה הפתרון המרכזי ש-TokenFlow מציע.",
    "correctAnswerText": "הבעיה היא חוסר עקביות (consistency) בזמן. כל פריים מעובד באופן בלתי תלוי, ולכן שינויים קטנים ואקראיים בתהליך ה-denoising גורמים להבהובים (flickering) וחוסר רציפות בתנועה ובמראה בין פריימים סמוכים. הפתרון של TokenFlow הוא לשמור על עקביות על ידי חישוב התאמות בין פיקסלים (או פיצ'רים) בפריימים שונים. הוא מפיץ את העריכה שנעשתה על פריים מפתח אחד לפריימים אחרים על בסיס התאמות אלו, ובכך מבטיח שאובייקט שזז ישמור על מראה עקבי.",
    "explanation": "העיקרון הוא שלא מספיק לערוך כל תמונה יפה בפני עצמה, אלא יש צורך במנגנון שיקשר בין הפריימים וישמר את התכונות הוויזואליות של אובייקטים לאורך זמן. TokenFlow עושה זאת על ידי יצירת 'שדה זרימה' של טוקנים."
  },
  {
    "type": "open",
    "question": "מה ההבדל העקרוני בין הגישה של Tune-A-Video לזו של AnimateDiff להפיכת מודל תמונה למודל וידאו?",
    "correctAnswerText": "ההבדל המרכזי הוא בהיקף ובשיטת האימון. Tune-A-Video מבצע fine-tuning למשקולות הטמפורליות החדשות על סרטון בודד כדי להתאים אותו לפרומפט ספציפי (למשל, להפוך סרטון קיים לאנימציה). לעומת זאת, AnimateDiff לומד 'מודול תנועה' כללי מדאטה-סט גדול של סרטונים. מודול זה יכול לאחר מכן 'להתחבר' לכל מודל תמונה סטנדרטי (T2I) כדי להפוך אותו למודל וידאו גנרי, מבלי צורך ב-fine-tuning על סרטון ספציפי בזמן ההסקה (inference).",
    "explanation": "במילים פשוטות, Tune-A-Video הוא גישה של התאמה אישית (customization) לסרטון ספציפי, בעוד AnimateDiff הוא גישה של לימוד יכולת כללית (general capability) ליצירת תנועה שניתן להוסיף למודלים קיימים."
  },
  {
    "type": "open",
    "question": "הסבר כיצד VideoJam משתמש ב-Optical Flow כדי לשפר את איכות התנועה בסרטונים שהוא מייצר.",
    "correctAnswerText": "VideoJam מאמן את מודל הדיפוזיה לבצע משימה כפולה: בנוסף לשחזור (denoising) של פריימי הווידאו, עליו לחזות במקביל גם את ייצוג הזרימה האופטית (Optical Flow) בין הפריימים. מכיוון שהזרימה האופטית מקודדת באופן מפורש את כיוון ומהירות התנועה, המודל מוכרח ללמוד את רציפות התנועה כדי להצליח במשימת החיזוי שלה. הוספת ה-loss על חיזוי הזרימה האופטית מאלצת את המודל לשים יותר דגש על יצירת תנועה חלקה וקוהרנטית, ולא רק על טקסטורות וצבעים נכונים בכל פריים בודד.",
    "explanation": "על ידי הוספת משימה משנית שממוקדת כולה בתנועה, המודל לא יכול 'להתעלם' ממנה. הלוס על הזרימה האופטית רגיש מאוד לחוסר רציפות, מה שמאלץ את המודל ללמוד כיצד תנועה עובדת."
  },
  {
    "type": "open",
    "question": "בשיטת 'Still Moving', מדוע יש צורך לאמן את המודל על סרטונים סטטיים (שכל הפריימים בהם זהים) מההתפלגות החדשה?",
    "correctAnswerText": "כאשר מכניסים מודל T2I שעבר פרסונליזציה (למשל, על צעצוע מסוים), הפיצ'רים שהוא מייצר 'זרים' למשקולות הטמפורליות של מודל הווידאו. אין לנו סרטונים של הצעצוע הזה בתנועה כדי לאמן עליהם את המודל. הפתרון הוא ליצור 'סרטון סטטי' על ידי שכפול תמונת הצעצוע. אימון המודל על הסרטון הסטטי הזה (כאשר פרמטר a=1) מלמד את המשקולות המרחביות החדשות (Spatial Adapter) כיצד להתמודד עם הפיצ'רים של הצעצוע ולהפיק פלט יציב, מבלי 'להרוס' את יכולות התנועה הכלליות של המודל.",
    "explanation": "זהו טריק המאפשר למודל 'להתרגל' למראה של האובייקט החדש בסביבה מבוקרת (ללא תנועה), כך שבשלב היצירה הסופי הוא ידע לשלב את המראה הזה עם יכולות התנועה הכלליות שלמד מדאטה אחר."
  },
  {
    "type": "open",
    "question": "מהי מטרת תהליך הפירוק לשכבות ב-Generative Omnimate, ומהם שני התוצרים העיקריים שהוא מפיק עבור כל אובייקט?",
    "correctAnswerText": "המטרה היא לבצע 'video decomposition' - לפרק סרטון מורכב לרכיביו הבסיסיים, כך שניתן יהיה לערוך כל רכיב בנפרד ולשלבם מחדש. התוצרים העיקריים הם: 1. סרטון של הרקע הנקי (Clean Background) לאחר הסרת כל האובייקטים. 2. עבור כל אובייקט, שכבת RGBA המכילה את האובייקט עצמו עם ההשפעות שלו (כמו צל או השתקפות) וערוץ אלפא (שקיפות) המפריד אותו מהרקע.",
    "explanation": "במקום לערוך פיקסלים בסרטון המקורי, גישה זו מאפשרת עריכה סמנטית ברמה גבוהה יותר, כמו הזזת אובייקט, מחיקתו או שינוי הרקע מאחוריו, על ידי מניפולציה של השכבות הנפרדות."
  },
  {
    "type": "open",
    "question": "הסבר מהי ארכיטקטורת Cascade של מודלים בהקשר של יצירת וידאו, ומהו היתרון והחיסרון העיקריים שלה כפי שצוין בהרצאה.",
    "correctAnswerText": "ארכיטקטורת Cascade היא שרשרת של מספר מודלים הפועלים בזה אחר זה. בדרך כלל, מודל בסיס מייצר וידאו ברזולוציה וקצב פריימים נמוכים, וכל מודל עוקב בשרשרת (המכונה לעיתים SSR או TSR) לוקח את הפלט ומשפר אותו - מעלה את הרזולוציה המרחבית (Spatial Super Resolution) או את הרזולוציה בזמן (Temporal Super Resolution). היתרון הוא שכל מודל מתמחה במשימה פשוטה יותר, וקל יותר לאמן מספר מודלים קטנים מאשר מודל אחד ענק. החיסרון הוא שקשה להשלים פרטים וליצור תנועה מורכבת בין פריימי מפתח, במיוחד כשהפלט של השלב הראשון דליל מאוד.",
    "explanation": "זה כמו קו ייצור: העובד הראשון יוצר שלד גס, השני מוסיף פרטים, והשלישי צובע. כל שלב מתבסס על קודמו, מה שמפשט את העבודה אך יכול גם להגביל את התוצאה הסופית אם שלב מוקדם נעשה בצורה גרועה."
  },
  {
    "type": "open",
    "question": "במה שונה הגישה של TokenFlow לשימור עקביות מהגישה של Temporal Attention (במודל כמו Tune-A-Video)?",
    "correctAnswerText": "ההבדל המרכזי הוא באופן שבו העקביות נאכפת. TokenFlow היא שיטה המופעלת בזמן העריכה (at inference time) והיא מפורשת: היא מחשבת התאמות (correspondences) בין פיקסלים בפריימים שונים (באמצעות KNN על פיצ'רים) ומעתיקה באופן אקטיבי מידע כדי לשמר מראה. לעומת זאת, Temporal Attention היא שינוי ארכיטקטוני במודל עצמו שנלמד במהלך האימון. היא מאפשרת למודל ללמוד באופן מרומז (implicitly) לשים לב לאזורים רלוונטיים בפריימים אחרים כדי לייצר פלט עקבי. היא לא מחשבת 'זרימה' מפורשת אלא לומדת קשרים סטטיסטיים.",
    "explanation": "TokenFlow הוא כמו אנימטור שעוקב ידנית אחרי נקודות מפתח, בעוד Temporal Attention הוא כמו שחקן שלומד תנועה באופן אינטואיטיבי מניסיון. הראשון מפורש ודיסקרטי, השני נלמד ורציף."
  },
  {
    "type": "open",
    "question": "כיצד מודל AnimateDiff מפריד בין לימוד המראה ללימוד התנועה כדי להימנע מהשפעות שליליות?",
    "correctAnswerText": "ההפרדה נעשית באמצעות אימון דו-שלבי ושימוש ברכיבים נפרדים. בשלב הראשון, מתבצעת התאמה עדינה (fine-tuning) של מודל התמונה באמצעות LoRA על פריימים מתוך וידאו. שלב זה 'מלמד' את המודל על ההתפלגות של תמונות וידאו (שיכולה לכלול טשטוש תנועה), מה שנחשב כהשפעה שלילית. בשלב נפרד, מודול תנועה (Temporal Module) מאומן על דאטה-סט של סרטונים. בזמן היצירה (inference), משתמשים במודל התמונה המקורי (ללא ה-LoRA מהשלב הראשון) ומחברים אליו רק את מודול התנועה. כך, מנצלים את יכולת התנועה שנלמדה, אך נמנעים מהירידה באיכות שהייתה מתקבלת משימוש במשקולות שהותאמו לפריימי וידאו מטושטשים.",
    "explanation": "הרעיון הוא 'לבודד' את ההשפעה השלילית (לימוד טשטוש) לרכיב אחד (LoRA) ואת ההשפעה החיובית (לימוד תנועה) לרכיב אחר (Motion Module), ואז להשתמש רק ברכיב החיובי בזמן היצירה."
  },
  {
    "type": "open",
    "question": "מהי מטרת הפיצול בין LoRA Set Encoding ו-LoRA Sequence Encoding בפרסונליזציה של קונספטים דינמיים?",
    "correctAnswerText": "מטרת הפיצול היא להפריד בין לימוד המראה (appearance) ללימוד התנועה (motion) של אובייקט. השלב הראשון, 'Set Encoding', לומד את המראה על ידי התייחסות לפריימים כאל קבוצה לא סדורה של תמונות. זה מאפשר למודל ללכוד את המאפיינים הוויזואליים של האובייקט מזוויות שונות. השלב השני, 'Sequence Encoding', מתבצע לאחר מכן ולומד את התנועה על ידי אימון על רצף הפריימים הסדור. פיצול זה הופך את בעיית הלמידה לקלה יותר ומאפשר גמישות, למשל, לשלב מראה של אובייקט אחד עם תנועה של אובייקט אחר.",
    "explanation": "זהו עיקרון של 'הפרד ומשול'. במקום לנסות ללמוד מראה ותנועה מורכבת בו-זמנית, השיטה מפרקת את הבעיה לשתי תת-בעיות פשוטות יותר ופותרת אותן בזו אחר זו."
  },
  {
    "type": "open",
    "question": "הסבר מהי בעיית חוסר העקביות (inconsistency) בעריכת וידאו, ומדוע היא לא קיימת באותה חומרה בעריכת תמונה בודדת.",
    "correctAnswerText": "בעריכת תמונה בודדת, מודל דיפוזיה מתחיל מרעש ומייצר תמונה אחת. התוצאה הסופית היא סטטית ועקבית בתוך עצמה. לעומת זאת, בווידאו, יש רצף של תמונות (פריימים). אם נפעיל את מודל הדיפוזיה על כל פריים בנפרד, האקראיות הטבועה בתהליך (גם אם היא קטנה) תגרום לכך שהפלט עבור כל פריים יהיה מעט שונה, גם אם הקלט היה כמעט זהה. הבדלים זעירים אלו בצבע, בטקסטורה או בצורה של אובייקטים בין פריימים סמוכים נתפסים על ידי הצופה כהבהובים (flickering) או ריצודים, מה שפוגע קשות בחוויית הצפייה ויוצר תוצר לא מקצועי. זוהי בעיית העקביות בזמן.",
    "explanation": "המימד הנוסף - זמן - הוא שמוליד את הבעיה. בעוד שבתמונה אנו דורשים עקביות מרחבית, בווידאו אנו דורשים גם עקביות מרחבית וגם עקביות טמפורלית (לאורך זמן)."
  }
]
