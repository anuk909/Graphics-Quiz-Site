[
  {
    "type": "mc",
    "question": "מהי המטרה העיקרית של 'פרסונליזציה' בתחום הגרפיקה, בהקשר של מודלים כמו CLIP?",
    "options": [
      "ליצור מודלים גרפיים חדשים לחלוטין.",
      "לגרום למודל קיים לזהות ולהכיר אובייקטים או מושגים ספציפיים השייכים למשתמש.",
      "להגדיל את מהירות האימון של מודלי גרפיקה.",
      "לשפר את הרזולוציה של תמונות שנוצרו על ידי המודל."
    ],
    "correctAnswerIndex": 1,
    "explanation": "פרסונליזציה מתייחסת להתאמת מודל כללי, כמו CLIP, כך שיוכל לזהות או לייצג מושגים ספציפיים ואישיים, לדוגמה, איך נראה הכלב של המשתמש."
  },
  {
    "type": "mc",
    "question": "בתהליך הלמידה של ייצוג $W_c$ עבור קונספט חדש (לדוגמה, חד-קרן בשם פיפי), מהו השלב הראשון לפי השיטה המתוארת?",
    "options": [
      "הכנסת תמונות הקונספט ישירות למקודד טקסט (Text Encoder).",
      "אימון אנקודר כללי (general encoder) שממפה מרחב תמונות של CLIP למרחב ההטמעות של מקודד הטקסט.",
      "הזרקת ה- $W_c$ למקודד הטקסט וקבלת $z_{hat}$ ללא שלבים מקדימים.",
      "יישום פונקציית הפסד כדי שתמונות ייטמעו קרוב לייצוג טקסט חדש באופן מיידי."
    ],
    "correctAnswerIndex": 1,
    "explanation": "השלב הראשון כולל אימון אנקודר כללי שממפה תמונות ממרחב תמונת CLIP למרחב ההטמעות של מקודד הטקסט, כפי שמתואר בשקף המתאים."
  },
  {
    "type": "mc",
    "question": "מדוע נדרשים שני שלבים בתהליך הפרסונליזציה של ייצוג קונספט (למשל, כלב ספציפי), ולא מספיק השלב הראשון בלבד?",
    "options": [
      "השלב הראשון משמש רק לאיפוס המודל, והשני מבצע את הלמידה האמיתית.",
      "השלב הראשון מביא את המודל לאזור הכללי של הקונספט (לדוגמה, 'כלבים'), והשלב השני מכוונן אותו לקונספט הספציפי (לדוגמה, 'הכלב שלי').",
      "השלב השני מיועד רק לבדיקת תקינות של הוקטור מהשלב הראשון.",
      "אין מספיק תמונות של הקונספט הכללי, ולכן השלב הראשון נחוץ כדי לייצר יותר דאטה."
    ],
    "correctAnswerIndex": 1,
    "explanation": "השלב הראשון מאפשר למודל להבין את הקטגוריה הרחבה (לדוגמה, כלבים באופן כללי), ואילו השלב השני, עם כמות קטנה יותר של דאטה ספציפי, מאפשר לכייל את הוקטור לייצוג מדויק של האובייקט האישי."
  },
  {
    "type": "mc",
    "question": "איזו שיטה מערבת אופטימיזציה של וקטור קלט חדש ל-Text Encoder כדי שיעשה Condition על יצירת התמונה, במטרה ליצור תמונה של אובייקט ספציפי?",
    "options": [
      "Dreambooth",
      "Textual Inversion",
      "Perfusion",
      "Personalization Encoders"
    ],
    "correctAnswerIndex": 1,
    "explanation": "ב-Textual Inversion, מוצב וקטור חדש בקלט של הטקסט אשר עובר דרך מנגנון Cross Attention כדי להתנות את יצירת התמונה, והוא מותאם להיות דומה לתמונה הרצויה."
  },
  {
    "type": "mc",
    "question": "מהי הבעיה העיקרית בשיטת Dreambooth בהשוואה ל-Textual Inversion, וכיצד היא נפתרת?",
    "options": [
      "המודל שוכח מידע כללי; נפתר על ידי הוספת Loss רגיל שגורם לו לזכור כיצד לייצר אובייקטים באופן כללי.",
      "המודל מייצר תמונות באיכות נמוכה; נפתר על ידי הגדלת גודל המודל.",
      "האימון איטי מדי; נפתר על ידי שימוש בחומרת GPU חזקה יותר.",
      "המודל לא יכול להתמודד עם מספר אובייקטים בו-זמנית; נפתר על ידי חלוקת האובייקטים למספר מודלים."
    ],
    "correctAnswerIndex": 0,
    "explanation": "ב-Dreambooth מאמנים את כל הרשת, מה שעלול לגרום למודל לשכוח מידע כללי. כדי למנוע זאת, מוסיפים Loss רגיל שגורם לו לזכור איך לייצר אובייקטים כלליים."
  },
  {
    "type": "mc",
    "question": "מה ההבדל המהותי בין שיטות 'Inversion' לשיטות 'Fine Tuning' בהקשר של פרסונליזציה?",
    "options": [
      "Inversion מאמנות את המודל כולו, בעוד Fine Tuning לומדות טוקן מיוחד ללא אימון המודל.",
      "Inversion לומדות טוקן מיוחד ללא אימון המודל, בעוד Fine Tuning מאמנות את המודל (או חלקים ממנו).",
      "אין הבדל מהותי, רק שמות שונים לאותה פעולה.",
      "Inversion מיועדות רק לטקסט, ו-Fine Tuning מיועדות רק לתמונות."
    ],
    "correctAnswerIndex": 1,
    "explanation": "שיטות Inversion מתמקדות בלמידת טוקן ייחודי ללא שינוי מבנה המודל, בעוד ששיטות Fine Tuning כוללות אימון של המודל עצמו או חלקים ממנו."
  },
  {
    "type": "mc",
    "question": "מהי המטרה של שיטת 'Perfusion' וכיצד היא שונה מ-Inversion ומ-Fine Tuning?",
    "options": [
      "מטרתה לאמן את כל הרשת, בדומה ל-Fine Tuning, אך מהר יותר.",
      "מטרתה לאמן חלק קטן של הרשת באופן מקומי, תוך שמירה על רוב הרשת קפואה, בניגוד ל-Inversion שלא מאמנת כלל ול-Fine Tuning שמאמנת יותר.",
      "מטרתה ליצור תמונות ללא צורך בקלט טקסטואלי כלל.",
      "מטרתה למצוא פתרון סגור לבעיות אופטימיזציה ללא שימוש ברשתות נוירונים."
    ],
    "correctAnswerIndex": 1,
    "explanation": "Perfusion שואפת לשנות את הרשת באופן מקומי בלבד, כך שהשינוי ישפיע רק על מה שרצינו לשנות, ושונה מ-Inversion שבה אין אימון מודל ו-Fine Tuning שבה מאמנים חלקים גדולים יותר."
  },
  {
    "type": "mc",
    "question": "בשיטת 'Break a Scene', כיצד מתאפשרת פרסונליזציה של מספר אובייקטים מתוך תמונה אחת?",
    "options": [
      "על ידי אימון מודל נפרד לכל אובייקט.",
      "על ידי שימוש במנגנוני מיסוך (masking) המתבססים על מודל סגמנטציה, ויישום Loss רק על האזור הרלוונטי לאובייקט.",
      "על ידי הכנסת התמונה כולה ללא חלוקה למודל ה-CLIP.",
      "על ידי יצירת תמונות מרובות של כל אובייקט בנפרד."
    ],
    "correctAnswerIndex": 1,
    "explanation": "בשיטת Break a Scene, נמצאות המסכות של האובייקטים באמצעות מודל סגמנטציה, וכאשר מייצרים תמונה של אובייקט ספציפי, ה-Loss מוחל רק על האזור של אותו אובייקט בתוך התמונה."
  },
  {
    "type": "mc",
    "question": "מהי הבעיה שנצפתה ב'Break a Scene' לאחר האימון, ומה היה הפתרון האינטואיטיבי אליה?",
    "options": [
      "המודל ייצר תמונות מטושטשות; הפתרון היה שימוש במודלי High-Resolution.",
      "המודל ייצר תמונות של כמה אובייקטים בלי שהתבקש; הפתרון היה לאלץ את מפות ה-Attention להתמקד רק באזורים הרצויים לאובייקט שלנו.",
      "האימון ארך זמן רב מדי; הפתרון היה להשתמש בחומרת GPU מתקדמת יותר.",
      "המודל לא הצליח לזהות את האובייקטים; הפתרון היה להגדיל את כמות נתוני האימון."
    ],
    "correctAnswerIndex": 1,
    "explanation": "הבעיה הייתה שלעיתים המודל יצר תמונות עם אובייקטים שלא נתבקשו, והפתרון היה לאלץ את מפות ה-Attention להתמקד רק באזורים הרצויים לאובייקט הספציפי."
  },
  {
    "type": "mc",
    "question": "מהי המטרה של Personalization Encoders, וכיצד הם מנסים לפתור את בעיית האיטיות של שיטות קודמות?",
    "options": [
      "לאמן אנקודר שיקבל את התמונה ויוציא ישירות את הטוקן הרלוונטי, במקום תהליך אופטימיזציה ארוך.",
      "לשפר את איכות התמונה הסופית על ידי הוספת שכבות חדשות למודל.",
      "להתאים את המודל למספר דומיינים בו-זמנית ללא צורך בפרסונליזציה.",
      "להקטין את גודל המודל כדי שיוכל לרוץ על חומרה חלשה יותר."
    ],
    "correctAnswerIndex": 0,
    "explanation": "Personalization Encoders נועדו לאמן אנקודר שמקבל תמונה ומייצר את הטוקן הרלוונטי באופן ישיר, במקום לבצע תהליך אופטימיזציה איטרטיבי, ובכך לזרז את התהליך."
  },
  {
    "type": "mc",
    "question": "מהי הבעיה העיקרית עם Personalization Encoders שגורמת להם להיות מוגבלים לדומיין אחד בלבד?",
    "options": [
      "הם דורשים כמות עצומה של נתוני אימון לכל דומיין חדש.",
      "הם מוגבלים ביכולתם להכליל למספר דומיינים שונים.",
      "הם לא מסוגלים לייצר תמונות ברזולוציה גבוהה מספיק.",
      "הם אינם תומכים בפרסונליזציה של מספר אובייקטים בו-זמנית."
    ],
    "correctAnswerIndex": 1,
    "explanation": "הבעיה הגדולה של Personalization Encoders היא שהאנקודר לא יכול להכליל למספר דומיינים והוא עובד רק עבור דומיין אחד בכל פעם."
  },
  {
    "type": "mc",
    "question": "מהו היתרון המרכזי של 'Tuning-Free Encoders' כמו IP-Adapter?",
    "options": [
      "הם דורשים אימון מקדים ממושך ועתיר משאבים.",
      "הם אינם דורשים כלל אימון (tuning) לאחר ההכנה, אלא רק Forward Pass.",
      "הם מיועדים רק לדומיינים ספציפיים ואינם ניתנים להכללה.",
      "הם מייצרים תמונות באיכות נמוכה יותר משיטות אחרות."
    ],
    "correctAnswerIndex": 1,
    "explanation": "Tuning-Free Encoders, כמו IP-Adapter, מצטיינים בכך שהם אינם דורשים אימון נוסף לאחר ה-Forward Pass, מה שהופך אותם ליעילים יותר מבחינת זמן ומשאבי מחשוב."
  },
  {
    "type": "mc",
    "question": "ב-IP-Adapter, כיצד נמנעים מ-Overfitting ומאפשרים גמישות רבה יותר, במקום להסתבך עם מנגנון ה-Cross-Attention הקיים?",
    "options": [
      "על ידי הסרת כל מנגנוני ה-Attention מהמודל.",
      "על ידי הוספת מנגנון Cross-Attention נוסף ומנותק (Decoupled Cross-Attention).",
      "על ידי איפוס כל המשקלים של המודל לפני כל שימוש.",
      "על ידי הקפאת כל שכבות המודל ומניעת כל שינוי בהן."
    ],
    "correctAnswerIndex": 1,
    "explanation": "ב-IP-Adapter, מתווסף מנגנון Cross-Attention נוסף, מנותק מהקיים, כדי למנוע Overfitting ולאפשר גמישות רבה יותר."
  },
  {
    "type": "mc",
    "question": "מהו 'Diffusion Transformer' וכיצד הוא משלב Self-Attention במודל דיפוזיה?",
    "options": [
      "מודל דיפוזיה שאינו משתמש ב-Self-Attention כלל.",
      "מודל דיפוזיה המשלב Self-Attention בתוך הבלוקים של מודל הדיפוזיה, עם אסימוני טקסט ותמונה.",
      "מודל טרנספורמר המשמש ליצירת תמונות ללא תהליך דיפוזיה.",
      "מודל המבצע רק סגמנטציה של תמונות."
    ],
    "correctAnswerIndex": 1,
    "explanation": "Diffusion Transformer משלב מנגנון Self-Attention בתוך הבלוקים של מודל הדיפוזיה, תוך שימוש באסימוני טקסט ותמונה."
  },
  {
    "type": "mc",
    "question": "מהו היתרון העיקרי של מנגנון 'Shared SA' (Shared Self-Attention) ב-OmniControl?",
    "options": [
      "הוא מקטין את כמות הזיכרון הנדרשת באופן דרמטי.",
      "הוא מאפשר למודל להתחשב גם במפתחות (keys) של תמונת ההתניה, בנוסף למפתחות של אסימוני הטקסט, ליצירת ייצוג עשיר יותר.",
      "הוא מונע את הצורך בשימוש באנקודר תמונה.",
      "הוא מבטל את הצורך בתהליך הדיפוזיה כולו."
    ],
    "correctAnswerIndex": 1,
    "explanation": "Shared SA מאפשר להוציא מפתחות גם עבור תמונת ההתניה, ובכך להתחשב בה, בנוסף לאסימוני הטקסט, מה שמוביל לייצוג עשיר יותר ומדויק יותר."
  },
  {
    "type": "mc",
    "question": "מהו המאפיין המרכזי של 'Nested Attention' ביחס ל-Normal Attention?",
    "options": [
      "ב-Nested Attention, כל שאילתה (query) מקבלת וקטור ערך (value) ייחודי, במיוחד עבור טוקנים חשובים, בניגוד ל-Normal Attention שבו כל השאילתות חולקות את אותו וקטור ערך.",
      "ב-Nested Attention, כל וקטורי הערך זהים לכל השאילתות, בעוד ב-Normal Attention הם שונים.",
      "Nested Attention מיועד רק לעיבוד טקסט, ו-Normal Attention לעיבוד תמונה.",
      "Nested Attention אינו משתמש במנגנוני Cross-Attention כלל."
    ],
    "correctAnswerIndex": 0,
    "explanation": "ב-Nested Attention, בניגוד ל-Normal Attention, כל שאילתה מקבלת וקטור ערך ייחודי, במיוחד עבור טוקנים ספציפיים וחשובים, מה שמאפשר התאמה מדויקת יותר של התמונה לטקסט."
  },
  {
    "type": "mc",
    "question": "מהי המטרה של 'Consistent Generation'?",
    "options": [
      "ליצור תמונה אחת באיכות גבוהה במיוחד.",
      "ליצור N תמונות שונות לחלוטין מ-K אובייקטים משתנים.",
      "ליצור קבוצה של N תמונות שבהן K אובייקטים חוזרים על עצמם נשמרים באופן עקבי ונראים זהים בכל התמונות.",
      "לשנות את מראה האובייקטים בתמונות רבות ככל האפשר."
    ],
    "correctAnswerIndex": 2,
    "explanation": "Consistent Generation שואפת ליצור קבוצה של N תמונות שבהן אובייקטים חוזרים על עצמם (K subjects) נשמרים בעקביות ונראים זהים בכל התמונות."
  },
  {
    "type": "mc",
    "question": "כיצד 'Consistent Generation' משיגה שיתוף מידע בין תמונות מרובות, ובאיזה שלב זה מתרחש?",
    "options": [
      "על ידי אימון נפרד של כל תמונה ולאחר מכן איחודן.",
      "על ידי מנגנון Shared Self-Attention המאפשר שיתוף מידע בין כל התמונות יחד, וכל זה קורה בזמן ה-Inference ללא אימון נוסף.",
      "על ידי שימוש באנקודרים נפרדים לכל תמונה.",
      "על ידי הקפאת כל שכבות המודל ומניעת שינויים בהן."
    ],
    "correctAnswerIndex": 1,
    "explanation": "Consistent Generation משתמשת במנגנון Shared Self-Attention כדי לשתף מידע בין התמונות, והתהליך כולו מתרחש בזמן ה-Inference ללא צורך באימון נוסף."
  },
  {
    "type": "mc",
    "question": "מהי הבעיה שנצפתה ב'Consistent Generation' ללא מיסוך, וכיצד היא נפתרת?",
    "options": [
      "התמונות נוצרו באיכות נמוכה; נפתרה על ידי הגדלת רזולוציית הפלט.",
      "היה שיתוף יתר של מידע בין אובייקטים שונים, מה שהוביל לאובייקטים כפולים או לא רצויים; נפתרה על ידי שימוש ב-Masking באמצעות מנגנון Cross-Attention.",
      "המודל לא הצליח לזהות את האובייקטים בתמונה; נפתרה על ידי שימוש באנקודר חזק יותר.",
      "היה צורך ביותר מדי נתוני אימון; נפתרה על ידי הקטנת כמות הדאטה."
    ],
    "correctAnswerIndex": 1,
    "explanation": "הבעיה הייתה שלעיתים היה שיתוף יתר של מידע בין תמונות, מה שהוביל ליצירת אובייקטים נוספים או לא רצויים. הפתרון היה להשתמש במיסוך באמצעות מנגנון Cross-Attention, כדי לשתף מידע רק באזורים הרלוונטיים."
  },
  {
    "type": "mc",
    "question": "כיצד 'Anchoring' משפרת את עקביות האובייקטים בתמונות חדשות שנוצרות לאחר תמונות ראשוניות?",
    "options": [
      "היא מונעת יצירה של תמונות נוספות לאחר תמונות ראשוניות.",
      "היא דורשת אימון מחדש של המודל על כל תמונה חדשה שנוצרת.",
      "היא יוצרת כמה תמונות ראשוניות ('Anchors') שרואות זו את זו ונוצרות יחד, ולאחר מכן משתמשת בהן כבסיס ליצירת תמונות נוספות של אותם אובייקטים, תוך שמירה על עקביות ללא אימון נוסף.",
      "היא משתמשת באנקודר חיצוני כדי לוודא עקביות בכל פעם מחדש."
    ],
    "correctAnswerIndex": 2,
    "explanation": "Anchoring יוצרת מספר תמונות 'עוגן' ראשוניות שנוצרות יחד ורואות זו את זו. לאחר מכן, תמונות נוספות של אותם אובייקטים נוצרות על בסיס העוגנים הללו, ובכך נשמרת עקביות ללא צורך באימון נוסף."
  },
  {
    "type": "open",
    "question": "הסבר מהי 'פרסונליזציה' בהקשר של מודלי גרפיקה, ותן דוגמה.",
    "correctAnswerText": "פרסונליזציה בתחום הגרפיקה היא תת-תחום שמטרתו לגרום למודל גדול, כמו CLIP, להכיר את עולם התוכן הספציפי של המשתמש. לדוגמה, ללמד את המודל איך נראה הכלב הספציפי של המשתמש, או חפץ אישי כמו חצאית מסוימת.",
    "explanation": "המטרה היא לייצר ייצוג $W_c$ שישקף את הקונספט האישי, ובכך לאפשר למודל לייצר תמונות או לזהות אובייקטים בצורה מותאמת אישית."
  },
  {
    "type": "open",
    "question": "תיאר בקצרה את שני השלבים העיקריים באימון ייצוג $W_c$ עבור קונספט חדש, ומדוע שניהם נחוצים.",
    "correctAnswerText": "שלב ראשון: אימון אנקודר כללי (general encoder) שממפה תמונות ממרחב תמונת CLIP למרחב ההטמעות של מקודד הטקסט, תוך יצירת Loss מעגלי כדי שהטקסט יהיה קרוב גם לתמונות של CLIP. שלב שני: אופטימיזציה נוספת של הוקטור (שעלול להיות כללי מהשלב הראשון) באמצעות מקודד הטקסט, תוך דרישה שיהיה קרוב לתמונות של האובייקט הספציפי ורחוק ממשפט כללי. השלב הראשון מביא את הייצוג לאזור הכללי של הקונספט (לדוגמה, 'כלבים'), והשני מכוונן אותו לקונספט הספציפי (לדוגמה, 'הכלב שלי'), וזאת משום שאין מספיק תמונות של האובייקט הספציפי לאימון ישיר.",
    "explanation": "שני השלבים מאפשרים לנצל את הידע הכללי של המודל בתחילה, ולאחר מכן לבצע כיוונון עדין על בסיס דאטה מוגבל יותר, ובכך להשיג פרסונליזציה יעילה."
  },
  {
    "type": "open",
    "question": "מה ההבדל העיקרי בין Textual Inversion ל-Dreambooth בהקשר של אימון ופרסונליזציה?",
    "correctAnswerText": "ב-Textual Inversion, אנו שמים וקטור חדש בקלט של הטקסט, אשר עובר ב-Cross Attention כדי לבצע Conditioning על יצירת התמונה, ומבצעים אופטימיזציה רק לוקטור זה ללא שינוי הרשת. ב-Dreambooth, מבצעים את אותה פעולה עם הוקטור הקבוע, אך מאמנים את כל הרשת של Diffusion U-Net. הבעיה ב-Dreambooth היא שהמודל עלול 'לשכוח' את הידע הכללי שלו, ולכן מוסיפים Loss רגיל שגורם לו לזכור איך לייצר אובייקטים באופן כללי.",
    "explanation": "ההבדל המרכזי הוא באימון: Textual Inversion מקפיאה את המודל ומאמנת רק טוקן חדש, בעוד Dreambooth מאמנת את המודל כולו (עם תוספת Loss לשמירה על ידע כללי)."
  },
  {
    "type": "open",
    "question": "הסבר מהי שיטת 'Perfusion' וכיצד היא מתמודדת עם הבעיות של Inversion ו-Fine Tuning.",
    "correctAnswerText": "Perfusion היא שיטה המשלבת אלמנטים מ-Inversion ומ-Fine Tuning. היא שואפת לשנות את הרשת באופן מקומי בלבד, כך שהשינוי ישפיע רק על מה שרצינו לשנות (לדוגמה, עבור טוקן ספציפי) תוך כדי שמירה על שאר הרשת ללא שינוי ככל האפשר. היא מנוסחת כבעיית אופטימיזציה מאולצת עם פתרון סגור (מטריצה עם Rank 1), ובכך מציעה גודל מודל קטן יחסית (כ-1MB) ומאפשרת עריכה טובה יותר ומספר נושאים, תוך ניסיון להתמודד עם ה-Identity-Editability Tradeoff שנמצא ב-Inversion והגודל העצום של Fine Tuning.",
    "explanation": "Perfusion מציעה פתרון ביניים המאפשר שינויים מקומיים ויעילים במודל, תוך שמירה על גמישות ועקביות טובה יותר."
  },
  {
    "type": "open",
    "question": "תאר את הרעיון מאחורי 'Break a Scene' לפרסונליזציה של מספר אובייקטים בתמונה אחת, וכיצד היא פותרת את בעיית יצירת האובייקטים הלא רצויים.",
    "correctAnswerText": "הרעיון ב-Break a Scene הוא לפרק תמונה עם מספר אובייקטים לחלקים, ולאפשר פרסונליזציה של כל אחד מהם בנפרד. זה נעשה על ידי מציאת המסכות של האובייקטים באמצעות מודל סגמנטציה. כאשר מייצרים תמונה של אובייקט ספציפי, ה-Loss מוחל רק על האזור של אותו אובייקט. כדי לפתור את הבעיה של יצירת אובייקטים לא רצויים (כאשר המודל ייצר מספר אובייקטים ללא בקשה), הפתרון היה לאלץ את מפות ה-Attention להתמקד רק באזורים הנכונים והמעניינים לאובייקט הספציפי, באמצעות Cross-Attention Loss.",
    "explanation": "השיטה משלבת סגמנטציה ומיסוך עם שליטה על ה-Attention כדי להשיג פרסונליזציה מדויקת של אובייקטים מרובים."
  },
  {
    "type": "open",
    "question": "מהי הגישה של 'Personalization Encoders' כדי לזרז את תהליך הפרסונליזציה, ומהי הבעיה העיקרית איתם?",
    "correctAnswerText": "Personalization Encoders נועדו לזרז את תהליך הפרסונליזציה על ידי אימון אנקודר שמקבל תמונה (Ic) ומוציא ישירות את הטוקן הרלוונטי (S*), במקום לבצע תהליך אופטימיזציה איטרטיבי. הבעיה העיקרית היא שלמרות שהם מתחילים מייצוג כללי (לדוגמה, 'חתול'), ישנם פתרונות רבים והם עלולים לצאת מההתפלגות של ה-Word Embedding. כדי למנוע זאת, דורשים שהשינויים שהמודל מוציא יהיו קטנים, אך עדיין האנקודר לא יכול להכליל למספר דומיינים ועובד רק לדומיין אחד בכל פעם.",
    "explanation": "הגישה מנסה להחליף אופטימיזציה ארוכה בפעולה ישירה של אנקודר, אך מוגבלת ביכולת ההכללה שלה לדומיינים שונים."
  },
  {
    "type": "open",
    "question": "הסבר מהו 'Tuning-Free Encoder' כמו IP-Adapter, וציין יתרון וחיסרון אחד שלו.",
    "correctAnswerText": "Tuning-Free Encoder, כמו IP-Adapter, הוא אנקודר שאינו דורש כלל אימון (tuning) לאחר ההכנה, אלא רק Forward Pass. הוא מוסיף מנגנון Cross-Attention נוסף (Decoupled Cross-Attention) כדי להימנע מ-Overfitting ולהיות גמיש יותר. יתרון: אינו Domain Specific, כלומר, הוא יכול להכליל למגוון רחב של דומיינים. חיסרון: קשה לו להתאים הן לטקסט והן לתמונה הרצויה בו-זמנית, מה שעלול לפגוע באיכות התוצאה הסופית.",
    "explanation": "היתרון העיקרי הוא היעילות והגמישות ללא צורך באימון, אך החיסרון הוא לעיתים קושי בשילוב מדויק של מידע טקסטואלי וויזואלי."
  },
  {
    "type": "open",
    "question": "מהו 'Nested Attention' וכיצד הוא שונה מ-Normal Attention? כיצד הוא תורם לשיפור יצירת תמונות?",
    "correctAnswerText": "ב-Normal Attention, כל השאילתות (queries) חולקות את אותו וקטור ערך (value). לעומת זאת, ב-Nested Attention, כל שאילתה מקבלת וקטור ערך ייחודי משלה, במיוחד עבור טוקנים חשובים כמו '<person>'. הדבר מאפשר למודל לקבל מידע ספציפי ומותאם אישית מהתמונה המקורית עבור כל פיקסל רלוונטי בטוקן. לדוגמה, פיקסלים המייצגים את העין של אדם בתמונה הסופית יקבלו מידע מדויק מפיקסלי העין בתמונת הקלט. זה תורם ליצירת תמונות מדויקות ועקביות יותר עם הקונספט המוזכר בטקסט, ומאפשר למודל להתאים את התמונה הסופית בצורה מדויקת יותר.",
    "explanation": "Nested Attention משפרת את דיוק יצירת התמונות על ידי התאמת ערכי ה-Attention באופן פרטני לכל שאילתה, במיוחד עבור אובייקטים חשובים בטקסט."
  },
  {
    "type": "open",
    "question": "הסבר את הרעיון של 'Consistent Generation' ואת היתרון הגדול שלו. מהי הבעיה שנצפתה בו בהתחלה וכיצד היא נפתרה?",
    "correctAnswerText": "Consistent Generation מתמודדת עם הבעיה של יצירת קבוצה של N תמונות שבהן K אובייקטים חוזרים על עצמם נשמרים באופן עקבי ונראים זהים בכל התמונות. היתרון הגדול הוא שזו שיטה ללא אימון כלל, והכול מתרחש בזמן ה-Inference. הבעיה שנצפתה הייתה שלפעמים היה יותר מדי שיתוף מידע, מה שהוביל ליצירת אובייקטים נוספים או כפולים בתמונות ללא בקשה. הפתרון היה להשתמש ב-Masking באמצעות מנגנון Cross-Attention, כדי לוודא ששיתוף המידע מתרחש רק באזורים הרלוונטיים שבהם התמונה אמורה להיווצר.",
    "explanation": "Consistent Generation מציעה פתרון יעיל לעקביות בין תמונות, ללא צורך באימון מחדש, ומתמודדת עם בעיות שיתוף יתר על ידי שליטה על מנגנוני ה-Attention."
  },
  {
    "type": "open",
    "question": "מהו 'Anchoring' בהקשר של יצירת תמונות עקביות, וכיצד הוא פועל?",
    "correctAnswerText": "Anchoring היא טכניקה ב-Consistent Generation שמטרתה לשמור על עקביות של אובייקטים בתמונות רבות שנוצרות בזו אחר זו. היא פועלת כך: תחילה, מייצרים כמה תמונות 'עוגן' (Anchors) אשר רואות זו את זו ונוצרות יחד, מה שמבטיח עקביות ראשונית. לאחר מכן, כאשר רוצים לייצר תמונות נוספות של אותם אובייקטים, הן מתבססות על המידע שנלמד מהעוגנים הללו. היתרון הגדול של Anchoring הוא שהיא פועלת ללא אימון נוסף כלל, והכול קורה בזמן ה-Inference, ובכך מאפשרת ייצור עקבי של אובייקטים במצבים שונים.",
    "explanation": "Anchoring מאפשרת הרחבת יצירת תמונות עקביות של אובייקטים לאורך זמן, תוך שימוש במערך ראשוני של תמונות עוגן וללא צורך באימון."
  }
]
