[
  {
    "type": "mc",
    "question": "מהי המשימה שמגדירים כ-Image-to-Image Translation?",
    "options": [
      "המרת וידאו לרצף של תמונות רציפות בזמן",
      "מיפוי תמונה מתחום מקור לתחום יעד תוך שמירת משמעות ומבנה",
      "זיהוי אובייקטים בתמונה באמצעות רשת ייעודית",
      "דחיסת תמונות באובדן מידע מינימלי"
    ],
    "correctAnswerIndex": 1,
    "explanation": "בתרגום תמונה-לתמונה רוצים לשנות סגנון או מאפיינים של התמונה ועדיין לשמור על התוכן והגאומטריה המקוריים."
  },
  {
    "type": "mc",
    "question": "מהו ההבדל העיקרי בין Supervised ל-Unsupervised Image Translation?",
    "options": [
      "ב-Supervised יש זוגות תמונות תואמים וב-Unsupervised אין התאמה ישירה",
      "ב-Supervised מאמנים GAN בלבד וב-Unsupervised מאמנים AutoEncoder בלבד",
      "ב-Supervised משתמשים רק בפונקציית L2 וב-Unsupervised רק ב-L1",
      "ב-Supervised עובדים ללא Data Augmentation וב-Unsupervised עם Augmentation אינטנסיבי"
    ],
    "correctAnswerIndex": 0,
    "explanation": "גישה מונחית נשענת על זוגות קלט-פלט ידועים, בעוד שבגישה בלתי-מונחית יש רק אוספים נפרדים משני תחומים."
  },
  {
    "type": "mc",
    "question": "מדוע מוסיף מודל U-Net קישורי Skip בין Encoder ל-Decoder?",
    "options": [
      "להגדיל את עומק הרשת ללא שינוי בזיכרון",
      "לשמר מידע מרחבי שאובד בדחיסה ולאפשר שחזור פרטים מדויקים",
      "לאכוף רגולריזציה על משקולות ה-Decoder בלבד",
      "להחליף צורך ב-Batch Normalization בשכבות העמוקות"
    ],
    "correctAnswerIndex": 1,
    "explanation": "ה-Skip Connections מחזירים מפות אקטיבציה ברזולוציה גבוהה, כך שה-Decoder משחזר מיקום וטקסטורה שאבדו."
  },
  {
    "type": "mc",
    "question": "למה פונקציות L1/L2 בתרגום תמונה-לתמונה נוטות ליצור תוצאות מטושטשות?",
    "options": [
      "הן גורמות לרשת להתמקד רק בפרטים עתירי-תדר",
      "הן מחשבות ממוצע פיקסל-לפיקסל ולכן מתכנסות לפתרון ממוצע של כל האפשרויות התקפות",
      "הן אינן גזירות ולכן הגרדיאנט מתאפס בשכבות מוקדמות",
      "הן מניחות שכל התמונות בסולם אפור ולא בצבע מלא"
    ],
    "correctAnswerIndex": 1,
    "explanation": "כאשר קיימים פתרונות רבים, ממוצע ליניארי של פיקסלים מוביל לטשטוש במקום לבחירת חדות ייחודית."
  },
  {
    "type": "mc",
    "question": "Perceptual Loss מודד הבדלים בין תמונות על ידי…",
    "options": [
      "השוואת היסטוגרם צבע גלובלי בכל תמונה ישירות",
      "חישוב פער בין מפות אקטיבציה פנימיות של רשת מאומנת מראש כמו VGG",
      "מדידת ממוצע ערכי ה-RGB בכל שכבת קונבולוציה",
      "ספירת מספר האובייקטים שזוהו בכל תמונה"
    ],
    "correctAnswerIndex": 1,
    "explanation": "במקום להשוות פיקסלים, מחשבים מרחק ב-feature space של רשת סיווג, המתיישר עם תפיסת חדות אנושית."
  },
  {
    "type": "mc",
    "question": "ב-Pix2Pix הן ה-Generator והן ה-Discriminator מקבלים את תמונת הקלט כתנאי כדי…",
    "options": [
      "להפחית את מספר שכבות ה-GAN לחצי",
      "להבטיח שהפלט קשור ספציפית לקלט ולא רק נראה ריאליסטי כללי",
      "לאפשר אימון ללא פונקציות רגולריזציה נוספות",
      "לאפס את הצורך בלוס מסוג L1"
    ],
    "correctAnswerIndex": 1,
    "explanation": "ה-Discriminator בודק זוג (קלט, פלט) ולכן דורש שה-Generator ישמור התאמה המבנית לקלט."
  },
  {
    "type": "mc",
    "question": "PatchGAN Discriminator ב-Pix2Pix פועל על חלונות קטנים כדי…",
    "options": [
      "ללמוד תדרים נמוכים בלבד",
      "להתמקד בפרטים מקומיים עתירי-תדר ולחסוך פרמטרים",
      "להחליף את הצורך ב-Cycle Consistency",
      "לשלוט בזיכרון מטמון של ה-GPU בזמן תרגום"
    ],
    "correctAnswerIndex": 1,
    "explanation": "בדיקה מקומית (למשל ‎70×70‎) מדגישה טקסטורה וחדות ומאפשרת רשת קטנה ויעילה."
  },
  {
    "type": "mc",
    "question": "באיזה Trade-off מאזנים ב-Pix2Pix בין שני רכיבי הפסד?",
    "options": [
      "בין L2 ל-Dropout כדי לשמר צבעים",
      "בין L1 לשכבות Pooling כדי לשלוט בגודל התמונה",
      "בין L1 להתחייבות Adversarial כדי לשמור מבנה ולשפר חדות",
      "בין KL Divergence ל-FID לצורך יציבות"
    ],
    "correctAnswerIndex": 2,
    "explanation": "L1 שומר על התאמה מבנית; Adversarial מחדד פרטים. משקלי-הפסד קובעים את האיזון."
  },
  {
    "type": "mc",
    "question": "CycleGAN משתמש בשני Generators ושני Discriminators כדי…",
    "options": [
      "להקטין את זמן האימון הכולל בחצי",
      "לאפשר תרגום דו-כיווני בין שני תחומים בלתי-מתואמים",
      "למחוק צורך במערך נתונים גדול",
      "להגביל את המספר הכולל של משקולות ברשת"
    ],
    "correctAnswerIndex": 1,
    "explanation": "כל Generator ממיר כיוון אחד, וה-Cycle Consistency מחייב שחזור חזרה לתמונה המקורית."
  },
  {
    "type": "mc",
    "question": "Cycle Consistency Loss ב-CycleGAN מבטיח ש…",
    "options": [
      "ה-Discriminator לא יראה את תמונת הקלט",
      "המרה הלוך-חזור תשחזר את התמונה המקורית בצורה מינימלית בהפסד",
      "ה-Generator יפיק בדיוק את אותן תמונות בכל איטרציה",
      "ה-PatchGAN יתמקד רק בתדרים נמוכים"
    ],
    "correctAnswerIndex": 1,
    "explanation": "הקריטריון מעודד את הרשת לשמר תוכן וגאומטריה, כי החזרה לתחום המקור חייבת להיות מדויקת."
  },
  {
    "type": "mc",
    "question": "הנחת Shared Latent Space ב-UNIT גורסת ש…",
    "options": [
      "לשני תחומים אין שום תכונה משותפת",
      "קיימת הפצה לטנטית משותפת שאליה ניתן למפות תמונות משני תחומים שונים",
      "כל Encoder פועל במרחב לטנטי נפרד ללא חפיפה",
      "רק אחד מה-Decoders משתמש ב-Discriminator"
    ],
    "correctAnswerIndex": 1,
    "explanation": "המודל מניח שניתן לקודד תמונות משני התחומים ל-z דומה, ואז לפענח כל תחום עם Decoder ייעודי."
  },
  {
    "type": "mc",
    "question": "MUNIT מפריד במפורש בין…",
    "options": [
      "גרדיאנטים חיוביים ושליליים בכל שכבה",
      "קוד תוכן יציב וקוד סגנון ניתן להחלפה",
      "ישויות GAN ודאטה אמיתית",
      "תדרים נמוכים ותדרים בינוניים בלבד"
    ],
    "correctAnswerIndex": 1,
    "explanation": "הפרדה זו מאפשרת דגימת סגנונות שונים עבור אותו תוכן וכך הפקת מגוון תרגומים."
  },
  {
    "type": "mc",
    "question": "ב-MUNIT שלב Cross-Domain Translation מבוצע על ידי…",
    "options": [
      "שילוב קוד התוכן המקורי עם קוד סגנון מדומיין מהתחום היעד",
      "החזרת פיקסלים זהים מהתמונה המקורית אל הפלט",
      "החלפת Encoder ו-Decoder בין התחומים ללא שינוי קוד",
      "הגדלת רזולוציה לפני כל מעבר תחום"
    ],
    "correctAnswerIndex": 0,
    "explanation": "שומרים את התוכן ומזריקים סגנון חדש כדי ליצור גרסה שייכת לתחום היעד אך עם אותו מבנה."
  },
  {
    "type": "mc",
    "question": "גישת Image Analogies מבצעת התאמת סגנון באמצעות…",
    "options": [
      "חישוב מאפייני Gram Matrix עמוק",
      "התאמת טלאים (Patch Matching) בין זוג תמונות דוגמה",
      "GAN מותנה עם U-Net בפלט",
      "הקטנת צבעים למרחב YIQ בלבד"
    ],
    "correctAnswerIndex": 1,
    "explanation": "השיטה הקלאסית עוד לפני רשתות, מוצאת טלאים דומים ומעבירה עיבוד דומה מתמונה A לתמונה B."
  },
  {
    "type": "mc",
    "question": "בשיטת העברת סגנון באמצעות רשתות נוירונים להעברת סגנון, הסגנון מיוצג על-ידי…",
    "options": [
      "שכבות Fully Connected ברשת",
      "מטריצת גרם של מפת האקטיבציות בשכבות רדודות",
      "הפרש פיקסלים מרובע בין שתי תמונות",
      "וקטור יחיד במרחב הלטנטי של GAN"
    ],
    "correctAnswerIndex": 1,
    "explanation": "קורלציית פילטרים (Gram) תופסת טקסטורה וצבע ללא תלות במיקום."
  },
  {
    "type": "mc",
    "question": "בתהליך Style Transfer של העברת סגנון באמצעות רשתות נוירונים העדכון מתבצע על…",
    "options": [
      "משקולות הרשת המאומנת מראש",
      "פיקסלי התמונה החדשה עצמה",
      "קוד הלטנט ברשת AutoEncoder",
      "משקולות ה-Discriminator בלבד"
    ],
    "correctAnswerIndex": 1,
    "explanation": "התמונה החדשה היא הפרמטר האופטימיזציוני; משקלי VGG נשארים קבועים."
  },
  {
    "type": "mc",
    "question": "Alpha ובטא ב-Style Transfer בחישוב ההפסד הבא: L_total(p,a,x)= αL_content(p,x)+ βL_style(a,x) שולטים ב...",
    "options": [
      "קצב למידה ומספר איטרציות",
      "משקל יחסי בין הפסד תוכן לפסד סגנון",
      "גודל חלונות ה-PatchGAN",
      "כמות הזוגות בהנחת Shared Latent Space"
    ],
    "correctAnswerIndex": 1,
    "explanation": "בחירה באלפא גדול מדגישה תוכן; בטא גדול מדגישה סגנון."
  },
  {
    "type": "mc",
    "question": "PatchGAN אופייני משתמש בגודל ‎70×70‎ כדי…",
    "options": [
      "לכנות את כל התמונה כ-Patch יחיד",
      "לכסות פרטים עתירי-תדר ברזלוציה מוקומית אפקטיבית",
      "להבטיח שה-GAN יתכנס תמיד אחרי 70 אפוקים",
      "להפחית את הצורך ב-Cycle Consistency Loss"
    ],
    "correctAnswerIndex": 1,
    "explanation": "חלון כזה מאזן בין טקסטורה מקומית למספיק הקשר, מה שמוכיח יעילות בתרגום."
  },
  {
    "type": "mc",
    "question": "איזה רכיב בפונקציית Loss של Pix2Pix אחראי לחדות הפרטים?",
    "options": [
      "L1 המבוסס פיקסל-לפיקסל",
      "Adversarial Loss מול ה-Discriminator",
      "Smoothness Constraint על גרדיאנט מרחבי",
      "KL Divergence אל הפצה סטנדרטית"
    ],
    "correctAnswerIndex": 1,
    "explanation": "ה-Discriminator מעניש טקסטורה מלאכותית ומעודד יצירת תדרים גבוהים ריאליסטיים."
  },
  {
    "type": "mc",
    "question": "ב-Style Transfer, שכבות עמוקות יותר ברשת VGG משמשות למדידת…",
    "options": [
      "סגנון בלבד כי הן רגישות לטקסטורה עדינה",
      "תוכן ומבנה סמנטי של האובייקטים בתמונה",
      "הפרשי בהירות בין פיקסלים סמוכים",
      "שכיחות רעש גאוסי דק"
    ],
    "correctAnswerIndex": 1,
    "explanation": "השכבות הגבוהות למדו ייצוגים ברמת אובייקט, לכן המרחק בהן משקף הבדל בתוכן."
  },
  {
    "type": "mc",
    "question": "חיסרון עיקרי של שיטת העברת סגנון באמצעות רשתות נוירונים בזמן ריצה הוא…",
    "options": [
      "חייבת חיבור לאינטרנט לאחזור מודל",
      "דורשת אופטימיזציה מחדש עבור כל תמונה, ולכן איטית בזמן אמת",
      "מסיקה רק על תמונות בגודל ‎224×224‎",
      "מחייבת נתוני אימון זוגיים לכל סגנון"
    ],
    "correctAnswerIndex": 1,
    "explanation": "מאחר ולא מאמנים רשת Feed-Forward אלא מבצעים גרדיאנט על הפיקסלים, התהליך ארוך."
  },
  {
    "type": "mc",
    "question": "איזה תדרי-תמונה מטופלים בעיקר על-ידי רכיב L1 ב-Pix2Pix?",
    "options": [
      "תדרים גבוהים בלבד",
      "תדרים נמוכים שמייצגים מבנה גלובלי",
      "תדרי ביניים סביב ‎1 kHz‎",
      "הרכיב לא תלוי בתדר כלל"
    ],
    "correctAnswerIndex": 1,
    "explanation": "השוואת פיקסל-לפיקסל מגיבה בעיקר למבנה ולערכים ממוצעים ולכן פותרת הבדלים בתדירות נמוכה."
  },
  {
    "type": "open",
    "question": "הסבר בקצרה כיצד Skip Connections ב-U-Net משפרים תרגום תמונה-לתמונה.",
    "correctAnswerText": "ה-Skip Connections מעבירים מפות אקטיבציה ברזולוציה גבוהה ישירות ל-Decoder, כך שה-Decoder מקבל מידע מרחבי מדויק שאבד בכיווץ, ומשחזר קצוות ומבנים קטנים ביתר חדות.",
    "explanation": "ללא החיבור, ה-latent הדחוס מאבד מיקום; צירוף הדרך הקצרה מחזיר את הפרטים בזמן ה-Upsampling."
  },
  {
    "type": "open",
    "question": "מדוע PatchGAN Discriminator חסכוני יותר מדיסקרימינטור גלובלי מלא?",
    "correctAnswerText": "ה-PatchGAN בודק בלוקים קטנים ולכן צריך הרבה פחות פילטרים ושכבות כדי לכסות את כל התמונה; אותו מסנן מוחל קונבולוציונית על כל מקום, מה שחוסך משקולות וזיכרון ומאפשר טיפול בתמונות גדולות ללא הגדלת הרשת.",
    "explanation": "על-ידי חלון נייח, הגודל של הרשת אינו תלוי ברזולוציה של התמונה אלא בגודל ה-Patch."
  },
  {
    "type": "open",
    "question": "מהו היתרון של שילוב Perceptual Loss עם L1 בתרגום מפוקח?",
    "correctAnswerText": "Perceptual Loss דואג שהפלט יהיה דומה אנושית במרחב-פיצ'רים, ול-L1 מבטיח התאמה במיקום פיקסלים. השילוב מונע טשטוש וגם שומר על פרטים מבניים מדויקים בו-זמנית.",
    "explanation": "כל רכיב מפצה על חסרונות השני: L1 לבדו מטשטש, Perceptual לבדו עלול שלא לשמר יישור גאומטרי."
  },
  {
    "type": "open",
    "question": "כיצד Cycle Consistency Loss עוזר ב-Unsupervised Translation לשמור על זהות אובייקטים?",
    "correctAnswerText": "אם תמונה מומרת מתחום A לתחום B ואז חזרה, הדרישה שהשחזור יהיה זהה למקור מאלצת את הרשת לא למחוק או לשנות פרטי תוכן חיוניים, ולכן זהותו ומיקומו של כל אובייקט נשמרים לאורך שני המעברים.",
    "explanation": "ללא אילוץ זה, ה-Generator עלול להפיק פלט סגנוני תקין אך לא קשור לתמונה המקורית."
  },
  {
    "type": "open",
    "question": "הסבר בקצרה את הנחת Shared Latent Space במודל UNIT.",
    "correctAnswerText": "UNIT מניח שקיימת הפצה סמויה משותפת שבה תמונות משני התחומים יכולות לקבל ייצוג זהה; לכן ניתן לקודד תמונה מכל תחום לאותו z ואז לפענח בעזרת Decoder של התחום השני לקבלת תרגום עקבי.",
    "explanation": "הנחה זו מחליפה צורך בזוגות נתונים תואמים כי היא מגדירה \"נקודת מפגש\" סמנטית לשני התחומים."
  },
  {
    "type": "open",
    "question": "כיצד MUNIT מאפשר יצירת מספר בלתי-מוגבל של תרגומים עבור אותה תמונה?",
    "correctAnswerText": "MUNIT שומר על קוד התוכן הקבוע ומדגם בכל פעם קוד סגנון חדש מההתפלגות של תחום היעד; ערבוב התוכן עם סגנונות שונים יוצר וריאציות רבות שאין להן גבול תאורטי.",
    "explanation": "ההפרדה המפורשת בין תוכן וסגנון מאפשרת קומבינציות רבות של אותו מבנה עם סגנונות משתנים."
  },
  {
    "type": "open",
    "question": "מה התפקיד של Gram Matrix בחישוב Style Loss בשיטת העברת סגנון באמצעות רשתות נוירונים?",
    "correctAnswerText": "Gram Matrix מודדת קורלציות בין פילטרים בשכבה, ולכן מתאר אילו תכונות ויזואליות מופיעות יחד. התאמת הגרמים בין תמונת הסגנון לתמונה החדשה מבטיחה שהטקסטורה, הצבע והמרקם יועתקו ללא קשר למיקום.",
    "explanation": "קורלציה תופסת סטטיסטיקה גלובלית של תבניות, מה שמספיק לשחזור \"תחושה\" סגנונית."
  },
  {
    "type": "open",
    "question": "מדוע Alpha גבוה ובטא נמוך מעניקים תוצאה עם פחות סגנון ויותר תוכן ב-Style Transfer? \n(L_total(p,a,x)= αL_content(p,x)+ βL_style(a,x))",
    "correctAnswerText": "Alpha שולט במשקל Loss התוכן; ערך גבוה גורם לאופטימיזציה להעדיף שימור מבנה. בטא נמוך מפחית חשיבות הסגנון ולכן הסגנון מוטמע בעדינות בלבד.",
    "explanation": "היחס Alpha:Beta מכתיב איזו שגיאה \"יקרה\" יותר באופטימיזציה על הפיקסלים."
  },
  {
    "type": "open",
    "question": "תאר תופעת Blurriness ב-Pix2Pix כשמגדילים את משקל L1 יתר על המידה.",
    "correctAnswerText": "כאשר L1 דומיננטי, המודל נענש על כל סטייה פיקסלית ולכן מתכנס לפתרון ממוצע של כל דוגמאות היעד האפשריות; הממוצע של סגנונות רבים הוא מטושטש וחסר פרטים חדים.",
    "explanation": "Adversarial Loss מתמקד בתדרים גבוהים שה-L1 מתעלם מהם; בלעדיו איכות הטקסטורה נפגעת."
  },
  {
    "type": "open",
    "question": "הסבר כיצד PatchGAN מקל על תרגום תמונות ברזולוציה גבוהה מבלי להגדיל את הדיסקרימינטור.",
    "correctAnswerText": "ה-Discriminator הוא רשת קונבולוציה קטנה שמופעלת לגמרי קונבולוציונית על כל מיקום בתמונה, ולכן מספר הפרמטרים קבוע. גודל התמונה משפיע רק על מספר ה-Patches שנבדקים אך לא על גודל הרשת.",
    "explanation": "השימוש בחלון מקומי וב-Stride מאפשר סריקה יעילה של תמונות גדולות ללא גדילה בזיכרון."
  },
  {
    "type": "open",
    "question": "מדוע שיטת העברת סגנון באמצעות רשתות נוירונים אינה מתאימה לשימוש בזמן-אמת?",
    "correctAnswerText": "השיטה מבצעת אופטימיזציה ישירה על פיקסלי התמונה החדשה, תהליך שדורש חישוב מחדש לכל תמונה ולכן אינו מתאים לזמן אמת",
    "explanation": "במקום לעדכן את משקלי הרשת כמו באימון רגיל, השיטה מבצעת Gradient Descent על פיקסלים של תמונה רנדומלית עד שהיא תשלב את התוכן והסגנון הרצויים. תהליך זה כולל חישובים רבים עבור כל תמונה מחדש ואינו יעיל למכשירים עם משאבים מוגבלים או ליישומים בזמן אמתץ לסיכום השיטה מאפשרת שליטה מדויקת על איזון בין תוכן לסגנון, אך דורשת חישוב מחדש לכל תמונה ולכן אינה מתאימה לעבודה בזמן אמת"
  },
  {
    "type": "mc",
    "question": "איזו פונקציית הפסד בפיקסלים מטפלת בעיקר בתדרים הנמוכים של התמונה בתרגום Pix2Pix?",
    "options": [
      "Adversarial Loss",
      "Cycle Consistency Loss",
      "L1 Loss פיקסל-לפיקסל",
      "Perceptual Loss מבוסס VGG"
    ],
    "correctAnswerIndex": 2,
    "explanation": "השוואת פיקסל-לפיקסל (L1) מעגנת את המבנה הגלובלי והצבעים – תדרים נמוכים – ומונעת שינויי מיקום."
  },
  {
    "type": "mc",
    "question": "באיזו מהרשתות הבאות נדרש זוג תמונות תואם (paired data) לאימון?",
    "options": [
      "Pix2Pix",
      "CycleGAN",
      "UNIT",
      "MUNIT"
    ],
    "correctAnswerIndex": 0,
    "explanation": "Pix2Pix הוא תרגום מונחה ודורש זוגות קלט-פלט כדי ללמוד התאמה ישירה."
  },
  {
    "type": "mc",
    "question": "איזה רכיב בפונקציית ההפסד של CycleGAN מחייב את המודל לשמר מבנה תוכן?",
    "options": [
      "PatchGAN Discriminator Loss",
      "Style Loss",
      "Cycle Consistency Loss",
      "Shared Latent Loss"
    ],
    "correctAnswerIndex": 2,
    "explanation": "הפסד עקיבות מעגלית מבטיח שתמונה תוחזר כמעט זהה למקור לאחר תרגום הלוך-חזור."
  },
  {
    "type": "mc",
    "question": "הנחת Shared Latent Space במודל UNIT גורסת כי…",
    "options": [
      "לכל תחום יש מרחב לטנטי ייחודי שאינו חופף",
      "קיימת הפצה לטנטית משותפת לשני התחומים",
      "רק ה-Decoders חולקים משקולות זהות",
      "Encoder אחד מספיק לשני התחומים יחד"
    ],
    "correctAnswerIndex": 1,
    "explanation": "UNIT מניח שתמונות משני תחומים ניתנות לייצוג באותו z וכך ניתן לתרגם ביניהן."
  },
  {
    "type": "mc",
    "question": "ב-MUNIT, אילו קודי ייצוג נשמרים ונדגמים בנפרד?",
    "options": [
      "קוד תוכן וקוד סגנון",
      "קוד צבע וקוד בהירות",
      "קוד עומק וקוד טקסטורה",
      "קוד רעש וקוד פיקסל"
    ],
    "correctAnswerIndex": 0,
    "explanation": "הפרדת Content ו-Style מאפשרת שילוב תוכן קבוע עם סגנונות מגוונים."
  },
  {
    "type": "mc",
    "question": "ב-CycleGAN, כמה Discriminators קיימים ומה תפקידם?",
    "options": [
      "אחד, לבחון זוגות קלט-פלט בו-זמנית",
      "שניים, אחד לכל תחום כדי לבדוק שהתוצאה נראית אמיתית בתחום היעד",
      "שלושה, לשני תחומים ולמרחב הלטנטי",
      "אף לא אחד – CycleGAN משתמש רק ב-L1"
    ],
    "correctAnswerIndex": 1,
    "explanation": "כל תחום מחזיק Discriminator משלו שבודק אם הפלט שייך להתפלגות התמונות באותו תחום."
  },
  {
    "type": "mc",
    "question": "מהו החיסרון המרכזי של שיטת Style Transfer של Gatys בזמן אמת?",
    "options": [
      "דורשת רשת גדולה ב-Inference",
      "מתבצעת אופטימיזציה איטית על פיקסלים לכל תמונה חדשה",
      "תומכת רק בתמונות בגודל ‎224×224‎ בדיוק",
      "זקוקה לזוגות תמונות תואמים"
    ],
    "correctAnswerIndex": 1,
    "explanation": "הגרדיאנט יורד על הפיקסלים במאות איטרציות ולכן אינו מתאים ליישומי זמן-אמת."
  },
  {
    "type": "mc",
    "question": "מטריצת גרם בשיטות העברת סגנון מקודדת…",
    "options": [
      "ערך ממוצע של כל פיקסל בשכבה",
      "קורלציות בין פילטרים באותה שכבה",
      "הפרשי בהירות בין שכבות עוקבות",
      "שונות מרחבית בין ערוצי צבע"
    ],
    "correctAnswerIndex": 1,
    "explanation": "הקורלציות משקפות אילו תכונות ויזואליות מופיעות יחד ובכך מייצגות טקסטורה."
  },
  {
    "type": "mc",
    "question": "Perceptual Loss מבוסס VGG נועד בעיקר…",
    "options": [
      "לשמור דיוק פיקסל-לפיקסל גבוה יותר",
      "למנוע טשטוש על-ידי השוואה במרחב פיצ'רים עמוק",
      "להחליף את Adversarial Loss ב-GANs",
      "להקטין את גודל הקלט הנדרש לרשת"
    ],
    "correctAnswerIndex": 1,
    "explanation": "מדידת מרחק ב-feature space של רשת מאומנת מראש מתיישרת טוב יותר עם תפיסת חדות אנושית."
  },
  {
    "type": "mc",
    "question": "איזה יתרון מרכזי מספק PatchGAN מעבר לחיסכון פרמטרים?",
    "options": [
      "פועל רק על תדרים נמוכים ומפחית חדות",
      "רמת דיוק גבוהה בהערכת טקסטורה עתירת-תדר",
      "מאפשר תרגום תמונות ללא צורך ב-Skip Connections",
      "מבטל את הצורך ב-L1 Loss"
    ],
    "correctAnswerIndex": 1,
    "explanation": "ה-Discriminator המקומי מתמקד בפרטים חדים ומעודד יצירת תדרים גבוהים ריאליסטיים."
  },
  {
    "type": "mc",
    "question": "U-Net מוסיף Skip Connections כדי…",
    "options": [
      "להקטין את עומק הרשת",
      "להחזיר מידע מרחבי מדויק ל-Decoder",
      "להחליף פונקציות הפעלה ב-Encoder",
      "למנוע לחלוטין שימוש ב-BatchNorm"
    ],
    "correctAnswerIndex": 1,
    "explanation": "ה-Decoder משחזר פרטים קטנים הודות למפות אקטיבציה ברזולוציה גבוהה שמוזרמות מה-Encoder."
  },
  {
    "type": "mc",
    "question": "איזה שילוב משקלי הפסד ב-Pix2Pix יוביל בדרך כלל לתוצאות חדות אך עלול לאבד מבנה?",
    "options": [
      "L1 גבוה, Adversarial נמוך",
      "L1 נמוך, Adversarial גבוה",
      "L1 = 0, Adversarial בלבד",
      "Adversarial = 0, L1 בלבד"
    ],
    "correctAnswerIndex": 1,
    "explanation": "Adversarial חזק מדגיש טקסטורה חדה אך אם L1 חלש המבנה עלול להיפגע."
  },
  {
    "type": "mc",
    "question": "ב-MUNIT ניתן ליצור מספר בלתי-מוגבל של תרגומים כי…",
    "options": [
      "מקודדים סגנון רנדומלי בכל איטרציה",
      "ה-Generator מכיל רעש לבן חופשי",
      "אין Discriminator הבודק איכות",
      "התוכן עצמו משתנה יחד עם הסגנון"
    ],
    "correctAnswerIndex": 0,
    "explanation": "דגימת קוד סגנון אקראי מהתחום היעד ובזיווגו עם תוכן קבוע מניבה וריאציות רבות."
  },
  {
    "type": "mc",
    "question": "איזו שיטה קדמה לעידן הרשתות ופתרה בעיות סגנון באמצעות Patch Matching קלאסי?",
    "options": [
      "Image Analogies",
      "UNIT",
      "AdaIN",
      "Fast Neural Style"
    ],
    "correctAnswerIndex": 0,
    "explanation": "Image Analogies מחפשת טלאים דומים בין תמונות הדוגמה ומעבירה את העיבוד לטלאים החדשים."
  },
  {
    "type": "mc",
    "question": "מה גורם לטשטוש (blurriness) כאשר מסתמכים רק על L2 או L1 בתרגום תמונה-לתמונה?",
    "options": [
      "רשת קטנה מדי שלא לומדת טקסטורה",
      "המודל מתכנס לפתרון ממוצע של כל הפתרונות האפשריים",
      "חוסר באוגמנטציית נתונים צבעונית",
      "שימוש ב-Skip Connections ללא BatchNorm"
    ],
    "correctAnswerIndex": 1,
    "explanation": "שגיאת ממוצע פיקסלי מעדיפה פתרון ביניים שמחליק פרטים חדים."
  },
  {
    "type": "mc",
    "question": "מה תפקידו של Discriminator בתרגום מותנה (Conditional GAN) כמו Pix2Pix?",
    "options": [
      "להבחין אם תמונת פלט כלשהי נראית מציאותית באופן כללי",
      "להבחין אם זוג (קלט, פלט) תואם להתפלגות זוגות האמת",
      "להפיק קוד סגנון עבור ה-Generator",
      "למדוד רק את ההבדל הפיקסלי בין תמונות"
    ],
    "correctAnswerIndex": 1,
    "explanation": "תנאי הקלט מאפשר לדיסקרימינטור לבדוק התאמה בין הפלט לקלט ולא רק ריאליזם כללי."
  },
  {
    "type": "mc",
    "question": "ב-CycleGAN, איזה רכיב ארכיטקטוני ממיר תמונות מתחום B חזרה לתחום A?",
    "options": [
      "Generator G_AB",
      "Generator G_BA",
      "Discriminator D_A",
      "Encoder משותף"
    ],
    "correctAnswerIndex": 1,
    "explanation": "שני גנרטורים פועלים בכיוונים מנוגדים; G_BA אחראי להמרה חזרה לתחום A."
  },
  {
    "type": "mc",
    "question": "איזו טכניקה מאפשרת ברשתות Style Transfer מהירות (Fast Neural Style) לעבד בזמן אמת?",
    "options": [
      "אופטימיזציה ארוכה על פיקסלים",
      "רשת Feed-Forward מאומנת מראש לכל סגנון",
      "Shared Latent Space",
      "PatchGAN Discriminator קטן"
    ],
    "correctAnswerIndex": 1,
    "explanation": "מאמנים רשת אחת מראש ולאחר מכן מעבירים בה תמונות חדשות ב-Inference יחיד."
  },
  {
    "type": "mc",
    "question": "באיזו שיטה נעשה שימוש מפורש בזוג Encoders נפרדים לתוכן ולסגנון?",
    "options": [
      "UNIT",
      "MUNIT",
      "CycleGAN",
      "Pix2Pix"
    ],
    "correctAnswerIndex": 1,
    "explanation": "MUNIT כולל Content Encoder ו-Style Encoder לכל תחום."
  },
  {
    "type": "mc",
    "question": "מהו האיזון האופטימלי בערכי α ו-β ב-Style Transfer אם רוצים הדגשת סגנון חזקה?",
    "options": [
      "α גבוה, β נמוך",
      "α נמוך, β גבוה",
      "α = β = 0",
      "α = β = ∞"
    ],
    "correctAnswerIndex": 1,
    "explanation": "בטא גבוה מגדיל את משקל Loss הסגנון יחסית ל-α ועל כן הסגנון מודגש על חשבון מבנה התוכן."
  },
  {
    "type": "open",
    "question": "תאר כיצד Weight Trade-off בין L1 ל-Adversarial משפיע על חדות מול נאמנות לקלט במודל Pix2Pix.",
    "correctAnswerText": "L1 גבוה מבטיח התאמה פיקסלית ולכן שומר מבנה אך יוצר טשטוש, בעוד Adversarial גבוה מחדד טקסטורה אך עלול לעוות את המבנה. בחירת משקלים מאוזנת מייצרת פלט חד ששומר על פריסת האובייקטים.",
    "explanation": "איזון קובע אם הפלט יהיה חד וריאליסטי אך אולי חופשי מדי, או צמוד מאוד לקלט אך מטושטש."
  },
  {
    "type": "open",
    "question": "הסבר את חשיבות Cycle Consistency Loss ב-CycleGAN עבור תרגום ללא זוגות תואמים.",
    "correctAnswerText": "כיוון שאין פלט אמת לכל קלט, דרישת עקיבות מעגלית מאלצת שהמרה הלוך-חזור תחזיר את התמונה המקורית; בכך הרשת לומדת לשמר תוכן ולא רק לייצר תמונות סגנוניות אמינות.",
    "explanation": "אילוץ זה מונע מה-Generator להתעלם מהקלט ומחייבו ללמוד מיפוי דו-כיווני עקבי."
  },
  {
    "type": "open",
    "question": "מהו ההסבר לשימוש במטריצת גרם לחישוב Style Loss בשיטת העברת סגנון באמצעות רשתות נוירונים?",
    "correctAnswerText": "מטריצת גרם מודדת קורלציות בין פילטרים בתוך שכבה, וכך לוכדת אילו דפוסים ויזואליים מופיעים יחד בלי קשר למיקומם. התאמת מטריצות גרם של הסגנון והפלט משווה טקסטורה וצבעים באופן נייטרלי למיקום.",
    "explanation": "הקורלציות מייצגות סגנון באופן אינסופי-מיקומי ולכן מתאימות להחלתו על תוכן אחר."
  },
  {
    "type": "open",
    "question": "פרט את ההבדלים העיקריים בדרישות הדאטה בין Pix2Pix ל-CycleGAN.",
    "correctAnswerText": "Pix2Pix זקוק לזוגות תמונות מתואמים (paired) כדי ללמוד מיפוי ישיר, בעוד CycleGAN עובד עם שני אוספים לא-מזוּוָגים (unpaired) ומסתמך על Cycle Consistency ו-GANs דו-כיווניים כדי ללמוד.",
    "explanation": "הבדל בזמינות זוגות משפיע על בחירת המודל למשימה מעשית."
  },
  {
    "type": "open",
    "question": "כיצד שלב Cross-Domain Translation ב-MUNIT יוצר וריאציות מרובות לתמונה אחת?",
    "correctAnswerText": "שומרים את קוד התוכן המקורי וממזגים אותו בכל פעם עם קוד סגנון חדש שנדגם אקראית מהתחום היעד. Decoder היעד מפענח את הצירוף לייצר פלט שונה בסגנון אך זהה במבנה.",
    "explanation": "שינוי רק רכיב הסגנון מאפשר אינסוף קומבינציות על אותו תוכן."
  },
  {
    "type": "open",
    "question": "מדוע PatchGAN Discriminator מאפשר עבודה עם תמונות ברזולוציה גבוהה מבלי להגדיל את גודל הרשת?",
    "correctAnswerText": "ה-Discriminator הוא מודול קונבולוציוני קטן שמחליק על התמונה; מספר הפרמטרים קבוע וגודל התמונה רק מגדיל את מספר ה-Patches הנבדקים, לא את משקלות הרשת.",
    "explanation": "כך מתקבל חיסכון בזיכרון ובמהירות, ותמונות גדולות נבדקות מקומית."
  },
  {
    "type": "open",
    "question": "הגדר בקצרה את הנחת Shared Latent Space וכיצד היא ממומשת ב-UNIT.",
    "correctAnswerText": "ההנחה אומרת שתמונות משני תחומים ניתנות לייצוג זהה במרחב לטנטי. ב-UNIT כל תחום מחזיק Encoder שממפה לתוך z משותף ו-Decoder שממפה חזרה, ואילוצי GAN/השוואת שחזור מכריחים את שני ה-Encoders לייצר קודים מאוחדים.",
    "explanation": "ייצוג משותף זה מאפשר תרגום ללא זוגות תואמים."
  },
  {
    "type": "open",
    "question": "כיצד Perceptual Loss פותר את בעיית הטשטוש שנגרמת משימוש ב-L2 בלבד?",
    "correctAnswerText": "במקום להשוות פיקסלים, Perceptual Loss מודד מרחק במרחב הפיצ'רים של רשת סיווג; בכך הוא מעניש הבדלים מבניים וטקסטורליים ומשמר פרטים חדים שה-L2 ממוצע עליהם.",
    "explanation": "Feature Space רגיש לטקסטורה ולכן מחזיר חדות וחוויית ראייה טבעית."
  },
  {
    "type": "open",
    "question": "תאר בקצרה את תהליך Image Analogies הקלאסי להעברת סגנון ללא רשתות.",
    "correctAnswerText": "השיטה מקבלת זוג תמונות A ו-A' (מקור וסגנון) ומוצאת עבור כל Patch בתמונה חדשה B את ה-Patch הדומה ביותר ב-A. התאמה זו מועתקת מ-A' ל-B', וכך מתקבל עיבוד בסגנון A' עבור תוכן B.",
    "explanation": "Patch Matching על פני פירמידת רזולוציות מאפשר העברת סגנון מדוגמה אחת לאחרת."
  }
]
