[
  {
    "type": "mc",
    "question": "מהו התוסף העיקרי של VQ-GAN לעומת VQ-VAE בסיסי?",
    "options": [
      "Codebook גדול יותר",
      "Discriminator שמוסיף GAN-Loss",
      "החלפת ה-Encoder ב-Transformer בלבד",
      "שימוש במסכת Attention במקום קידוד דיסקרטי"
    ],
    "correctAnswerIndex": 1,
    "explanation": "GAN-Loss מדריך את הדקודר להפיק תמונות חדות ומציאותיות."
  },
  {
    "type": "mc",
    "question": "לשם מה משמש ה-Transformer בתוך VQ-GAN?",
    "options": [
      "לשחזור צבעים חסרים",
      "ללמוד התפלגות של רצף הקודים הדיסקרטיים",
      "לדחוס את התמונות בלחיצה אחת",
      "לסנן רעש גבוה מהפלט"
    ],
    "correctAnswerIndex": 1,
    "explanation": "הוא מתייחס לקודים כאל טוקנים ומבטיח קשרים גלובליים בתמונה."
  },
  {
    "type": "mc",
    "question": "כיצד DALL·E מחולל תמונה מתיאור טקסטואלי?",
    "options": [
      "GAN מותנה בטקסט דרך Discriminator",
      "מודל אוטורגרסיבי שמשלים רצף של קודי תמונה לצד הטקסט",
      "שילוב של CLIP עם StyleGAN",
      "תהליך דיפוזיוני בשני שלבים"
    ],
    "correctAnswerIndex": 1,
    "explanation": "הטקסט וקודי-VQ מופיעים כרצף אחד שהמודל משלים שלב-אחר-שלב."
  },
  {
    "type": "mc",
    "question": "מהי תרומת Perceptual Loss בהעברת סגנון בזמן אמת?",
    "options": [
      "מדידת שגיאת פיקסל-לפיקסל מדויקת",
      "השוואת ייצוגים עמוקים ברשת מאומנת (כגון VGG)",
      "יצירת Codebook נפרד לכל סגנון",
      "ביטול הצורך ב-Decoder"
    ],
    "correctAnswerIndex": 1,
    "explanation": "ה-Loss נשען על דמיון תפיסתי ולא רק על התאמה פיקסלית."
  },
  {
    "type": "mc",
    "question": "מטריצת Gram בהעברת סגנון משמשת ל-…",
    "options": [
      "קביעת חדות הגבולות",
      "לכידת קשרים בין ערוצי הפיצ'רים המאפיינים טקסטורה",
      "מדידת בהירות ממוצעת",
      "חישוב זווית סיבוב בתמונה"
    ],
    "correctAnswerIndex": 1,
    "explanation": "הקורלציות בין פילטרים משקפות את הסגנון הגלובלי של התמונה."
  },
  {
    "type": "mc",
    "question": "למה Instance Normalization החליף Batch Normalization בהעברת סגנון מהירה?",
    "options": [
      "הוא מהיר יותר ב-GPU",
      "הוא מנרמל כל תמונה בנפרד ומנטרל קונטרסט תוכן",
      "הוא מחליף לגמרי את Gram Matrix",
      "הוא מפחית את גודל המודל בחצי"
    ],
    "correctAnswerIndex": 1,
    "explanation": "נרמול פר-דוגמה מפחית השפעות תאורה ומאפשר התאמת סגנון עקבית."
  },
  {
    "type": "mc",
    "question": "AdaIN מאפשר העברת סגנון דינמית מפני שהוא…",
    "options": [
      "מאמן רשת חדשה לכל סגנון",
      "מתאים ממוצע ושונות של התוכן לסטטיסטיקות הסגנון בזמן הריצה",
      "דורש משקולות CLIP בכל שכבה",
      "מחליף את ה-Decoder בפילטרים קבועים"
    ],
    "correctAnswerIndex": 1,
    "explanation": "סטטיסטיקות הסגנון מחושבות ישירות מכל תמונה חדשה – ללא אימון נוסף."
  },
  {
    "type": "mc",
    "question": "ב-SimCLR, זוג חיובי מוגדר כ-…",
    "options": [
      "שתי תמונות מאותה תווית",
      "שתי Augmentations שונות של אותה תמונה",
      "טקסט ותמונה תואמים",
      "שני פאטצ'ים סמוכים באותה תמונה"
    ],
    "correctAnswerIndex": 1,
    "explanation": "האוגמנטציות השונות חולקות זהות ולכן צריכות להיות קרובות במרחב הייצוג."
  },
  {
    "type": "mc",
    "question": "Projection Head ב-SimCLR נועד ל-…",
    "options": [
      "להוסיף Augmentation נוסף בזמן ריצה",
      "למפות פיצ'רים למרחב קטן שבו Loss קונטרסטיבי פועל טוב יותר",
      "לקוונטז את הפיצ'רים לדיסקרט",
      "למתן רעש גבוהה בשכבות העליונות"
    ],
    "correctAnswerIndex": 1,
    "explanation": "ה-Head מוסר לאחר האימון ומשאיר ייצוג כללי למשימות המשך."
  },
  {
    "type": "mc",
    "question": "ב-MAE מסתירים חלק גדול מהפאטצ'ים כדי…",
    "options": [
      "לחסוך זיכרון בלבד",
      "לאלץ את הרשת להבין מבנה גלובלי לצורך שחזור",
      "למנוע צורך ב-Decoder",
      "לבטל Augmentations חזקות"
    ],
    "correctAnswerIndex": 1,
    "explanation": "כאשר מרבית המידע חסר, צריך הקשר חזותי עמוק כדי לשחזרו."
  },
  {
    "type": "mc",
    "question": "Context Prediction מוגדר כמשימת Pretext מסוג…",
    "options": [
      "Generative (שחזור נתונים)",
      "Discriminative (סיווג מיקום יחסי)",
      "Multimodal (טקסט-תמונה)",
      "דיפוזיה הדרגתית"
    ],
    "correctAnswerIndex": 1,
    "explanation": "המשימה היא לסווג לאיזו קטגוריית מיקום שייך פאטצ' ביחס לפאטצ' אחר."
  },
  {
    "type": "mc",
    "question": "Inpainting כ-Pretext מאלץ את הרשת…",
    "options": [
      "להבדיל בין צבעים משלימים",
      "להשלים אזור חסר על סמך ההקשר הגלובלי",
      "לזהות זווית סיבוב",
      "לאתר אובייקטים קטנים"
    ],
    "correctAnswerIndex": 1,
    "explanation": "שחזור מוצלח דורש הבנה של מבנה האובייקט והשדה החזותי כולו."
  },
  {
    "type": "mc",
    "question": "RotNet דורש מהרשת להבין…",
    "options": [
      "שונות ערוצית של סגנון",
      "מהי הזווית הנכונה של אובייקט בתמונה",
      "קוד דיסקרטי של רעש",
      "חישוב גרם מטריקס בזמן ריצה"
    ],
    "correctAnswerIndex": 1,
    "explanation": "כדי לסווג את הסיבוב יש ללמוד מאפייני צורה תקניים."
  },
  {
    "type": "mc",
    "question": "CLIP משתמש ב-Contrastive Loss כדי…",
    "options": [
      "לשחזר טקסט מתוך תמונה",
      "לקרב פיצ'ר תמונה לפיצ'ר הטקסט המתאים ולהרחיק אחרים",
      "להמיר תמונה לקוד VQ",
      "להקטין סטטיסטיקת סגנון"
    ],
    "correctAnswerIndex": 1,
    "explanation": "ההתאמה הדו-כיוונית מאפשרת התאמה ללא תוויות ידניות."
  },
  {
    "type": "mc",
    "question": "היכולת Zero-Shot של CLIP נובעת מ-…",
    "options": [
      "הוספת שכבת סיווג ייעודית בזמן הריצה",
      "השוואת פיצ'ר תמונה לטקסטים חדשים שלא נראו באימון",
      "פיינ-טיונינג קצר על כל מחלקה",
      "שימוש ב-GAN Loss פנימי"
    ],
    "correctAnswerIndex": 1,
    "explanation": "המבנה המשותף של מרחב התמונה והטקסט מאפשר התאמה מיידית."
  },
  {
    "type": "mc",
    "question": "ב-Contrastive Learning Augmentations חזקות חשובות כי הן…",
    "options": [
      "מונעות צורך ב-Batch גדול",
      "מאלצות את הרשת לזהות מהות משותפת ולא פרטים שטחיים",
      "מגדילות את Codebook",
      "מבטלות את הצורך ב-Projection Head"
    ],
    "correctAnswerIndex": 1,
    "explanation": "שונות גדולה בין שני Augs מלמדת את המודל הכללה חזקה."
  },
  {
    "type": "mc",
    "question": "מהו החיסרון המרכזי של משימות Pretext ידניות כמו Jigsaw או SplitBrain, לפי ההרצאה?",
    "options": [
      "דורשות Discriminator כבד",
      "כל משימה דורשת רעיון ועיצוב נפרד ואינה תמיד כללית",
      "לא תומכות ב-GPU מהיר",
      "אינן ניתנות לשילוב עם טקסט"
    ],
    "correctAnswerIndex": 1,
    "explanation": "התחזוקה והתכנון ידניים ועלולים להניב ייצוגים מוגבלים."
  },
  {
    "type": "mc",
    "question": "Vision-Aided Neural Graphics משתמש ב-Self-Supervised Learning כדי…",
    "options": [
      "להימנע מכל צורך בתמונות אמיתיות",
      "לצמצם עלויות תיוג יקרות ולהעביר ייצוגים למשימות מטרה",
      "לשפר רזולוציה על-ידי דה-נויסינג בלבד",
      "לטפל במודלי דיפוזיה כבדים"
    ],
    "correctAnswerIndex": 1,
    "explanation": "ייצוגים שנלמדו ממשימות עזר מאיצים משימות עם מעט תוויות."
  },
  {
    "type": "mc",
    "question": "לפי ההרצאה, Fluid (2024) מדגים…",
    "options": [
      "GAN קלאסי דו-שלבי",
      "מודל אוטורגרסיבי שיוצר תמונה רציפה עם שליטה קומפוזיציונית גבוהה",
      "דיפיוזיה מואצת ב-Scheduler חדש",
      "Inpainting רב-שלבי בטקסטורות טבע"
    ],
    "correctAnswerIndex": 1,
    "explanation": "Fluid משפר יצירת טקסט-לתמונה עם תהליך רציף ודינמי."
  },
  {
    "type": "mc",
    "question": "GPT-4o מתואר כ-…",
    "options": [
      "מודל דיפוזיוני קלאסי",
      "מודל אוטורגרסיבי רב-מודאלי היוצר תמונה שלב-אחרי-שלב",
      "Decoder יחיד של CLIP",
      "Pretext למשימת Colorization"
    ],
    "correctAnswerIndex": 1,
    "explanation": "הוא בונה את התמונה בדינמיקה רציפה כמו טקסט."
  },
  {
    "type": "mc",
    "question": "Inpainting עם Adversarial Loss מפיק תוצאות חדות יותר משום ש-…",
    "options": [
      "L2 בלבד מוסיף רעש-גבוה",
      "המפלה מענישה חוסר מציאותיות ומחדדת טקסטורה",
      "אין צורך ב-Decoder",
      "ה-Encoder לומד סיבוב תמונה"
    ],
    "correctAnswerIndex": 1,
    "explanation": "GAN-Loss מוסיף פידבק דמוי-אדם מעבר לשגיאה פיקסלית."
  },
  {
    "type": "mc",
    "question": "ב-MAE, ה-Encoder רואה רק פאטצ'ים גלויים כדי…",
    "options": [
      "לסלק רעש דיגיטלי",
      "לחסוך חישוב ולהתמקד בחלקי מידע משמעותיים",
      "לקודד טקסט בתמונה",
      "להפחית קונטרסט יתר"
    ],
    "correctAnswerIndex": 1,
    "explanation": "ה-Decoder משלים את השאר; Encoder קטן וממוקד."
  },
  {
    "type": "mc",
    "question": "מדוע שילוב טקסט בלמידה מולטימודלית נחשב 'צפוף סמנטית'?",
    "options": [
      "טקסט צורך פחות VRAM מתמונה",
      "מילים בודדות מתארות רעיונות רבים ולכן מוסיפות מידע עשיר בפחות נתונים",
      "טקסט דורש Encoder קטן יותר",
      "תמונות לא מכילות מידע סמנטי כלל"
    ],
    "correctAnswerIndex": 1,
    "explanation": "השפה מאפשרת תיאור מפורט של רעיונות במעט תווים."
  },
  {
    "type": "mc",
    "question": "כיצד Contrastive Learning במולטימודאליות (CLIP) משפר חיפוש תמונות?",
    "options": [
      "יוצר תמונות חדשות מטקסט",
      "ממקם תמונות וטקסט באותו מרחב וקטורי להשוואה ישירה",
      "משחזר אזורים חסרים בתמונה",
      "מפיק סגנון חזותי אחיד לכל תמונות המאגר"
    ],
    "correctAnswerIndex": 1,
    "explanation": "אפשר למדוד דמיון קוסיני בין תיאור מילולי לבין כל תמונה."
  },
  {
    "type": "open",
    "question": "הסבר כיצד GAN-Loss מחדד את תוצרי VQ-GAN ביחס ל-VQ-VAE רגיל.",
    "correctAnswerText": "המפלה מענישה טקסטורות לא טבעיות, כך שהדקודר לומד להפיק פרטים חדים וריאליסטיים ולא רק לשחזר ממוצע פיקסלי.",
    "explanation": "Feedback חזותי נוסף מעבר ל-L2 מפחית טשטוש ורעש."
  },
  {
    "type": "open",
    "question": "תאר בקצרה את תהליך DALL·E: מטקסט לפיקסלים.",
    "correctAnswerText": "הטקסט מצורף כטוקנים לתחילת רצף. מודל אוטורגרסיבי ממשיך את הרצף בקודים דיסקרטיים של תמונה (VQ). לבסוף Decoder מפענח את הקודים לפיקסלים.",
    "explanation": "הקישור מתרחש כולו ברמת טוקנים – אין מפלה או דיפוזיה."
  },
  {
    "type": "open",
    "question": "כיצד AdaIN מאפשר העברת סגנון לרוחב סגנונות שונים בזמן-אמת?",
    "correctAnswerText": "AdaIN מנרמל פיצ'רי תוכן לאמצע ואחד, ואז מותח ומזיז אותם לפי ממוצע ושונות של פיצ'רי הסגנון שמחושבים על-הפרח בשלב הריצה.",
    "explanation": "אין צורך לאמן רשת חדשה – החישוב סטטיסטי בלבד."
  },
  {
    "type": "open",
    "question": "למה Instance Normalization מועיל בהעברת סגנון מהירה?",
    "correctAnswerText": "הוא מסיר קונטרסט ותאורה ספציפיים של תוכן כך שהרשת מתמקדת בטקסטורת הסגנון, ולכן מתקבלת התוצאה עקבית בין תמונות שונות.",
    "explanation": "BatchNorm היה משמר הבדלים בין דוגמאות בקבוצה."
  },
  {
    "type": "open",
    "question": "הסבר את העיקרון הקונטרסטיבי של SimCLR בלי להיכנס לנוסחאות.",
    "correctAnswerText": "הרשת מקרבת ייצוגים של שתי גרסאות של אותה תמונה ומרחיקה ייצוגים של תמונות אחרות באותו batch, ובכך לומדת מאפיינים כללים של התמונה ללא תוויות.",
    "explanation": "Positive = Augs זהות; Negative = שאר התמונות."
  },
  {
    "type": "open",
    "question": "מדוע MAE מסתיר כ-75 % מהפאטצ'ים ולא אחוז קטן בהרבה?",
    "correctAnswerText": "כשמרבית הפאטצ'ים חסרים, הרשת חייבת להבין את מבנה האובייקטים והשדה החזותי כדי לשחזרם, במקום להסתמך על סימנים מקומיים בלבד.",
    "explanation": "המשימה הופכת ממוקדת בהבנה גלובלית ולא בשינון פרטים."
  },
  {
    "type": "open",
    "question": "כיצד CLIP משתמש בזוגות תמונה-טקסט ללמידה ללא תוויות מפורשות?",
    "correctAnswerText": "במהלך האימון הוא מקרב את הווקטור של תמונה אל הווקטור של הטקסט שתיאר אותה ומרחיק מטקסטים אחרים, וכך יוצר מרחב משותף שמקשר בין חזות לשפה.",
    "explanation": "הדמיון מחושב על כל זוגות באצווה ומשמש כ-Loss קונטרסטיבי."
  },
  {
    "type": "open",
    "question": "מהו היתרון של Contrastive Learning על פני Pretext גנרטיבי כמו Inpainting?",
    "correctAnswerText": "במקום לשחזר פיקסלים, הוא ממקם דוגמאות במרחב ייצוג כך שחיוביות קרובות ושליליות רחוקות, מה שמוביל לייצוג כללי וגמיש יותר למשימות שונות.",
    "explanation": "הוא מתמקד במרחקי תכונות, לא בשחזורים ספציפיים."
  },
  {
    "type": "open",
    "question": "איך Inpainting עם Adversarial Loss משפר את חדות התוצאה לעומת L2 בלבד?",
    "correctAnswerText": "L2 מביא לשחזור ממוצע מטושטש; המפלה מוסיפה קריטריון מציאותי ודוחפת את הרשת ליצור טקסטורה חדה שתיראה אמינה לעין אדם.",
    "explanation": "GAN מספק איתות איכותי מורכב מעבר לשגיאת פיקסל."
  },
  {
    "type": "open",
    "question": "תאר את התרומה של משימות כמו RotNet להבנת מבנה אובייקט ללא תוויות.",
    "correctAnswerText": "חיזוי זווית הסיבוב דורש לדעת כיצד נראה האובייקט 'נכון', ולכן הרשת לומדת צורה גלובלית ותבניות מבניות, לא רק טקסטורות מקומיות.",
    "explanation": "זה מקנה מה שנקרא 'שכל ישר ויזואלי'."
  },
  {
    "type": "open",
    "question": "מדוע משימות Pretext ידניות (למשל Jigsaw) אינן תמיד סקלאביליות?",
    "correctAnswerText": "כל משימה מחייבת הגדרה ידנית חדשה, ולעיתים הייצוג שהיא מלמדת מתאים רק למשימה המסוימת ולא כללית למשימות עתידיות.",
    "explanation": "המצאת Pretext חדש דורשת ניסוי וטעייה ממושכים."
  },
  {
    "type": "open",
    "question": "כיצד Vision-Aided Neural Graphics מנצל Self-Supervised Learning לחיסכון בעלויות תיוג?",
    "correctAnswerText": "מאמנים רשת על משימת Pretext אוטומטית, ואז משתמשים ב-Encoder המאומן כבסיס למשימות מטרה עם מעט דוגמאות מתויגות, וכך חוסכים בתהליך תיוג יקר.",
    "explanation": "הייצוגים שנלמדו מראש מקצרים את הדרך בפיקוח חלקי."
  },
  {
    "type": "open",
    "question": "מה האתגר ש-PixelCNN מציב, וכיצד VQ-GAN עוקף אותו?",
    "correctAnswerText": "PixelCNN דוגם פיקסל-אחר-פיקסל ולכן איטי ומוגבל ברזולוציה. VQ-GAN דוגם רצף קודים דיסקרטיים קצר ומשחזר מהם תמונה שלמה וחדה במהירות.",
    "explanation": "המעבר לקוד דיסקרטי ול-GAN Loss מביא איכות וקצב גבוהים."
  },
  {
    "type": "open",
    "question": "כיצד CLIP מאפשר חיפוש תמונות לפי תיאור מילולי ללא Fine-Tuning נוסף?",
    "correctAnswerText": "המנוע מחשב וקטור לכל טקסט שאנו מזינים ווקטור לכל תמונה, ואז מדרג את הדמיון הקוסיני ביניהם; התאמה גבוהה מצביעה על רלוונטיות ללא צורך באימון נוסף.",
    "explanation": "המרחב המשותף נלמד מראש על זוגות תמונה-טקסט."
  },
  {
    "type": "open",
    "question": "מהי 'צפיפות סמנטית' של טקסט, וכיצד היא מסייעת בלמידה מולטימודלית?",
    "correctAnswerText": "טקסט מאפשר לתאר מושגים רבים במעט מילים, ולכן מוסיף מידע משמעותי בדחיסות גבוהה. שילובו עם תמונות מעשיר את הייצוג ומשפר הכללה.",
    "explanation": "השפה משמשת 'עוגן' סמנטי שלתמונה לבדה חסר."
  },
  {
    "type": "open",
    "question": "כיצד MAE מעצב את הארכיטקטורה שלו כך שה-Decoder קטן מה-Encoder?",
    "correctAnswerText": "ה-Encoder רואה רק רבע מהפאטצ'ים הגלויים, ולכן ה-Decoder צריך לשחזר רק את החסרים, ויכול להיות קומפקטי ועדיין להשלים את הפרטים הנותרים.",
    "explanation": "פחות מידע להזנה ⇒ פחות חישוב בשלב הפענוח."
  },
  {
    "type": "open",
    "question": "תן דוגמה ליישום שבו שילוב טקסט בתמונה (Multimodal SSL) הכרחי לפי ההרצאה.",
    "correctAnswerText": "מודלי טקסט-לתמונה כמו GPT-4o יוצרים תמונה מורכבת מתוך תיאור מילולי עשיר, ומצריכים הבנה משולבת של שני הערוצים.",
    "explanation": "הטקסט מספק הנחיה קומפוזיציונית והתמונה מגשימה אותה חזותית."
  },
  {
    "type": "open",
    "question": "הסבר בקצרה את תרומת Fluid (2024) ביחס למודלי טקסט-לתמונה קודמים.",
    "correctAnswerText": "Fluid משתמש ביצירה אוטורגרסיבית רציפה שמאפשרת שליטה מדויקת בקומפוזיציה, סגנון וגיוון – כל זאת ללא תהליך דיפוזיה כבד, ובתיאום צמוד לטקסט.",
    "explanation": "יצירה שורה-אחרי-שורה נותנת שליטה וגמישות בזמן ההפקה."
  },
  {
    "type": "open",
    "question": "מדוע GPT-4o נתפס כקפיצת מדרגה ביצירת תמונות, על-פי הסיכום?",
    "correctAnswerText": "GPT-4o בונה את התמונה אוטורגרסיבית בדומה לטקסט, כך שהמבנה הגלובלי והפרטים העדינים מתהווים יחד לאורך התהליך – ללא שלבים נפרדים.",
    "explanation": "התמונה נוצרת 'בחוט אחד' כמו רצף מילים, ומפיקה חדות ושליטה גבוהה."
  }
]
