[
  {
    "type": "mc",
    "question": "מהי המטרה העיקרית של שכבת הקונבולוציה ב-CNN?",
    "options": [
      "לצמצם ממדים על-ידי בחירת ערך בודד מכל חלון קטן",
      "לחלץ תכונות מקומיות בעזרת גרעינים קבועי-גודל המחליקים על התמונה",
      "למפות את הפלט להסתברויות סופיות עבור כל מחלקה אפשרית",
      "לאפס ערכים שליליים כדי לשמור מידע חיובי בלבד"
    ],
    "correctAnswerIndex": 1,
    "explanation": "גרעיני הקונבולוציה מבצעים מכפלה קונבולוציונית בחלונות מקומיים וכך יוצרים feature maps המציגות קצוות, טקסטורות וצורות."
  },
  {
    "type": "mc",
    "question": "באיזו שכבה ב-CNN מצמצמים את ממדי הנתונים תוך שמירה על תכונות חשובות?",
    "options": [
      "שכבת קונבולוציה עם stride גדול במיוחד",
      "שכבת הפעלה Sigmoid לא-לינארית",
      "שכבת Pooling המחלקת את המפה לחלונות ומחלצת ערך נציג",
      "שכבת Fully Connected עם כמות נוירונים מוגבלת"
    ],
    "correctAnswerIndex": 2,
    "explanation": "בשכבת Pooling מבוצע down-sampling (למשל Max או Average Pooling) ולכן המפה קטנה אך שומרת מידע קריטי."
  },
  {
    "type": "mc",
    "question": "מהו תפקיד פונקציית ReLU בהקשר של CNN?",
    "options": [
      "לנרמל את פלטי השכבות לטווח (0, 1) בדיוק",
      "להחליף ערכים חיוביים בערכים שליליים כדי להדגיש ניגוד",
      "להכניס אי-לינאריות על-ידי איפוס ערכים שליליים ושמירת החיוביים",
      "לבצע דגימה אקראית המונעת התאמה-יתרה בזמן האימון"
    ],
    "correctAnswerIndex": 2,
    "explanation": "ReLU (x → max(0,x)) משאיר רק ערכים חיוביים, שובר ליניאריות ומפחית סכנת vanishing gradients."
  },
  {
    "type": "mc",
    "question": "איזה יתרון מרכזי מספקים Feature Descriptors קלאסיים (HOG, Color Histograms)?",
    "options": [
      "מאפשרים דגימה ישירה ממרחב לטנטי רציף",
      "מפחיתים מורכבות על-ידי שמירת הנתונים המשמעותיים בלבד",
      "מבטלים את הצורך ברשתות עמוקות ללמידה",
      "משפרים את ה-dropout ומונעים התאמה-יתרה"
    ],
    "correctAnswerIndex": 1,
    "explanation": "המתארים מייצרים וקטור קומפקטי של מאפיינים מרכזיים ולכן מקטינים מימד ונטל חישובי."
  },
  {
    "type": "mc",
    "question": "למידה בלתי-מונחית (Unsupervised) נבדלת ממונחית בכך שהמודל…",
    "options": [
      "מזהה דפוסים סמויים בדאטה ללא התאמה בין תמונה לפלט מסוים",
      "מתבסס תמיד על פונקציית Softmax בסיום הרשת",
      "דורש מערכות תגמול חיצוניות מבוססות חיזוקים",
      "משתמש רק בבקרת dropout לשמירת כלליות"
    ],
    "correctAnswerIndex": 0,
    "explanation": "ללא תוויות, המודל מגלה מבנה פנימי (קיבוץ, הפחתת ממדים) במקום למפות לקלאסים ידועים."
  },
  {
    "type": "mc",
    "question": "מדוע AutoEncoder רגיל מוגבל ביצירת דוגמאות חדשות?",
    "options": [
      "אינו מכיל שכבות קונבולוציה ולכן מפיק תמונות מטושטשות",
      "מקודד כל דגימה לנקודה יחידה, ולכן המרחב הלטנטי אינו רציף",
      "מסתמך על רעש אחיד במקום על התפלגות גאוסיאנית",
      "משתמש ב-KL Divergence כמרכיב פסד יחיד"
    ],
    "correctAnswerIndex": 1,
    "explanation": "נקודות בדידות יוצרות “חורים” במרחב; דגימה ביניהן מובילה לאובייקטים לא-מציאותיים."
  },
  {
    "type": "mc",
    "question": "מה מוסיף VAE ביחס לאוטואנקודר רגיל?",
    "options": [
      "שכבת Pooling חדשה המפחיתה רעשים בתמונה",
      "הטלת אילוץ שהייצוג הלטנטי יתפלג כ-N(0,1) רציף",
      "החלפה של ReLU ב-LeakyReLU בכל השכבות",
      "כפל רשת מפלה לבקרת איכות הפלט"
    ],
    "correctAnswerIndex": 1,
    "explanation": "על-ידי KL Divergence המודל מקרב את התפלגות q(z|x) לנורמלית סטנדרטית, ומאפשר דגימה רציפה."
  },
  {
    "type": "mc",
    "question": "איזו נוסחה מיישמת את Reparameterization Trick ב-VAE?",
    "options": [
      "z = σ · ε + μ, ε ~ Uniform(-1,1)",
      "z = ε + σ · μ, ε ~ N(0,1)",
      "z = μ + σ · ε, ε ~ N(0,1)",
      "z = μ · σ + ε, ε ~ Laplace(0,1)"
    ],
    "correctAnswerIndex": 2,
    "explanation": "נוסחה זו מבטיחה כי z נגזרת ממשתנים נפרדים (μ, σ) ו-ε הניתן להחזרה בשרשרת הגרדיאנטים."
  },
  {
    "type": "mc",
    "question": "ב-GAN מי אחראי להבחין בין דוגמאות אמיתיות למזויפות?",
    "options": [
      "Encoder בעל שכבות BatchNorm רצופות",
      "Discriminator הלומד לסווג כנכונות או כמזויפות",
      "Generator המפיק רעש מותאם והופכו לתמונה",
      "רשת Fully Connected המשמשת כבקר-איכות"
    ],
    "correctAnswerIndex": 1,
    "explanation": "המפלה מקבלת גם דוגמאות אמיתיות וגם פלטי הגנרטור ומעדכן את עצמו לשפר אבחנה ביניהן."
  },
  {
    "type": "mc",
    "question": "מהי בעיית Mode Collapse באימון GAN?",
    "options": [
      "המפלה מפסיק לקבל גרדיאנט ולכן נעצר",
      "נצפה פיזור יתר של הדוגמאות הלטנטיות",
      "הגנרטור מייצר שוב ושוב וריאציה אחת במקום מגוון רחב",
      "פונקציית ההפסד מתכנסת לערך אפסי מהר מדי"
    ],
    "correctAnswerIndex": 2,
    "explanation": "הגנרטור “נתקע” במצב שמספר מצומצם של דוגמאות מטעות את המפלה, ולכן הגיוון אובד."
  },
  {
    "type": "mc",
    "question": "איזו טכניקה מרכזית ב-DCGAN משפרת יציבות אימון?",
    "options": [
      "שימוש עקבי בפונקציית Tanh ב-Discriminator",
      "הפחתה ליניארית של גודל ה-batch כל 5 איטרציות",
      "החלפת שכבות Fully Connected בשכבות קונבולוציה עמוקות",
      "תוספת רעש לבן לפילטרים בכל שכבה"
    ],
    "correctAnswerIndex": 2,
    "explanation": "מבנה קונבולוציוני מאפשר ניצול מבנה תמונה ומקטין רגישות לפרמטרים, ובשילוב Batch Norm מתקבלת יציבות טובה יותר."
  },
  {
    "type": "mc",
    "question": "מה מודד Fréchet Inception Distance (FID)?",
    "options": [
      "השונות הכוללת של ערכי הפיקסלים בין דוגמאות",
      "המרחק בין התפלגויות תכונות של תמונות אמיתיות ומזויפות",
      "משך האימון הנדרש להגיע לאיכות סבירה",
      "מספר המצבים הסטטיסטיים בהם התרחש Mode Collapse"
    ],
    "correctAnswerIndex": 1,
    "explanation": "FID מחשב מרחק פרמאטרי בין גאוסיאנים במרחב תכונות של Inception Net; ערך נמוך משמעו איכות גבוהה."
  },
  {
    "type": "mc",
    "question": "איזו שכבה ב-CNN ממירה את הפלט להסתברויות עבור כל מחלקה?",
    "options": [
      "שכבת Pooling בעלת חלון 2×2",
      "שכבת Softmax בסיום ה-Fully Connected",
      "שכבת הפעלה ReLU לפני ה-decoder",
      "שכבת Batch Norm המנרמלת את האינדקס"
    ],
    "correctAnswerIndex": 1,
    "explanation": "Softmax מיישרת את הערכים לסכום 1 ומאפשרת פרשנות הסתברותית לסיווג."
  },
  {
    "type": "mc",
    "question": "היכן נעשה שימוש ב-KL Divergence בפונקציית הפסד של VAE?",
    "options": [
      "לקירוב התפלגות הפלט ל-Uniform(0,1)",
      "להבטיח שהתפלגות ה-encoder קרובה ל-N(0,1)",
      "למדידת טשטוש בפלט ה-decoder",
      "להשוואת הגנרטור למפלה ב-GAN"
    ],
    "correctAnswerIndex": 1,
    "explanation": "רכיב KL מעניש סטיות מנורמלית סטנדרטית וכך מטפח מרחב לטנטי חלק ורציף."
  },
  {
    "type": "mc",
    "question": "מהו היתרון הבולט של אינטרפולציה במרחב הלטנטי של VAE?",
    "options": [
      "יצירת רצף חלק של דוגמאות אמיתיות-למחצה בין שתי נקודות",
      "שמירה על חדות-על בהשוואה ל-GAN",
      "הפחתת זמן האימון בשליש",
      "ביטול הצורך בתגיות בפאזה המפוקחת"
    ],
    "correctAnswerIndex": 0,
    "explanation": "רציפות המרחב מאפשרת מעבר מדורג בין מאפייני דוגמאות, שימושי להדגמה ולערבוב תוכן."
  },
  {
    "type": "mc",
    "question": "איזה שילוב רכיבים יוצר משחק סכום-אפס ב-GAN?",
    "options": [
      "Dropout מול Batch Norm במקביל",
      "ReLU מול Sigmoid באותה שכבה",
      "Generator השואף להטעות מול Discriminator השואף לזהות",
      "Fully Connected מול Convolutional במפלה"
    ],
    "correctAnswerIndex": 2,
    "explanation": "כל רשת מנסה למקסם פונקציית מטרה הפוכה לשנייה, ולכן הסכום הכולל הוא אפס."
  },
  {
    "type": "mc",
    "question": "מדוע Batch Normalization חשוב ב-DCGAN?",
    "options": [
      "מייצב את הפצת האקטיבציות ומאיץ התכנסות",
      "מחליף את הצורך בפונקציות הפעלה לא-לינאריות",
      "מפחית גודל kernel בלי לאבד מידע",
      "מוסיף דגימה רנדומלית למרחב הלטנטי"
    ],
    "correctAnswerIndex": 0,
    "explanation": "נרמול הפיצ’רים בכל מיני-batch שומר טווח ערכים עקבי, מקל על למידה ומזער סטיות רחבות."
  },
  {
    "type": "mc",
    "question": "כיצד CNN לומדת היררכיה של תכונות?",
    "options": [
      "מתחילה בשכבות Fully Connected ומסיימת בקונבולוציה",
      "שכבות ראשונות מזהות קצוות; עמוקות מזהות צורות מורכבות",
      "בכל שכבה משמרת בדיוק את אותה רמת פירוט",
      "רק עם Pooling ללא שכבות הפעלה"
    ],
    "correctAnswerIndex": 1,
    "explanation": "המודל בונה תכונות בסיס לקומפוזיציות מורכבות בהדרגה, לכן הקצוות משמשים לבניית אובייקטים שלמים."
  },
  {
    "type": "mc",
    "question": "מהו שימוש מרכזי אחד במודלים גנרטיביים לפי ההרצאה?",
    "options": [
      "הפחתת רזולוציית תמונה לחיסכון בזיכרון",
      "הגדלת מערכי נתונים בעת מחסור בדוגמאות מסומנות",
      "השבתת שכבות כדי למנוע התאמה-יתרה",
      "סיווג מהיר של תמונות בזמן-אמת"
    ],
    "correctAnswerIndex": 1,
    "explanation": "יצירת דוגמאות סינתטיות מגבירה גיוון ויכולה לשמש לאימון משופר במצב דאטה-דל."
  },
  {
    "type": "mc",
    "question": "איזו בעיה מופיעה כאשר הגנרטור חזק מדי והמפלה חלש מדי?",
    "options": [
      "Mode Collapse קבוע ולא ניתן לתיקון",
      "Vanishing Gradient היוצר גרדיאנטים זעירים במפלה",
      "התפלגות N(0,1) הופכת ל-Uniform",
      "FID גדל למרות ש-Inception Score קטן"
    ],
    "correctAnswerIndex": 0,
    "explanation": "כאשר הגנרטור חזק מדי והדיסקרימינטור חלש, הדיסקרימינטור לא מצליח להבחין בין דוגמאות אמיתיות למזויפות. לכן, הגנרטור מקבל פידבק חלש וחסר הבחנה, וממשיך לייצר את אותן דוגמאות שוב ושוב – תופעה שנקראת Mode Collapse. במצב זה, גם אם מזינים לגנרטור וקטורי רעש שונים, הוא מייצר פלטים דומים מאוד, כי אין לו תמריץ לגוון."
  },
  {
    "type": "mc",
    "question": "איזו טכניקה מסייעת למנוע התאמה-יתרה ב-Fully Connected?",
    "options": [
      "Dropout הכבה אקראית של נוירונים בזמן האימון",
      "Zero-padding להגדלת קלט השכבה",
      "Upsampling לשכבת הקונבולוציה הקודמת",
      "שימוש ב-Stride גדול מאוד בפילטר"
    ],
    "correctAnswerIndex": 0,
    "explanation": "Dropout מאלץ את הרשת לא להסתמך יתר־על-המידה על נוירונים בודדים וכך משפר הכללה."
  },
  {
    "type": "open",
    "question": "הסבר בקצרה כיצד Supervised Learning שונה מ-Unsupervised Learning.",
    "correctAnswerText": "בלמידה מונחית המודל מתאמן על נתונים עם תוויות ידועות מראש ומלמד קשר ישיר קלט-פלט; בלמידה בלתי-מונחית אין תוויות והמודל מחפש מבנים סמויים בדאטה.",
    "explanation": "ההבדל הוא קיומן של תוויות: בפיקוח יש יעד מפורש, בלא-פיקוח המודל רק מגלה קבוצות או התפלגויות בעצמו."
  },
  {
    "type": "open",
    "question": "מהי מטרת רכיב Reconstruction Loss ב-VAE, וכיצד הוא מאוזן מול KL Loss?",
    "correctAnswerText": "Reconstruction Loss דואג שה-decoder ישחזר קלט באיכות גבוהה; הוא מאוזן מול KL Divergence כדי למנוע מרחב לטנטי בלתי-רציף אך גם להימנע מטשטוש יתר.",
    "explanation": "מתן משקל יתר לשחזור פוגע ברציפות הלטנטי, ומשקל יתר ל-KL פוגע באיכות הפלט; נדרש איזון."
  },
  {
    "type": "open",
    "question": "תאר את עקרון הפעולה של Discriminator ב-GAN.",
    "correctAnswerText": "המפלה מקבל גם דוגמאות אמיתיות מדאטה וגם דוגמאות שיוצרו בידי הגנרטור, ומפיק הסתברות “אמיתי/מזויף”. הוא מתעדכן כך שהסתברותו תהיה נכונה ככל האפשר.",
    "explanation": "ככל שהמפלה משתפר, הגנרטור חייב לייצר דוגמאות מציאותיות יותר כדי להטעותו, מה שמניע את שני הצדדים."
  },
  {
    "type": "open",
    "question": "למה Feature Descriptors היסטוריים כמו HOG היו חשובים לפני עליית CNN?",
    "correctAnswerText": "הם סיפקו ייצוג קומפקטי וברור של קצוות וכיווני גרדיאנט, כך שמסווגים קלאסיים יכלו לעבוד ביעילות גם בלי רשתות עמוקות.",
    "explanation": "ללא למידה עמוקה, תהליך חילוץ ידני של מאפיינים היה חיוני להפחתת ממד ולדיוק זיהוי."
  },
  {
    "type": "open",
    "question": "מהם שלושת השלבים העיקריים בתהליך CNN כפי שתואר בהרצאה?",
    "correctAnswerText": "קלט תמונה → חילוץ תכונות (Convolution + Activation + Pooling) → סיווג סופי (Fully Connected + Softmax).",
    "explanation": "שכבות הקונבולוציה בונות תכונות, pooling מצמצם ממד, Fully Connected ממפה למרחב המחלקות."
  },
  {
    "type": "open",
    "question": "הסבר בקצרה מהו Mode Collapse ואיך ניתן להפחיתו.",
    "correctAnswerText": "Mode Collapse מתרחש כש-generator מייצר מספר מוגבל של דוגמאות חוזרות; ניתן להפחיתו בשיטות כמו minibatch discrimination, שינוי קצב הלמידה או שימוש ב-WGAN.",
    "explanation": "גיוון בקלט ובאסטרטגיות הפסד מאלץ את הגנרטור לחקור אזורים נוספים בהתפלגות."
  },
  {
    "type": "open",
    "question": "כיצד DCGAN משפר את האימון יחסית ל-GAN בסיסי?",
    "correctAnswerText": "על-ידי שימוש בשכבות קונבולוציה עמוקות, Batch Norm, ופונקציות ReLU/LeakyReLU מתאימות, DCGAN מייצב גרדיאנטים ומפיק תמונות באיכות גבוהה יותר.",
    "explanation": "המבנה הקונבולוציוני מנצל את טבע התמונה, והנרמול מונע פיצוץ או דעיכת גרדיאנטים."
  },
  {
    "type": "open",
    "question": "מדוע נדרש טריק reparameterization ב-VAE",
    "correctAnswerText": "דגימה אקראית אינה גזירה; על-ידי פירוק z ל-μ, σ ו-ε נפרד, הפונקציה הופכת להפרדה ליניארית בגורם גזיר, ומאפשרת חישוב גרדיאנטים ל-μ, σ.",
    "explanation": "הגרדיאנט זורם דרך פעולות אלגבריות אך לא דרך פעולה לא-דטרמיניסטית; הפירוק עוקף מגבלה זו."
  },
  {
    "type": "open",
    "question": "כיצד Softmax מאפשר סיווג רב-מחלקתי?",
    "correctAnswerText": "הוא מנרמל את כל הלוגיטים לסכום 1, ולכן כל ערך אפשר לפרש כהסתברות למחלקה.",
    "explanation": "המחלקה עם ההסתברות הגבוהה ביותר נבחרת; הפלט רציף ומאפשר אובדן קרוס-אנטרופי."
  },
  {
    "type": "open",
    "question": "מה תפקיד Dropout באימון רשת עמוקה?",
    "correctAnswerText": "כיבוי אקראי של נוירונים מפחית תלות-יתר במאפיין יחיד ומשפר יכולת הכללה.",
    "explanation": "בזמן בדיקה כל הנוירונים פעילים והמשקולות מקונסקלים, מה שממוצע התנהגות רשתות משנה רבות."
  },
  {
    "type": "mc",
    "question": "מהו התפקיד המרכזי של ‎zero-padding‎ בשכבת קונבולוציה?",
    "options": [
      "שמירה על גודל ה-feature map כדי לא לאבד מידע שוליים",
      "אכיפת חציצה בין ערוצי צבע שונים",
      "הפחתת מורכבות חישובית בזמן בדיקה",
      "יצירת חיווי דליל עבור pooling עתידי"
    ],
    "correctAnswerIndex": 0,
    "explanation": "מילוי האפסים מאפשר לגרעין “לבקר” גם על הקצה כך שממד היציאה נותר שווה לממד הקלט."
  },
  {
    "type": "mc",
    "question": "ל-Max Pooling יש יתרון על Average Pooling בעיקר ב-…",
    "options": [
      "שימור צבעים נייטרליים בתמונות טבע",
      "שמירת הקצוות והתכונות הדומיננטיות החזותיות",
      "צמצום דרסטי-יותר של ממדים במשוואות ליניאריות",
      "לימוד מהיר יותר של שכבת Softmax"
    ],
    "correctAnswerIndex": 1,
    "explanation": "ערך-המקסימום מעביר את האות החזק ביותר בכל חלון, ולכן קצוות בולטים נשמרים טוב יותר."
  },
  {
    "type": "mc",
    "question": "מהו translation invariance ברשת קונבולוציה?",
    "options": [
      "יכולת לזהות את אותו דפוס גם אם זז במרחב התמונה",
      "התאמה ל-augmentation צבעוני",
      "הפחתת כמות הפילטרים בפולינג",
      "אילוץ שה-Softmax לא יושפע מרוטציה"
    ],
    "correctAnswerIndex": 0,
    "explanation": "גרעינים נודדים בכל מיקום, ולכן מאפיינים דומים יופעלו גם אם האובייקט מוסת."
  },
  {
    "type": "mc",
    "question": "בהקשר AutoEncoder, “חורי” מרחב לטנטי פירושם…",
    "options": [
      "דגימות שעברו dropout פנימי",
      "אזורים שלא נלמדו ולכן מפיקים פלט לא-מציאותי",
      "ערוצי latent שהוצאו בשל נירמול",
      "שימוש ב-stride משתנה בין שכבות"
    ],
    "correctAnswerIndex": 1,
    "explanation": "מאחר שה-encoder ראה רק נקודות מסוימות, דגימה ביניהן מניבה שחזור דל-איכות."
  },
  {
    "type": "mc",
    "question": "מהו latent space arithmetic?",
    "options": [
      "שימוש בפרמטרים מורחבים לגנרטור",
      "חיבור/חיסור וקטורים לטנטיים לקבלת שינוי תוכן ברור",
      "השוואת תווי-פנים בפולינג",
      "נירמול משקלות ב-Batch Norm"
    ],
    "correctAnswerIndex": 1,
    "explanation": "למשל “חיוך” = z(פנים-מחייכות) − z(פנים-ניטרליות); הוספת הווקטור יוצרת חיוך בדוגמאות חדשות."
  },
  {
    "type": "mc",
    "question": "ל-LeakyReLU יש יתרון על ReLU בכך ש-…",
    "options": [
      "מפחית את כמות הזיכרון לשמירת אקטיבציות",
      "מאפשר מעבר גרדיאנט גם עבור ערכים שליליים קטנים",
      "מבטל צורך ב-Batch Norm",
      "מאריך את תחום הקלט לפני saturation"
    ],
    "correctAnswerIndex": 1,
    "explanation": "δ·x כאשר x<0 מונע \"מוות\" נוירונים ומקל על התכנסות ברשת עמוקה."
  },
  {
    "type": "mc",
    "question": "מדוע Batch Norm אחרי קונבולוציה אך לפני ReLU?",
    "options": [
      "מנקה רעש GPU",
      "מנרמלת את הפלט תחילה כדי ש-ReLU לא יחתוך טווחים חריגים",
      "מגדילה את ממד ה-feature map",
      "מאלצת ReLU לעבוד בתחום ‎(-1, 1)‎"
    ],
    "correctAnswerIndex": 1,
    "explanation": "נירמול •→ שיפוע יציב; הפעלה לאחר מכן מונעת קיטום ערכים חריגים בלתי-מנורמלים."
  },
  {
    "type": "open",
    "question": "כיצד Zero-Padding תורם ל-translation invariance של CNN?",
    "correctAnswerText": "הוא מאפשר לגרעין ללמוד תכונות גם בגבולות כך שהאובייקט יכול לנוע לשוליים בלי “להיעלם” בממד ה-feature.",
    "explanation": "בלעדיו, הקצה עובר חיתוך ולכן הרשת הייתה רגישה למיקום מוקצין."
  },
  {
    "type": "open",
    "question": "פרט את שני מרכיבי הפסד ב-VAE והסבר איזון משוקלל ביניהם.",
    "correctAnswerText": "Reconstruction Loss מודד דיוק לשחזר את הקלט; KL Divergence מיישר את התפלגות ה-latent לנורמלית. משקל λ: קטן מדי → יצירה חדה עם \"חורים\"; גדול מדי → פלט מטושטש אך מרחב רציף.",
    "explanation": "האיזון קובע איכות מול רגולריות במרחב הלטנטי."
  },
  {
    "type": "open",
    "question": "הסבר בקצרה את הרעיון של latent disentanglement.",
    "correctAnswerText": "כל ממד ב-z אחראי לתכונה בודדת (למשל סיבוב ראש); שינויו מניב שינוי צפוי בלי להשפיע על תכונות אחרות.",
    "explanation": "מאפשר בקרה יצירתית והעברת-סגנון נשלטת."
  },
  {
    "type": "open",
    "question": "תאר בקצרה מה מדד FID מחשב.",
    "correctAnswerText": "משווה בין התפלגות התכונות של תמונות אמיתיות לבין אלו שנוצרו על ידי הגנרטור",
    "explanation": "ערך נמוך מצביע על חפיפה גבוהה בין אמיתי וסינתטי."
  }
]

