[
  {
    "type": "mc",
    "question": "איזו פונקציית הפסד בפיקסלים מטפלת בעיקר בתדרים הנמוכים של התמונה בתרגום Pix2Pix?",
    "options": [
      "Adversarial Loss",
      "Cycle Consistency Loss",
      "L1 Loss פיקסל-לפיקסל",
      "Perceptual Loss מבוסס VGG"
    ],
    "correctAnswerIndex": 2,
    "explanation": "השוואת פיקסל-לפיקסל (L1) מעגנת את המבנה הגלובלי והצבעים – תדרים נמוכים – ומונעת שינויי מיקום."
  },
  {
    "type": "mc",
    "question": "באיזו משתי הרשתות הבאות נדרש זוג תמונות תואם (paired data) לאימון?",
    "options": [
      "Pix2Pix",
      "CycleGAN",
      "UNIT",
      "MUNIT"
    ],
    "correctAnswerIndex": 0,
    "explanation": "Pix2Pix הוא תרגום מונחה ודורש זוגות קלט-פלט כדי ללמוד התאמה ישירה."
  },
  {
    "type": "mc",
    "question": "איזה רכיב בפונקציית ההפסד של CycleGAN מחייב את המודל לשמר מבנה תוכן?",
    "options": [
      "PatchGAN Discriminator Loss",
      "Style Loss",
      "Cycle Consistency Loss",
      "Shared Latent Loss"
    ],
    "correctAnswerIndex": 2,
    "explanation": "הפסד עקיבות מעגלית מבטיח שתמונה תוחזר כמעט זהה למקור לאחר תרגום הלוך-חזור."
  },
  {
    "type": "mc",
    "question": "הנחת Shared Latent Space במודל UNIT גורסת כי…",
    "options": [
      "לכל תחום יש מרחב לטנטי ייחודי שאינו חופף",
      "קיימת הפצה לטנטית משותפת לשני התחומים",
      "רק ה-Decoders חולקים משקולות זהות",
      "Encoder אחד מספיק לשני התחומים יחד"
    ],
    "correctAnswerIndex": 1,
    "explanation": "UNIT מניח שתמונות משני תחומים ניתנות לייצוג באותו z וכך ניתן לתרגם ביניהן."
  },
  {
    "type": "mc",
    "question": "ב-MUNIT, אילו קודי ייצוג נשמרים ונדגמים בנפרד?",
    "options": [
      "קוד תוכן וקוד סגנון",
      "קוד צבע וקוד בהירות",
      "קוד עומק וקוד טקסטורה",
      "קוד רעש וקוד פיקסל"
    ],
    "correctAnswerIndex": 0,
    "explanation": "הפרדת Content ו-Style מאפשרת שילוב תוכן קבוע עם סגנונות מגוונים."
  },
  {
    "type": "mc",
    "question": "ב-CycleGAN, כמה Discriminators קיימים ומה תפקידם?",
    "options": [
      "אחד, לבחון זוגות קלט-פלט בו-זמנית",
      "שניים, אחד לכל תחום כדי לבדוק שהתוצאה נראית אמיתית בתחום היעד",
      "שלושה, לשני תחומים ולמרחב הלטנטי",
      "אף לא אחד – CycleGAN משתמש רק ב-L1"
    ],
    "correctAnswerIndex": 1,
    "explanation": "כל תחום מחזיק Discriminator משלו שבודק אם הפלט שייך להתפלגות התמונות באותו תחום."
  },
  {
    "type": "mc",
    "question": "מהו החיסרון המרכזי של שיטת Style Transfer של Gatys בזמן אמת?",
    "options": [
      "דורשת רשת גדולה ב-Inference",
      "מתבצעת אופטימיזציה איטית על פיקסלים לכל תמונה חדשה",
      "תומכת רק בתמונות בגודל ‎224×224‎ בדיוק",
      "זקוקה לזוגות תמונות תואמים"
    ],
    "correctAnswerIndex": 1,
    "explanation": "הגרדיאנט יורד על הפיקסלים במאות איטרציות ולכן אינו מתאים ליישומי זמן-אמת."
  },
  {
    "type": "mc",
    "question": "מטריצת גרם בשיטות העברת סגנון מקודדת…",
    "options": [
      "ערך ממוצע של כל פיקסל בשכבה",
      "קורלציות בין פילטרים באותה שכבה",
      "הפרשי בהירות בין שכבות עוקבות",
      "שונות מרחבית בין ערוצי צבע"
    ],
    "correctAnswerIndex": 1,
    "explanation": "הקורלציות משקפות אילו תכונות ויזואליות מופיעות יחד ובכך מייצגות טקסטורה."
  },
  {
    "type": "mc",
    "question": "Perceptual Loss מבוסס VGG נועד בעיקר…",
    "options": [
      "לשמור דיוק פיקסל-לפיקסל גבוה יותר",
      "למנוע טשטוש על-ידי השוואה במרחב פיצ'רים עמוק",
      "להחליף את Adversarial Loss ב-GANs",
      "להקטין את גודל הקלט הנדרש לרשת"
    ],
    "correctAnswerIndex": 1,
    "explanation": "מדידת מרחק ב-feature space של רשת מאומנת מראש מתיישרת טוב יותר עם תפיסת חדות אנושית."
  },
  {
    "type": "mc",
    "question": "איזה יתרון מרכזי מספק PatchGAN מעבר לחיסכון פרמטרים?",
    "options": [
      "פועל רק על תדרים נמוכים ומפחית חדות",
      "רמת דיוק גבוהה בהערכת טקסטורה עתירת-תדר",
      "מאפשר תרגום תמונות ללא צורך ב-Skip Connections",
      "מבטל את הצורך ב-L1 Loss"
    ],
    "correctAnswerIndex": 1,
    "explanation": "ה-Discriminator המקומי מתמקד בפרטים חדים ומעודד יצירת תדרים גבוהים ריאליסטיים."
  },
  {
    "type": "mc",
    "question": "U-Net מוסיף Skip Connections כדי…",
    "options": [
      "להקטין את עומק הרשת",
      "להחזיר מידע מרחבי מדויק ל-Decoder",
      "להחליף פונקציות הפעלה ב-Encoder",
      "למנוע לחלוטין שימוש ב-BatchNorm"
    ],
    "correctAnswerIndex": 1,
    "explanation": "ה-Decoder משחזר פרטים קטנים הודות למפות אקטיבציה ברזולוציה גבוהה שמוזרמות מה-Encoder."
  },
  {
    "type": "mc",
    "question": "איזה שילוב משקלי הפסד ב-Pix2Pix יוביל בדרך כלל לתוצאות חדות אך עלול לאבד מבנה?",
    "options": [
      "L1 גבוה, Adversarial נמוך",
      "L1 נמוך, Adversarial גבוה",
      "L1 = 0, Adversarial בלבד",
      "Adversarial = 0, L1 בלבד"
    ],
    "correctAnswerIndex": 1,
    "explanation": "Adversarial חזק מדגיש טקסטורה חדה אך אם L1 חלש המבנה עלול להיפגע."
  },
  {
    "type": "mc",
    "question": "ב-MUNIT ניתן ליצור מספר בלתי-מוגבל של תרגומים כי…",
    "options": [
      "מקודדים סגנון רנדומלי בכל איטרציה",
      "ה-Generator מכיל רעש לבן חופשי",
      "אין Discriminator הבודק איכות",
      "התוכן עצמו משתנה יחד עם הסגנון"
    ],
    "correctAnswerIndex": 0,
    "explanation": "דגימת קוד סגנון אקראי מהתחום היעד ובזיווגו עם תוכן קבוע מניבה וריאציות רבות."
  },
  {
    "type": "mc",
    "question": "איזו שיטה קדמה לעידן הרשתות ופתרה בעיות סגנון באמצעות Patch Matching קלאסי?",
    "options": [
      "Image Analogies",
      "UNIT",
      "AdaIN",
      "Fast Neural Style"
    ],
    "correctAnswerIndex": 0,
    "explanation": "Image Analogies מחפשת טלאים דומים בין תמונות הדוגמה ומעבירה את העיבוד לטלאים החדשים."
  },
  {
    "type": "mc",
    "question": "באיזו שכבה של VGG מודדים לרוב את Loss התוכן בתהליך Style Transfer?",
    "options": [
      "שכבה רדודה (conv1_1)",
      "שכבה עמוקה (conv4_2 או conv5_2)",
      "Fully Connected ראשונה",
      "שכבת Softmax"
    ],
    "correctAnswerIndex": 1,
    "explanation": "שכבות עמוקות מקודדות מבנה סמנטי ולכן משמשות למדידת דמיון תוכן."
  },
  {
    "type": "mc",
    "question": "מה גורם לטשטוש (blurriness) כאשר מסתמכים רק על L2 או L1 בתרגום תמונה-לתמונה?",
    "options": [
      "רשת קטנה מדי שלא לומדת טקסטורה",
      "המודל מתכנס לפתרון ממוצע של כל הפתרונות האפשריים",
      "חוסר באוגמנטציית נתונים צבעונית",
      "שימוש ב-Skip Connections ללא BatchNorm"
    ],
    "correctAnswerIndex": 1,
    "explanation": "שגיאת ממוצע פיקסלי מעדיפה פתרון ביניים שמחליק פרטים חדים."
  },
  {
    "type": "mc",
    "question": "מה תפקידו של Discriminator בתרגום מותנה (Conditional GAN) כמו Pix2Pix?",
    "options": [
      "להבחין אם תמונת פלט כלשהי נראית מציאותית באופן כללי",
      "להבחין אם זוג (קלט, פלט) תואם להתפלגות זוגות האמת",
      "להפיק קוד סגנון עבור ה-Generator",
      "למדוד רק את ההבדל הפיקסלי בין תמונות"
    ],
    "correctAnswerIndex": 1,
    "explanation": "תנאי הקלט מאפשר לדיסקרימינטור לבדוק התאמה בין הפלט לקלט ולא רק ריאליזם כללי."
  },
  {
    "type": "mc",
    "question": "ב-CycleGAN, איזה רכיב ארכיטקטוני ממיר תמונות מתחום B חזרה לתחום A?",
    "options": [
      "Generator G_AB",
      "Generator G_BA",
      "Discriminator D_A",
      "Encoder משותף"
    ],
    "correctAnswerIndex": 1,
    "explanation": "שני גנרטורים פועלים בכיוונים מנוגדים; G_BA אחראי להמרה חזרה לתחום A."
  },
  {
    "type": "mc",
    "question": "איזו טכניקה מאפשרת ברשתות Style Transfer מהירות (Fast Neural Style) לעבד בזמן אמת?",
    "options": [
      "אופטימיזציה ארוכה על פיקסלים",
      "רשת Feed-Forward מאומנת מראש לכל סגנון",
      "Shared Latent Space",
      "PatchGAN Discriminator קטן"
    ],
    "correctAnswerIndex": 1,
    "explanation": "מאמנים רשת אחת מראש ולאחר מכן מעבירים בה תמונות חדשות ב-Inference יחיד."
  },
  {
    "type": "mc",
    "question": "באיזו שיטה נעשה שימוש מפורש בזוג Encoders נפרדים לתוכן ולסגנון?",
    "options": [
      "UNIT",
      "MUNIT",
      "CycleGAN",
      "Pix2Pix"
    ],
    "correctAnswerIndex": 1,
    "explanation": "MUNIT כולל Content Encoder ו-Style Encoder לכל תחום."
  },
  {
    "type": "mc",
    "question": "מהו האיזון האופטימלי בערכי α ו-β ב-Style Transfer אם רוצים הדגשת סגנון חזקה?",
    "options": [
      "α גבוה, β נמוך",
      "α נמוך, β גבוה",
      "α = β = 0",
      "α = β = ∞"
    ],
    "correctAnswerIndex": 1,
    "explanation": "בטא גבוה מגדיל את משקל Loss הסגנון יחסית ל-α ועל כן הסגנון מודגש על חשבון מבנה התוכן."
  },
  {
    "type": "open",
    "question": "תאר כיצד Weight Trade-off בין L1 ל-Adversarial משפיע על חדות מול נאמנות לקלט במודל Pix2Pix.",
    "correctAnswerText": "L1 גבוה מבטיח התאמה פיקסלית ולכן שומר מבנה אך יוצר טשטוש, בעוד Adversarial גבוה מחדד טקסטורה אך עלול לעוות את המבנה. בחירת משקלים מאוזנת מייצרת פלט חד ששומר על פריסת האובייקטים.",
    "explanation": "איזון קובע אם הפלט יהיה חד וריאליסטי אך אולי חופשי מדי, או צמוד מאוד לקלט אך מטושטש."
  },
  {
    "type": "open",
    "question": "הסבר את חשיבות Cycle Consistency Loss ב-CycleGAN עבור תרגום ללא זוגות תואמים.",
    "correctAnswerText": "כיוון שאין פלט אמת לכל קלט, דרישת עקיבות מעגלית מאלצת שהמרה הלוך-חזור תחזיר את התמונה המקורית; בכך הרשת לומדת לשמר תוכן ולא רק לייצר תמונות סגנוניות אמינות.",
    "explanation": "אילוץ זה מונע מה-Generator להתעלם מהקלט ומחייבו ללמוד מיפוי דו-כיווני עקבי."
  },
  {
    "type": "open",
    "question": "מהו ההסבר לשימוש במטריצת גרם לחישוב Style Loss בשיטת Gatys?",
    "correctAnswerText": "מטריצת גרם מודדת קורלציות בין פילטרים בתוך שכבה, וכך לוכדת אילו דפוסים ויזואליים מופיעים יחד בלי קשר למיקומם. התאמת מטריצות גרם של הסגנון והפלט מש复ה טקסטורה וצבעים באופן נייטרלי למיקום.",
    "explanation": "הקורלציות מייצגות סגנון באופן אינסופי-מיקומי ולכן מתאימות להחלתו על תוכן אחר."
  },
  {
    "type": "open",
    "question": "פרט את ההבדלים העיקריים בדרישות הדאטה בין Pix2Pix ל-CycleGAN.",
    "correctAnswerText": "Pix2Pix זקוק לזוגות תמונות מתואמים (paired) כדי ללמוד מיפוי ישיר, בעוד CycleGAN עובד עם שני אוספים לא-מזוּוָגים (unpaired) ומסתמך על Cycle Consistency ו-GANs דו-כיווניים כדי ללמוד.",
    "explanation": "הבדל בזמינות זוגות משפיע על בחירת המודל למשימה מעשית."
  },
  {
    "type": "open",
    "question": "כיצד שלב Cross-Domain Translation ב-MUNIT יוצר וריאציות מרובות לתמונה אחת?",
    "correctAnswerText": "שומרים את קוד התוכן המקורי וממזגים אותו בכל פעם עם קוד סגנון חדש שנדגם אקראית מהתחום היעד. Decoder היעד מפענח את הצירוף לייצר פלט שונה בסגנון אך זהה במבנה.",
    "explanation": "שינוי רק רכיב הסגנון מאפשר אינסוף קומבינציות על אותו תוכן."
  },
  {
    "type": "open",
    "question": "מדוע PatchGAN Discriminator מאפשר עבודה עם תמונות ברזולוציה גבוהה מבלי להגדיל את גודל הרשת?",
    "correctAnswerText": "ה-Discriminator הוא מודול קונבולוציוני קטן שמחליק על התמונה; מספר הפרמטרים קבוע וגודל התמונה רק מגדיל את מספר ה-Patches הנבדקים, לא את משקלות הרשת.",
    "explanation": "כך מתקבל חיסכון בזיכרון ובמהירות, ותמונות גדולות נבדקות מקומית."
  },
  {
    "type": "open",
    "question": "הגדר בקצרה את הנחת Shared Latent Space וכיצד היא ממומשת ב-UNIT.",
    "correctAnswerText": "ההנחה אומרת שתמונות משני תחומים ניתנות לייצוג זהה במרחב לטנטי. ב-UNIT כל תחום מחזיק Encoder שממפה לתוך z משותף ו-Decoder שממפה חזרה, ואילוצי GAN/השוואת שחזור מכריחים את שני ה-Encoders לייצר קודים מאוחדים.",
    "explanation": "ייצוג משותף זה מאפשר תרגום ללא זוגות תואמים."
  },
  {
    "type": "open",
    "question": "כיצד Perceptual Loss פותר את בעיית הטשטוש שנגרמת משימוש ב-L2 בלבד?",
    "correctAnswerText": "במקום להשוות פיקסלים, Perceptual Loss מודד מרחק במרחב הפיצ'רים של רשת סיווג; בכך הוא מעניש הבדלים מבניים וטקסטורליים ומשמר פרטים חדים שה-L2 ממוצע עליהם.",
    "explanation": "Feature Space רגיש לטקסטורה ולכן מחזיר חדות וחוויית ראייה טבעית."
  },
  {
    "type": "open",
    "question": "תאר בקצרה את תהליך Image Analogies הקלאסי להעברת סגנון ללא רשתות.",
    "correctAnswerText": "השיטה מקבלת זוג תמונות A ו-A' (מקור וסגנון) ומוצאת עבור כל Patch בתמונה חדשה B את ה-Patch הדומה ביותר ב-A. התאמה זו מועתקת מ-A' ל-B', וכך מתקבל עיבוד בסגנון A' עבור תוכן B.",
    "explanation": "Patch Matching על פני פירמידת רזולוציות מאפשר העברת סגנון מדוגמה אחת לאחרת."
  }
]
