[
  {
    "type": "mc",
    "question": "מהו הרעיון המרכזי של Single-Image Learning שהודגש בהרצאה?",
    "options": [
      "שימוש במיליוני תמונות כדי ללמוד סגנון אחיד",
      "ניצול חזרתיות פנימית בטלאים של תמונה בודדת כדי ללמוד ממנה לבדה",
      "החלפת כל שכבות הקונבולוציה בשכבות Fully Connected",
      "חיבור בין שתי תמונות שונות ליצירת תבנית ביניים"
    ],
    "correctAnswerIndex": 1,
    "explanation": "ב־Single-Image Learning מנצלים את העובדה שה-patchים בתמונה מופיעים שוב ושוב ברמות קנה-מידה שונות, ולכן ניתן ללמוד מבנה ומרקם גם ללא דאטה חיצוני."
  },
  {
    "type": "mc",
    "question": "מהו ה-prior שעליו מסתמכת שיטת Deep Image Prior (DIP)?",
    "options": [
      "Regularization חיצוני המבוסס על Gram Matrix",
      "מבנה הרשת הקונבולוציונית עצמו שמעדיף תבניות טבעיות על רעש",
      "מאגר גדול של תמונות מאומנות מראש",
      "טכניקת Data Augmentation אינטנסיבית בחלון 70×70"
    ],
    "correctAnswerIndex": 1,
    "explanation": "גם עם משקולות אקראיים, ארכיטקטורת CNN נוטה לשחזר מבנים מסודרים יותר מרעש ולכן פועלת כ-prior פנימי."
  },
  {
    "type": "mc",
    "question": "באימון DIP מבוצעת האופטימיזציה על…",
    "options": [
      "פיקסלי התמונה עצמה בכל איטרציה",
      "משקולות הרשת בלבד בעוד קלט הרעש נשאר קבוע",
      "וקטור הלטנט Z שמשתנה כל צעד",
      "ה-Discriminator של GAN חיצוני"
    ],
    "correctAnswerIndex": 1,
    "explanation": "בדיפ האופטימיזציה מעדכנת את θ — פרמטרי הרשת — בעוד שהקלט z הוא וקטור רעש קבוע."
  },
  {
    "type": "mc",
    "question": "למה נדרש Early Stopping ב-DIP לפי ההרצאה?",
    "options": [
      "כדי למנוע מאפס-גרדיאנט בשכבות עמוקות",
      "כדי לעצור לפני שהרשת תלמד גם את הרעש והפגמים",
      "כדי לחסוך זמן ולאמן פחות משכבות",
      "כדי להקטין את זיכרון ה-GPU הדרוש לתמונה"
    ],
    "correctAnswerIndex": 1,
    "explanation": "אם ממשיכים יותר מדי איטרציות, הרשת תתאים את עצמה גם לרעש שבקלט ותאבד את אפקט הסינון."
  },
  {
    "type": "mc",
    "question": "Inpainting ב-DIP מבוצע על-ידי…",
    "options": [
      "חישוב הפסד על כל הפיקסלים כולל האזורים החסרים",
      "התעלמות מהאזורים החסרים בעזרת מסכה בינארית ולמידה רק על הפיקסלים הידועים",
      "הזרקת רעש חדש בכל איטרציה במקום התמונה החסרה",
      "הוספת Discriminator מקומי לאזור המסכה"
    ],
    "correctAnswerIndex": 1,
    "explanation": "ה-loss מחושב רק על הפיקסלים שמחוץ למסכה, והרשת משלימה את החסר בעזרת prior המבנה שלה."
  },
  {
    "type": "mc",
    "question": "איזה קשר קיים בין Patch Entropy להצלחת DIP?",
    "options": [
      "אנטרופיה גבוהה מקלה על הרשת ללמוד מהר",
      "אנטרופיה נמוכה מסייעת לרשת לשחזר מבנים חזרתיים בדיוק גבוה",
      "אנטרופיה אינה משפיעה על תהליך השחזור",
      "רק אנטרופיה שלילית מאפשרת Early Stopping"
    ],
    "correctAnswerIndex": 1,
    "explanation": "ככל שה-patchים דומים זה לזה (אנטרופיה נמוכה) קל יותר לרשת לשחזר את התמונה ולסנן רעש."
  },
  {
    "type": "mc",
    "question": "Double-DIP מפרק תמונה לשכבות שונות באמצעות…",
    "options": [
      "GAN יחיד עם Discriminator גלובלי",
      "מספר רשתות DIP נפרדות בתוספת Exclusion Loss בין הרכיבים",
      "רשת יחידה עם Dropout גבוה במיוחד",
      "חישוב PCA ישיר על פיקסלי התמונה"
    ],
    "correctAnswerIndex": 1,
    "explanation": "כל רכיב משוחזר ע\"י DIP ייעודי, ו-Exclusion Loss מבטיח שהרכיבים לא יחפפו סטטיסטית."
  },
  {
    "type": "mc",
    "question": "מה תפקיד Entropy Loss במסכה ב-Double-DIP?",
    "options": [
      "לכפות על המסכה להיות חלקה (Smooth)",
      "לעודד את המסכה להיות בינארית וברורה",
      "להוסיף רעש אקראי למסכה לחיזוק ההפרדה",
      "להחליף את הצורך ב-Exclusion Loss"
    ],
    "correctAnswerIndex": 1,
    "explanation": "מסכה בינארית בעלת אנטרופיה נמוכה מבהירה אילו פיקסלים שייכים לכל רכיב וכך משפרת את הפירוק."
  },
  {
    "type": "mc",
    "question": "ב-SinGAN מאמנים…",
    "options": [
      "רשת אחת ברזולוציה המקורית בלבד",
      "פירמידה של זוגות Generator-Discriminator, אחד לכל רזולוציה",
      "שלושה Generators בשכבות Style שונות",
      "Encoder יחיד עם Loss פיקסלי בלבד"
    ],
    "correctAnswerIndex": 1,
    "explanation": "לכל סקאלה יש גנרטור ומפלה הלומדים מאותה תמונה מוקטנת, והאימון פרוגרסיבי כלפי מעלה."
  },
  {
    "type": "mc",
    "question": "היתרון הבולט של SinGAN הוא…",
    "options": [
      "דרישה למאגר תמונות ענק לאימון",
      "יכולת ללמוד סגנון ומבנה מתמונה בודדת וליצור וריאציות חדשות",
      "תהליך אימון מהיר על כל תמונה",
      "ביטול מוחלט של ארטיפקטים בכל מצב"
    ],
    "correctAnswerIndex": 1,
    "explanation": "SinGAN לומד מהחזרתיות הפנימית של תמונה אחת בלבד ומסוגל להפיק דוגמאות חדשות בסגנון זה."
  },
  {
    "type": "mc",
    "question": "באיזו נקודה מוסיף StyleGAN את הווקטור w לשכבות הגנרטור?",
    "options": [
      "רק בשכבה הראשונה של Upsampling",
      "בכל שכבה דרך מנגנון AdaIN המווסת ערוצי-תכונה",
      "רק בשכבה הסופית לפני Tanh",
      "ב־Discriminator במקום ב-Generator"
    ],
    "correctAnswerIndex": 1,
    "explanation": "AdaIN מזריק את וקטור הסגנון לכל שכבה וכך שולט בהיבטים שונים של התמונה במדרג."
  },
  {
    "type": "mc",
    "question": "StyleGAN2 החליף את AdaIN במנגנון…",
    "options": [
      "Batch Normalization גלובלי",
      "Modulation + Demodulation להפחתת ארטיפקטים",
      "Dropout מבוקר",
      "Spatial Transformer בפלט"
    ],
    "correctAnswerIndex": 1,
    "explanation": "Mod/Demod מרכך סטטיסטיקות ערוצים ומקטין קווים לא רצויים שהתגלו ב-StyleGAN1."
  },
  {
    "type": "mc",
    "question": "GANSpace מגלה כיווני עריכה סמנטיים על-ידי…",
    "options": [
      "חיפוש גרדיאנט טקסטואלי דרך CLIP",
      "PCA על האקטיבציות הפנימיות של StyleGAN ללא תוויות",
      "אימון Mapper מפוקח עם סט תוויות גיל ומגדר",
      "שילוב Exclusion Loss בין שכבות"
    ],
    "correctAnswerIndex": 1,
    "explanation": "ניתוח השונות הגבוהה במרחב הפנימי מגלה צירים המשנים מאפיינים כמו תנוחת ראש או תאורה."
  },
  {
    "type": "mc",
    "question": "בשיטת StyleCLIP – Latent Optimization, מאופטם…",
    "options": [
      "משקולות הגנרטור כולו מחדש",
      "הקוד הלטנטי של תמונה נתונה כך שתתקרב לטקסט ב-CLIP Space",
      "רק שכבת ה-Discriminator הסופית",
      "וקטור רעש ב-StyleGAN’s Z בלבד ללא מעבר ב-W"
    ],
    "correctAnswerIndex": 1,
    "explanation": "Gradient Descent על הקוד הלטנטי מזיז את התמונה לכיוון התיאור הטקסטואלי המבוקש."
  },
  {
    "type": "mc",
    "question": "Global Direction ב-StyleCLIP מגדיר כיוון קבוע במרחב…",
    "options": [
      "Z כדי לשנות טקסטורה דקיקה",
      "S (Style) כדי להחיל שינוי סמנטי בצורה ישירה ומהירה",
      "W בלבד תוך שימוש ב-AdaIN",
      "Latent Mapper מאומן בנפרד"
    ],
    "correctAnswerIndex": 1,
    "explanation": "הכיוון מחושב פעם אחת בעזרת CLIP ואז מוחל מידית על כל תמונה דרך הזזת ה-style codes."
  },
  {
    "type": "mc",
    "question": "אחת המגבלות שהוזכרו לעריכת תכונות בקווים לינאריים במרחב הלטנט היא…",
    "options": [
      "דרישת זיכרון חריגה בזמן ריצה",
      "תכונות מסוימות אינן ניתנות לייצוג מדויק באמצעות כיוון לינארי יחיד",
      "אי-יכולת לשלוט על עוצמת העריכה",
      "היעדר תכונות סמנטיות במרחב W"
    ],
    "correctAnswerIndex": 1,
    "explanation": "תכונות מורכבות עלולות להיות לא-לינאריות; תזוזה בקו ישר יכולה לגרור שינויים בלתי-רצויים."
  },
  {
    "type": "mc",
    "question": "Latent Mapper ב-StyleCLIP מאפשר…",
    "options": [
      "עריכה מהירה בזמן אמת לאחר אימון חד-פעמי",
      "דיוק גבוה יותר ממסלול Latent Optimization",
      "ביטול הצורך ב-CLIP בזמן ריצה",
      "יצירת תמונות ללא הגבלת גודל"
    ],
    "correctAnswerIndex": 0,
    "explanation": "לאחר שה-Mapper מאומן, כל עריכה היא pass יחיד ולכן מהירה, אך פחות מותאמת אישית."
  },
  {
    "type": "open",
    "question": "תאר כיצד חזרתיות פנימית של טלאים מסייעת ל-Single-Image Learning.",
    "correctAnswerText": "בתוך כל תמונה מופיעים טלאים דומים שוב ושוב בקני-מידה שונים. מודל הלומד מהתמונה בלבד יכול לאתר את התבניות החוזרות וכך לחזות מידע חסר או לשחזר מרקם, ללא צורך בדוגמאות חיצוניות.",
    "explanation": "הסטטיסטיקה הפנימית מספקת 'דאטה' מספק בתוך התמונה עצמה."
  },
  {
    "type": "open",
    "question": "מדוע Exclusion Loss נדרש ב-Double-DIP?",
    "correctAnswerText": "Exclusion Loss מעניש חפיפה סטטיסטית בין הרכיבים שנותחו על-ידי שתי רשתות DIP, וכך מבטיח שכל רשת תלמד חלק שונה של התמונה (למשל אובייקט מול רקע) במקום ששתיהן ישחזרו אותו אזור.",
    "explanation": "ללא אילוץ, שתי הרשתות עלולות להתמקד באותו רכיב ולא להשיג פירוק אמיתי."
  },
  {
    "type": "open",
    "question": "הסבר בקצרה את תהליך האימון הפרוגרסיבי ב-SinGAN ולמה הוא חשוב.",
    "correctAnswerText": "האימון מתחיל בגרסה קטנה מאוד של התמונה; גנרטור-מפלה לומדים מבנה גלובלי. לאחר התייצבות, עוברים לרזולוציה גבוהה יותר ומוסיפים פרטים על בסיס הפלט מהשלב הקודם. כך המודל לומד בהדרגה הן תכונות גסות והן טקסטורות עדינות.",
    "explanation": "חלוקת המשימה מקלה על ה-GAN ללמוד בלי לקרוס ומאפשרת יציבות וגיוון."
  },
  {
    "type": "open",
    "question": "הבדל בין מרחבי W, W+ ו-S ב-StyleGAN כפי שהוסבר בהרצאה.",
    "correctAnswerText": "W הוא מרחב ביניים גמיש שמתקבל מרשת מיפוי; W+ נותן וקטור נפרד לכל שכבה וכך מאפשר שליטה מדויקת אך פחות מפוצלת; S הוא מרחב לאחר מודולציה, נחשב עוד יותר disentangled ומתאים לעריכות נקיות.",
    "explanation": "הבחירה במרחב משפיעה על רמת השליטה וה-leakage בין תכונות."
  },
  {
    "type": "open",
    "question": "כיצד Modulation + Demodulation ב-StyleGAN2 מפחית ארטיפקטים יחסית ל-AdaIN?",
    "correctAnswerText": "המנגנון משתמש ב-scale שהופק מ-w כדי למודולץ את הפילטר ואז מנרמל (demodulate) חזרה, מה שמאזן סטטיסטיקות הערוץ ומונע קווים דמויי קופסה שהתגלו ב-StyleGAN1.",
    "explanation": "שימור ממוצע ואנרגיה אחידה בערוצים מפחית תבניות לא רצויות."
  },
  {
    "type": "open",
    "question": "פרט שלושה צעדים עיקריים בגישת Global Direction ב-StyleCLIP.",
    "correctAnswerText": "1. מגדירים כיוון סמנטי ב-CLIP על-ידי הבדל בין שני טקסטים.\n2. מודדים עבור כל ערוץ במרחב S איך שינוי קטן משפיע על הכיוון ב-CLIP.\n3. בוחרים ערוצים מועילים ומרכיבים וקטור כיוון סגנון שמוחל על תמונות חדשות.",
    "explanation": "השיטה מצרפת כוח חישובי חד-פעמי עם יישום מיידי על תמונות רבות."
  },
  {
    "type": "open",
    "question": "מהי המגבלה שההרצאה ציינה לגבי עריכה לינארית במרחב הלטנט של GAN?",
    "correctAnswerText": "הנחה שתכונה מיוצגת בכיוון לינארי יחיד אינה תמיד מדויקת; לעיתים תכונה מורכבת ולכן תזוזה ישרה משפיעה גם על תכונות אחרות ויוצרת תוצאות בלתי-רצויות.",
    "explanation": "תלות הדדית וביאס בדאטה גורמים לערבוב תכונות לאורך קווים 'ישרים'."
  },
  {
    "type": "open",
    "question": "מה היתרון ומה החיסרון של Latent Mapper בהשוואה ל-Latent Optimization ב-StyleCLIP?",
    "correctAnswerText": "יתרון: העריכה מתבצעת במהירות בזמן ריצה הודות ל-forward יחיד; חיסרון: היא פחות מותאמת אישית ועלולה להיות פחות מדויקת כי המיפוי נלמד באופן כללי מראש.",
    "explanation": "Mapper = מהיר אך גנרי; Optimization = איטי אך מותאם לדוגמה הספציפית."
  }
]
