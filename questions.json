[
  {
    "type": "mc",
    "question": "מהי המטרה העיקרית של שכבת הקונבולוציה ב-CNN?",
    "options": [
      "לצמצם ממדים על-ידי בחירת ערך בודד מכל חלון קטן",
      "לחלץ תכונות מקומיות בעזרת גרעינים קבועי-גודל המחליקים על התמונה",
      "למפות את הפלט להסתברויות סופיות עבור כל מחלקה אפשרית",
      "לאפס ערכים שליליים כדי לשמור מידע חיובי בלבד"
    ],
    "correctAnswerIndex": 1,
    "explanation": "גרעיני הקונבולוציה מבצעים מכפלה קונבולוציונית בחלונות מקומיים וכך יוצרים feature maps המציגות קצוות, טקסטורות וצורות."
  },
  {
    "type": "mc",
    "question": "באיזו שכבה ב-CNN מצמצמים את ממדי הנתונים תוך שמירה על תכונות חשובות?",
    "options": [
      "שכבת קונבולוציה עם stride גדול במיוחד",
      "שכבת הפעלה Sigmoid לא-לינארית",
      "שכבת Pooling המחלקת את המפה לחלונות ומחלצת ערך נציג",
      "שכבת Fully Connected עם כמות נוירונים מוגבלת"
    ],
    "correctAnswerIndex": 2,
    "explanation": "בשכבת Pooling מבוצע down-sampling (למשל Max או Average Pooling) ולכן המפה קטנה אך שומרת מידע קריטי."
  },
  {
    "type": "mc",
    "question": "מהו תפקיד פונקציית ReLU בהקשר של CNN?",
    "options": [
      "לנרמל את פלטי השכבות לטווח (0, 1) בדיוק",
      "להחליף ערכים חיוביים בערכים שליליים כדי להדגיש ניגוד",
      "להכניס אי-לינאריות על-ידי איפוס ערכים שליליים ושמירת החיוביים",
      "לבצע דגימה אקראית המונעת התאמה-יתרה בזמן האימון"
    ],
    "correctAnswerIndex": 2,
    "explanation": "ReLU (x → max(0,x)) משאיר רק ערכים חיוביים, שובר ליניאריות ומפחית סכנת vanishing gradients."
  },
  {
    "type": "mc",
    "question": "מהו ההבדל העיקרי בין גרפיקה ממוחשבת לראייה ממוחשבת לפי ההרצאה?",
    "options": [
      "החישוב בגרפיקה נעשה תמיד בזמן-אמת בעוד בראייה הוא אופליין",
      "ראייה יוצרת מודלים תלת-ממדיים וגרפיקה מנתחת תמונות קיימות",
      "גרפיקה מייצרת תמונות מסצנות; ראייה מנתחת תמונות להבנת תוכן",
      "שתי התחומים זהים פרט לאופן אחסון הנתונים בקבצים"
    ],
    "correctAnswerIndex": 2,
    "explanation": "גרפיקה “מציירת” מתוך מודלים מתמטיים; ראייה “מפענחת” מידע מתמונות אל העולם."
  },
  {
    "type": "mc",
    "question": "איזה יתרון מרכזי מספקים Feature Descriptors קלאסיים (HOG, Color Histograms)?",
    "options": [
      "מאפשרים דגימה ישירה ממרחב לטנטי רציף",
      "מפחיתים מורכבות על-ידי שמירת הנתונים המשמעותיים בלבד",
      "מבטלים את הצורך ברשתות עמוקות ללמידה",
      "משפרים את ה-dropout ומונעים התאמה-יתרה"
    ],
    "correctAnswerIndex": 1,
    "explanation": "המתארים מייצרים וקטור קומפקטי של מאפיינים מרכזיים ולכן מקטינים מימד ונטל חישובי."
  },
  {
    "type": "mc",
    "question": "למידה בלתי-מונחית (Unsupervised) נבדלת ממונחית בכך שהמודל…",
    "options": [
      "מזהה דפוסים סמויים בדאטה ללא תוויות רקע כלשהן",
      "מתבסס תמיד על פונקציית Softmax בסיום הרשת",
      "דורש מערכות תגמול חיצוניות מבוססות חיזוקים",
      "משתמש רק בבקרת dropout לשמירת כלליות"
    ],
    "correctAnswerIndex": 0,
    "explanation": "ללא תוויות, המודל מגלה מבנה פנימי (קיבוץ, הפחתת ממדים) במקום למפות לקלאסים ידועים."
  },
  {
    "type": "mc",
    "question": "מדוע AutoEncoder רגיל מוגבל ביצירת דוגמאות חדשות?",
    "options": [
      "אינו מכיל שכבות קונבולוציה ולכן מפיק תמונות מטושטשות",
      "מקודד כל דגימה לנקודה יחידה, ולכן המרחב הלטנטי אינו רציף",
      "מסתמך על רעש אחיד במקום על התפלגות גאוסיאנית",
      "משתמש ב-KL Divergence כמרכיב פסד יחיד"
    ],
    "correctAnswerIndex": 1,
    "explanation": "נקודות בדידות יוצרות “חורים” במרחב; דגימה ביניהן מובילה לאובייקטים לא-מציאותיים."
  },
  {
    "type": "mc",
    "question": "מה מוסיף VAE ביחס לאוטואנקודר רגיל?",
    "options": [
      "שכבת Pooling חדשה המפחיתה רעשים בתמונה",
      "הטלת אילוץ שהייצוג הלטנטי יתפלג כ-N(0,1) רציף",
      "החלפה של ReLU ב-LeakyReLU בכל השכבות",
      "כפל רשת מפלה לבקרת איכות הפלט"
    ],
    "correctAnswerIndex": 1,
    "explanation": "על-ידי KL Divergence המודל מקרב את התפלגות q(z|x) לנורמלית סטנדרטית, ומאפשר דגימה רציפה."
  },
  {
    "type": "mc",
    "question": "איזו נוסחה מיישמת את Reparameterization Trick ב-VAE?",
    "options": [
      "z = σ · ε + μ, ε ~ Uniform(-1,1)",
      "z = ε + σ · μ, ε ~ N(0,1)",
      "z = μ + σ · ε, ε ~ N(0,1)",
      "z = μ · σ + ε, ε ~ Laplace(0,1)"
    ],
    "correctAnswerIndex": 2,
    "explanation": "נוסחה זו מבטיחה כי z נגזרת ממשתנים נפרדים (μ, σ) ו-ε הניתן להחזרה בשרשרת הגרדיאנטים."
  },
  {
    "type": "mc",
    "question": "ב-GAN מי אחראי להבחין בין דוגמאות אמיתיות למזויפות?",
    "options": [
      "Encoder בעל שכבות BatchNorm רצופות",
      "Discriminator הלומד לסווג כנכונות או כנכונות-שווא",
      "Generator המפיק רעש מותאם והופכו לתמונה",
      "רשת Fully Connected המשמשת כבקר-איכות"
    ],
    "correctAnswerIndex": 1,
    "explanation": "המפלה מקבלת גם דוגמאות אמיתיות וגם פלטי הגנרטור ומעדכן את עצמו לשפר אבחנה ביניהן."
  },
  {
    "type": "mc",
    "question": "מהי בעיית Mode Collapse באימון GAN?",
    "options": [
      "המפלה מפסיק לקבל גרדיאנט ולכן נעצר",
      "נצפה פיזור יתר של הדוגמאות הלטנטיות",
      "הגנרטור מייצר שוב ושוב וריאציה אחת במקום מגוון רחב",
      "פונקציית ההפסד מתכנסת לערך אפסי מהר מדי"
    ],
    "correctAnswerIndex": 2,
    "explanation": "הגנרטור “נתקע” במצב שמספר מצומצם של דוגמאות מטעות את המפלה, ולכן הגיוון אובד."
  },
  {
    "type": "mc",
    "question": "איזו טכניקה מרכזית ב-DCGAN משפרת יציבות אימון?",
    "options": [
      "שימוש עקבי בפונקציית Tanh ב-Discriminator",
      "הפחתה ליניארית של גודל ה-batch כל 5 איטרציות",
      "החלפת שכבות Fully Connected בשכבות קונבולוציה עמוקות",
      "תוספת רעש לבן לפילטרים בכל שכבה"
    ],
    "correctAnswerIndex": 2,
    "explanation": "מבנה קונבולוציוני מאפשר ניצול מבנה תמונה ומקטין רגישות לפרמטרים, ובשילוב Batch Norm מתקבלת יציבות טובה יותר."
  },
  {
    "type": "mc",
    "question": "מה מודד Fréchet Inception Distance (FID)?",
    "options": [
      "השונות הכוללת של ערכי הפיקסלים בין דוגמאות",
      "המרחק בין התפלגויות תכונות של תמונות אמיתיות ומזויפות",
      "משך האימון הנדרש להגיע לאיכות סבירה",
      "מספר המצבים הסטטיסטיים בהם התרחש Mode Collapse"
    ],
    "correctAnswerIndex": 1,
    "explanation": "FID מחשב מרחק פרמאטרי בין גאוסיאנים במרחב תכונות של Inception Net; ערך נמוך משמעו איכות גבוהה."
  },
  {
    "type": "mc",
    "question": "איזו שכבה ב-CNN ממירה את הפלט להסתברויות עבור כל מחלקה?",
    "options": [
      "שכבת Pooling בעלת חלון 2×2",
      "שכבת Softmax בסיום ה-Fully Connected",
      "שכבת הפעלה ReLU לפני ה-decoder",
      "שכבת Batch Norm המנרמלת את האינדקס"
    ],
    "correctAnswerIndex": 1,
    "explanation": "Softmax מיישרת את הערכים לסכום 1 ומאפשרת פרשנות הסתברותית לסיווג."
  },
  {
    "type": "mc",
    "question": "היכן נעשה שימוש ב-KL Divergence בפונקציית הפסד של VAE?",
    "options": [
      "לקירוב התפלגות הפלט ל-Uniform(0,1)",
      "להבטיח שהתפלגות ה-encoder קרובה ל-N(0,1)",
      "למדידת טשטוש בפלט ה-decoder",
      "להשוואת הגנרטור למפלה ב-GAN"
    ],
    "correctAnswerIndex": 1,
    "explanation": "רכיב KL מעניש סטיות מנורמלית סטנדרטית וכך מטפח מרחב לטנטי חלק ורציף."
  },
  {
    "type": "mc",
    "question": "מהו היתרון הבולט של אינטרפולציה במרחב הלטנטי של VAE?",
    "options": [
      "יצירת רצף חלק של דוגמאות אמיתיות-למחצה בין שתי נקודות",
      "שמירה על חדות-על בהשוואה ל-GAN",
      "הפחתת זמן האימון בשליש",
      "ביטול הצורך בתגיות בפאזה המפוקחת"
    ],
    "correctAnswerIndex": 0,
    "explanation": "רציפות המרחב מאפשרת מעבר מדורג בין מאפייני דוגמאות, שימושי להדגמה ולערבוב תוכן."
  },
  {
    "type": "mc",
    "question": "איזה שילוב רכיבים יוצר משחק סכום-אפס ב-GAN?",
    "options": [
      "Dropout מול Batch Norm במקביל",
      "ReLU מול Sigmoid באותה שכבה",
      "Generator השואף להטעות מול Discriminator השואף לזהות",
      "Fully Connected מול Convolutional במפלה"
    ],
    "correctAnswerIndex": 2,
    "explanation": "כל רשת מנסה למקסם פונקציית מטרה הפוכה לשנייה, ולכן הסכום הכולל הוא אפס."
  },
  {
    "type": "mc",
    "question": "מדוע Batch Normalization חשוב ב-DCGAN?",
    "options": [
      "מייצב את הפצת האקטיבציות ומאיץ התכנסות",
      "מחליף את הצורך בפונקציות הפעלה לא-לינאריות",
      "מפחית גודל kernel בלי לאבד מידע",
      "מוסיף דגימה רנדומלית למרחב הלטנטי"
    ],
    "correctAnswerIndex": 0,
    "explanation": "נרמול הפיצ’רים בכל מיני-batch שומר טווח ערכים עקבי, מקל על למידה ומזער סטיות רחבות."
  },
  {
    "type": "mc",
    "question": "כיצד CNN לומדת היררכיה של תכונות?",
    "options": [
      "מתחילה בשכבות Fully Connected ומסיימת בקונבולוציה",
      "שכבות ראשונות מזהות קצוות; עמוקות מזהות צורות מורכבות",
      "בכל שכבה משמרת בדיוק את אותה רמת פירוט",
      "רק עם Pooling ללא שכבות הפעלה"
    ],
    "correctAnswerIndex": 1,
    "explanation": "המודל בונה תכונות בסיס לקומפוזיציות מורכבות בהדרגה, לכן הקצוות משמשים לבניית אובייקטים שלמים."
  },
  {
    "type": "mc",
    "question": "מהו שימוש מרכזי אחד במודלים גנרטיביים לפי ההרצאה?",
    "options": [
      "הפחתת רזולוציית תמונה לחיסכון בזיכרון",
      "הגדלת מערכי נתונים בעת מחסור בדוגמאות מסומנות",
      "השבתת שכבות כדי למנוע התאמה-יתרה",
      "סיווג מהיר של תמונות בזמן-אמת"
    ],
    "correctAnswerIndex": 1,
    "explanation": "יצירת דוגמאות סינתטיות מגבירה גיוון ויכולה לשמש לאימון משופר במצב דאטה-דל."
  },
  {
    "type": "mc",
    "question": "איזו בעיה מופיעה כאשר הגנרטור חזק מדי והמפלה חלש מדי?",
    "options": [
      "Mode Collapse קבוע ולא ניתן לתיקון",
      "Vanishing Gradient היוצר גרדיאנטים זעירים במפלה",
      "התפלגות N(0,1) הופכת ל-Uniform",
      "FID גדל למרות ש-Inception Score קטן"
    ],
    "correctAnswerIndex": 1,
    "explanation": "מפלה כמעט לא לומד, הגנרטור לא מקבל פידבק משמעותי, והאימון “קופא”."
  },
  {
    "type": "mc",
    "question": "איזו פונקציית הפעלה מועדפת בגנרטור של DCGAN לפי ההמלצות?",
    "options": [
      "LeakyReLU בכל השכבות",
      "Sigmoid לשכבות הפנימיות",
      "ReLU לשכבות הפנימיות, Tanh לפלט",
      "Softplus להבטחת רציפות"
    ],
    "correctAnswerIndex": 2,
    "explanation": "ReLU מפיק אקטיבציות לא-שליליות, בעוד Tanh בטווח (-1, 1) מתאים לפיקסלים מנורמלים."
  },
  {
    "type": "mc",
    "question": "איזו טכניקה מסייעת למנוע התאמה-יתרה ב-Fully Connected?",
    "options": [
      "Dropout הכבה אקראית של נוירונים בזמן האימון",
      "Zero-padding להגדלת קלט השכבה",
      "Upsampling לשכבת הקונבולוציה הקודמת",
      "שימוש ב-Stride גדול מאוד בפילטר"
    ],
    "correctAnswerIndex": 0,
    "explanation": "Dropout מאלץ את הרשת לא להסתמך יתר־על-המידה על נוירונים בודדים וכך משפר הכללה."
  },
  {
    "type": "open",
    "question": "הסבר בקצרה כיצד Supervised Learning שונה מ-Unsupervised Learning.",
    "correctAnswerText": "בלמידה מונחית המודל מתאמן על נתונים עם תוויות ידועות מראש ומלמד קשר ישיר קלט-פלט; בלמידה בלתי-מונחית אין תוויות והמודל מחפש מבנים סמויים בדאטה.",
    "explanation": "ההבדל הוא קיומן של תוויות: בפיקוח יש יעד מפורש, בלא-פיקוח המודל רק מגלה קבוצות או התפלגויות בעצמו."
  },
  {
    "type": "open",
    "question": "מהי מטרת רכיב Reconstruction Loss ב-VAE, וכיצד הוא מאוזן מול KL Loss?",
    "correctAnswerText": "Reconstruction Loss דואג שה-decoder ישחזר קלט באיכות גבוהה; הוא מאוזן מול KL Divergence כדי למנוע מרחב לטנטי בלתי-רציף אך גם להימנע מטשטוש יתר.",
    "explanation": "מתן משקל יתר לשחזור פוגע ברציפות הלטנטי, ומשקל יתר ל-KL פוגע באיכות הפלט; נדרש איזון."
  },
  {
    "type": "open",
    "question": "תאר את עקרון הפעולה של Discriminator ב-GAN.",
    "correctAnswerText": "המפלה מקבל גם דוגמאות אמיתיות מדאטה וגם דוגמאות שיוצרו בידי הגנרטור, ומפיק הסתברות “אמיתי/מזויף”. הוא מתעדכן כך שהסתברותו תהיה נכונה ככל האפשר.",
    "explanation": "ככל שהמפלה משתפר, הגנרטור חייב לייצר דוגמאות מציאותיות יותר כדי להטעותו, מה שמניע את שני הצדדים."
  },
  {
    "type": "open",
    "question": "למה Feature Descriptors היסטוריים כמו HOG היו חשובים לפני עליית CNN?",
    "correctAnswerText": "הם סיפקו ייצוג קומפקטי וברור של קצוות וכיווני גרדיאנט, כך שמסווגים קלאסיים יכלו לעבוד ביעילות גם בלי רשתות עמוקות.",
    "explanation": "ללא למידה עמוקה, תהליך חילוץ ידני של מאפיינים היה חיוני להפחתת ממד ולדיוק זיהוי."
  },
  {
    "type": "open",
    "question": "מהם שלושת השלבים העיקריים בתהליך CNN כפי שתואר בהרצאה?",
    "correctAnswerText": "קלט תמונה → חילוץ תכונות (Convolution + Activation + Pooling) → סיווג סופי (Fully Connected + Softmax).",
    "explanation": "שכבות הקונבולוציה בונות תכונות, pooling מצמצם ממד, Fully Connected ממפה למרחב המחלקות."
  },
  {
    "type": "open",
    "question": "הסבר בקצרה מהו Mode Collapse ואיך ניתן להפחיתו.",
    "correctAnswerText": "Mode Collapse מתרחש כש-generator מייצר מספר מוגבל של דוגמאות חוזרות; ניתן להפחיתו בשיטות כמו minibatch discrimination, שינוי קצב הלמידה או שימוש ב-WGAN.",
    "explanation": "גיוון בקלט ובאסטרטגיות הפסד מאלץ את הגנרטור לחקור אזורים נוספים בהתפלגות."
  },
  {
    "type": "open",
    "question": "כיצד DCGAN משפר את האימון יחסית ל-GAN בסיסי?",
    "correctAnswerText": "על-ידי שימוש בשכבות קונבולוציה עמוקות, Batch Norm, ופונקציות ReLU/LeakyReLU מתאימות, DCGAN מייצב גרדיאנטים ומפיק תמונות באיכות גבוהה יותר.",
    "explanation": "המבנה הקונבולוציוני מנצל את טבע התמונה, והנרמול מונע פיצוץ או דעיכת גרדיאנטים."
  },
  {
    "type": "open",
    "question": "מדוע נדרש Reparameterization Trick כדי לאפשר Back-Propagation דרך שלב הדגימה?",
    "correctAnswerText": "דגימה אקראית אינה גזירה; על-ידי פירוק z ל-μ, σ ו-ε נפרד, הפונקציה הופכת להפרדה ליניארית בגורם גזיר, ומאפשרת חישוב גרדיאנטים ל-μ, σ.",
    "explanation": "הגרדיאנט זורם דרך פעולות אלגבריות אך לא דרך פעולה לא-דטרמיניסטית; הפירוק עוקף מגבלה זו."
  },
  {
    "type": "open",
    "question": "תן דוגמה לשימוש דומיין-ספציפי במודלים גנרטיביים מעבר לאומנות.",
    "correctAnswerText": "יצירת דאטה רפואי סינתטי לשמירה על פרטיות החולים תוך אימון מודלים לאבחון.",
    "explanation": "כך ניתן להעשיר מאגרי MRI/CT מבלי לחשוף פרטים מזהים, ועדיין ללמד רשתות."
  },
  {
    "type": "open",
    "question": "כיצד Softmax מאפשר סיווג רב-מחלקתי?",
    "correctAnswerText": "הוא מנרמל את כל הלוגיטים לסכום 1, ולכן כל ערך אפשר לפרש כהסתברות למחלקה.",
    "explanation": "המחלקה עם ההסתברות הגבוהה ביותר נבחרת; הפלט רציף ומאפשר אובדן קרוס-אנטרופי."
  },
  {
    "type": "open",
    "question": "מה תפקיד Dropout באימון רשת עמוקה?",
    "correctAnswerText": "כיבוי אקראי של נוירונים מפחית תלות-יתר במאפיין יחיד ומשפר יכולת הכללה.",
    "explanation": "בזמן בדיקה כל הנוירונים פעילים והמשקולות מקונסקלים, מה שממוצע התנהגות רשתות משנה רבות."
  },
  {
    "type": "open",
    "question": "למה ReLU מועדפת על Sigmoid ברשתות עמוקות?",
    "correctAnswerText": "ReLU אינה רוויה עבור ערכים חיוביים, ולכן גרדיאנט נשמר ואינו “נכחד” כמו ב-Sigmoid.",
    "explanation": "ב-Sigmoid, גרדיאנטים קטנים באזורי רוויה, מה שמאט או עוצר למידה בשכבות מוקדמות."
  },
  {
    "type": "mc",
    "question": "איזו תכונה מעניקה שכבת קונבולוציה באמצעות parameter sharing?",
    "options": [
      "הפחתת רעש אקראי בכל ערוץ קלט",
      "חסכון מהותי במספר המשקולות ללא פגיעה בכיסוי המרחבי",
      "התכנסות מהירה יותר בזכות שימוש ב-stride משתנה",
      "נירמול אוטומטי של פלט ה-feature map לטווח ‎(-1, 1)‎"
    ],
    "correctAnswerIndex": 1,
    "explanation": "אותו גרעין מוחל על כל מיקום ולכן כמות הפרמטרים קטנה אך הכיסוי נשמר – זה הבסיס ל-translation invariance."
  },
  {
    "type": "mc",
    "question": "מהו התפקיד המרכזי של ‎zero-padding‎ בשכבת קונבולוציה?",
    "options": [
      "שמירה על גודל ה-feature map כדי לא לאבד מידע שוליים",
      "אכיפת חציצה בין ערוצי צבע שונים",
      "הפחתת מורכבות חישובית בזמן בדיקה",
      "יצירת חיווי דליל עבור pooling עתידי"
    ],
    "correctAnswerIndex": 0,
    "explanation": "מילוי האפסים מאפשר לגרעין “לבקר” גם על הקצה כך שממד היציאה נותר שווה לממד הקלט."
  },
  {
    "type": "mc",
    "question": "כיצד stride > 1 משפיע על פלט שכבת קונבולוציה?",
    "options": [
      "מקטין את הרזולוציה על-ידי דילוג על מיקומים",
      "מעלה את מספר הערוצים לפני pooling",
      "מבטל צורך ב-ReLU בשכבה הבאה",
      "מאריך את זמן האימון כי יש יותר קונבולוציות"
    ],
    "correctAnswerIndex": 0,
    "explanation": "דילוגי-גרעין מדלגים על חלק מהפיקסלים ולכן מתקבל down-sampling מובנה."
  },
  {
    "type": "mc",
    "question": "ל-Max Pooling יש יתרון על Average Pooling בעיקר ב-…",
    "options": [
      "שימור צבעים נייטרליים בתמונות טבע",
      "שמירת הקצוות והתכונות הדומיננטיות החזותיות",
      "צמצום דרסטי-יותר של ממדים במשוואות ליניאריות",
      "לימוד מהיר יותר של שכבת Softmax"
    ],
    "correctAnswerIndex": 1,
    "explanation": "ערך-המקסימום מעביר את האות החזק ביותר בכל חלון, ולכן קצוות בולטים נשמרים טוב יותר."
  },
  {
    "type": "mc",
    "question": "מדוע נדרש שלב flatten לפני מעבר ל-Fully Connected?",
    "options": [
      "לאפשר שימוש ב-Batch Norm בשכבה האחרונה",
      "המרת המפה התלת-ממדית לווקטור 1-D אחיד עבור נוירוני הסיווג",
      "שיפור גרדיאנטים בשכבת ההפעלה",
      "אחסון יעיל יותר בזיכרון ה-GPU"
    ],
    "correctAnswerIndex": 1,
    "explanation": "שכבות צפופות מצפות לווקטור; שימור המבנה המרחבי כבר אינו נדרש בשלב הסיווג."
  },
  {
    "type": "mc",
    "question": "מהו translation invariance ברשת קונבולוציה?",
    "options": [
      "יכולת לזהות את אותו דפוס גם אם זז במרחב התמונה",
      "התאמה ל-augmentation צבעוני",
      "הפחתת כמות הפילטרים בפולינג",
      "אילוץ שה-Softmax לא יושפע מרוטציה"
    ],
    "correctAnswerIndex": 0,
    "explanation": "גרעינים נודדים בכל מיקום, ולכן מאפיינים דומים יופעלו גם אם האובייקט מוסת."
  },
  {
    "type": "mc",
    "question": "בלמידה מונחית, train / validation split הכרחי כדי…",
    "options": [
      "לצמצם את גודל ה-dataset ולזרז חישוב",
      "להעריך הכללה על דגימות שלא נראו בזמן האימון",
      "לייצר איזון מספרי בין מחלקות",
      "לבדוק שעוצמת הרעש נותרת קבועה"
    ],
    "correctAnswerIndex": 1,
    "explanation": "סט validation מייצג \"עתיד\" לא-ידוע ולכן משקף האם המודל עבר התאמה-יתר."
  },
  {
    "type": "mc",
    "question": "איזו בעיה מרכזית עלולה להופיע אם ‎learning rate‎ גבוה מדי?",
    "options": [
      "עדכוני משקולות יקפצו מעל מינימום ויגרמו לאי-התכנסות",
      "הרשת עלולה להתכנס מהר מדי לנקודת אוכף",
      "Mode Collapse יופיע גם ברשת מפלה",
      "ה-ReLU יתחיל לרווץ לערכים שליליים"
    ],
    "correctAnswerIndex": 0,
    "explanation": "צעד גדול מדי ב-SGD גורם לאוסילציות סביב אזור המינימום ואף לבריחה החוצה."
  },
  {
    "type": "mc",
    "question": "בהקשר AutoEncoder, “חורי” מרחב לטנטי פירושם…",
    "options": [
      "דגימות שעברו dropout פנימי",
      "אזורים שלא נלמדו ולכן מפיקים פלט לא-מציאותי",
      "ערוצי latent שהוצאו בשל נירמול",
      "שימוש ב-stride משתנה בין שכבות"
    ],
    "correctAnswerIndex": 1,
    "explanation": "מאחר שה-encoder ראה רק נקודות מסוימות, דגימה ביניהן מניבה שחזור דל-איכות."
  },
  {
    "type": "mc",
    "question": "ב-β-VAE מעלה המקדם β את KL Divergence כדי…",
    "options": [
      "להעדיף מרחב לטנטי מובנה ונקי על פני דיוק שחזור גס",
      "להקטין את גודל ה-batch הדרוש",
      "לשפר בהירות צבע בפלט ה-decoder",
      "להאיץ גרדיאנטים ב-generator"
    ],
    "correctAnswerIndex": 0,
    "explanation": "β > 1 מגדיל את המשקל על סדירות ה-latent ומוביל ל-disentanglement."
  },
  {
    "type": "mc",
    "question": "מהו latent space arithmetic?",
    "options": [
      "שימוש בפרמטרים מורחבים לגנרטור",
      "חיבור/חיסור וקטורים לטנטיים לקבלת שינוי תוכן ברור",
      "השוואת תווי-פנים בפולינג",
      "נירמול משקלות ב-Batch Norm"
    ],
    "correctAnswerIndex": 1,
    "explanation": "למשל “חיוך” = z(פנים-מחייכות) − z(פנים-ניטרליות); הוספת הווקטור יוצרת חיוך בדוגמאות חדשות."
  },
  {
    "type": "mc",
    "question": "ב-GAN, label smoothing למפלה נועד ל-…",
    "options": [
      "להפחית זיכרון GPU הדרוש לאימון",
      "למנוע מהמפלח להיות \"בטוח מדי\" ובכך להחליש גרדיאנטים",
      "להגביר contrast בפלט הגנרטור",
      "לשנות את גודל רעש הקלט"
    ],
    "correctAnswerIndex": 1,
    "explanation": "קירוב התווית 1 ל-0.9 ושל 0 ל-0.1 שומר על גרדיאנטים יציבים ומפחית over-confidence."
  },
  {
    "type": "mc",
    "question": "מה מודד Inception Score (IS) בניגוד ל-FID?",
    "options": [
      "סטיית-תקן של צבעי-רקע",
      "שונות-התמונות מול איכות-הקונפידנס של Inception Net",
      "מרחק פרמאטרי בין גאוסיאנים ב-feature space",
      "שיעור Mode Collapse בפרק זמן נתון"
    ],
    "correctAnswerIndex": 1,
    "explanation": "IS גבוה מצביע על פלט מגוון (שונות בין-תמונות) ובעל תוויות נחרצות (אמון)."
  },
  {
    "type": "mc",
    "question": "ל-LeakyReLU יש יתרון על ReLU בכך ש-…",
    "options": [
      "מפחית את כמות הזיכרון לשמירת אקטיבציות",
      "מאפשר מעבר גרדיאנט גם עבור ערכים שליליים קטנים",
      "מבטל צורך ב-Batch Norm",
      "מאריך את תחום הקלט לפני saturation"
    ],
    "correctAnswerIndex": 1,
    "explanation": "δ·x כאשר x<0 מונע \"מוות\" נוירונים ומקל על התכנסות ברשת עמוקה."
  },
  {
    "type": "mc",
    "question": "איזה פתרון נחשב יעיל נגד vanishing gradients ברשתות עמוקות?",
    "options": [
      "הוספת קישורי קיצור – Residual Connections",
      "העלאת ה-dropout ל-70 %",
      "מעבר ל-tanh בכל שכבה",
      "שימוש בקצב-למידה משתנה אקספוננציאלית"
    ],
    "correctAnswerIndex": 0,
    "explanation": "חיבור ישיר בין שכבות מאפשר מעבר מידע וגרדיאנט ללא דעיכה."
  },
  {
    "type": "mc",
    "question": "ב-GAN קלאסי, \"משחק סכום-אפס\" משמעו שהמטרה הכוללת…",
    "options": [
      "L_G + L_D = 1 תמיד",
      "L_G = −L_D בדיוק",
      "שיפור אחד פוגע בשני כך שסכום הרווחים קבוע",
      "אין שינוי אחרי איטרציית אימון אחת"
    ],
    "correctAnswerIndex": 2,
    "explanation": "זהו ניסוח ה-min-max: הגנרטור ממזער, המפלה ממקסם אותה פונקציה."
  },
  {
    "type": "mc",
    "question": "מהו עקרון minibatch discrimination להפחתת Mode Collapse?",
    "options": [
      "שילוב dropout נוסף רק לגנרטור",
      "הוספת מידע על שונות בתוך המיני-batch בשכבת המפלה",
      "חיבור רשת קטנה מסייעת לאוגמנטציה",
      "מעבר לקצב-למידה קבוע"
    ],
    "correctAnswerIndex": 1,
    "explanation": "המפלה \"מבין\" האם דוגמאות מגוונות ולכן מעניש גנרטור חוזרני."
  },
  {
    "type": "mc",
    "question": "ב-DCGAN ממליצים על “progressive stacking” כדי…",
    "options": [
      "לבנות את התמונה בהדרגה מרזולוציה נמוכה לגבוהה",
      "לצמצם פורמטים של קובצי-קלט",
      "לנרמל משקולות לאחר כל אימון",
      "לצמיד ReLU ל-Softplus"
    ],
    "correctAnswerIndex": 0,
    "explanation": "גישה זו מספקת יציבות ומפחיתה עומס גרדיאנטים בתחילת האימון."
  },
  {
    "type": "mc",
    "question": "מדוע Batch Norm אחרי קונבולוציה אך לפני ReLU?",
    "options": [
      "מנקה רעש GPU",
      "מנרמלת את הפלט תחילה כדי ש-ReLU לא יחתוך טווחים חריגים",
      "מגדילה את ממד ה-feature map",
      "מאלצת ReLU לעבוד בתחום ‎(-1, 1)‎"
    ],
    "correctAnswerIndex": 1,
    "explanation": "נירמול •→ שיפוע יציב; הפעלה לאחר מכן מונעת קיטום ערכים חריגים בלתי-מנורמלים."
  },
  {
    "type": "mc",
    "question": "איזו טקטיקה מונעת discriminator overfitting?",
    "options": [
      "העלאת עומק הגנרטור",
      "עדכון המפלה בתדירות נמוכה יותר מהגנרטור",
      "פונקציית הפסד L1 במקום BCE",
      "הגדלת גודל ה-latent vector מ-100 ל-1000"
    ],
    "correctAnswerIndex": 1,
    "explanation": "אם המפלה משתפר מהר מדי, הגנרטור לא מקבל פידבק משמעותי; האטת עדכוניו מאזנת את המשחק."
  },
  {
    "type": "mc",
    "question": "מהי תרומת drop-connect לעומת dropout?",
    "options": [
      "השמטת משקולות במקום נוירונים",
      "חיסכון גדול יותר בפרמטרים תוך שמירה על מבנה הרשת",
      "שימוש ב-tanh קבוע בשכבות",
      "התאמת רעש לטנט ראשוני"
    ],
    "correctAnswerIndex": 1,
    "explanation": "כיבוי קשתות (משקולות) יוצר תתי-רשתות רבות ומצמצם פרמטרים בלי לפגוע במבנה הכללי."
  },
  {
    "type": "mc",
    "question": "Early Stopping עוצר אימון כאשר…",
    "options": [
      "מדד זמן-אמת אינו משתפר על סט validation לאורך K איטרציות",
      "ה-loss ב-train יורד מתחת לאחוז אחד",
      "ה-GAN מגיע לערך ‎FID < 50‎",
      "ReLU ברשת מתחיל להחזיר אפס תמיד"
    ],
    "correctAnswerIndex": 0,
    "explanation": "חוסר-שיפור מצביע על סכנת התאמה-יתר; עצירה חוסכת משאבים ומשמרת מודל כללי."
  },
  {
    "type": "open",
    "question": "הסבר בקצרה כיצד parameter sharing יוצר חיסכון חישובי ברשתות קונבולוציה.",
    "correctAnswerText": "גרעין יחיד מחליף אלפי משקולות נפרדות משום שהוא מוזז על פני כל מיקום קלט; כך מספר הפרמטרים תלוי בגודל הגרעין ולא בגודל התמונה.",
    "explanation": "חיסכון זה מאפשר רשת עמוקה עם מיליוני פיקסלים אך אלפי פרמטרים בלבד בשכבות הראשונות."
  },
  {
    "type": "open",
    "question": "כיצד Zero-Padding תורם ל-translation invariance של CNN?",
    "correctAnswerText": "הוא מאפשר לגרעין ללמוד תכונות גם בגבולות כך שהאובייקט יכול לנוע לשוליים בלי “להיעלם” בממד ה-feature.",
    "explanation": "בלעדיו, הקצה עובר חיתוך ולכן הרשת הייתה רגישה למיקום מוקצין."
  },
  {
    "type": "open",
    "question": "מה ההבדל המרכזי בין Underfitting ל-Overfitting? הבא דוגמה מספרית.",
    "correctAnswerText": "Underfitting – ה-train loss גבוה; Overfitting – train loss נמוך ו-val loss גבוה. לדוגמה, L_train = 0.5 ו-L_val = 0.48 מצביע על Underfitting; L_train = 0.02 ו-L_val = 0.4 על Overfitting.",
    "explanation": "המדד הוא פער ביצועים בין קבוצות הדאטה."
  },
  {
    "type": "open",
    "question": "פרט את שני מרכיבי הפסד ב-VAE והסבר איזון משוקלל ביניהם.",
    "correctAnswerText": "Reconstruction Loss מודד דיוק לשחזר את הקלט; KL Divergence מיישר את התפלגות ה-latent לנורמלית. משקל λ: קטן מדי → יצירה חדה עם \"חורים\"; גדול מדי → פלט מטושטש אך מרחב רציף.",
    "explanation": "האיזון קובע איכות מול רגולריות במרחב הלטנטי."
  },
  {
    "type": "open",
    "question": "למה ReLU מונע vanishing gradient טוב יותר מ-Sigmoid?",
    "correctAnswerText": "ReLU גוזר לשיפוע קבוע 1 לערכים חיוביים, בעוד Sigmoid מתקרבת ל-0 בשני קצותיה ולכן השיפועים קטנים ומתאפסים לאורך שכבות רבות.",
    "explanation": "שיפוע קבוע מעביר מידע לאחור גם ברשת עמוקה."
  },
  {
    "type": "open",
    "question": "סכם יתרון וחיסרון מרכזיים של Batch Normalization.",
    "correctAnswerText": "יתרון – נירמול אקטיבציות מייצב התכנסות ומאפשר קצבי-למידה גבוהים; חסרון – תלות ב-batch size ועלות חישובית נוספת וכן מורכבות בזמן-ריצה.",
    "explanation": "יעיל בשלב האימון אך דורש סטטיסטיקה קבועה בזמן ההסקה."
  },
  {
    "type": "open",
    "question": "כיצד progressive growing ב-Generator מפחית Mode Collapse?",
    "correctAnswerText": "הגנרטור לומד קודם מבנה כללי ברזולוציה נמוכה ואז מוסיף פרטים; כך נוצר דגש על שונות גלובלית לפני פרטים קטנים, מה שמונע התמחות מוקדמת בדגימות מעטות.",
    "explanation": "הגדלה הדרגתית של הרזולוציה מאפשרת חיפוש מרחב לטנט רחב בשלב מוקדם."
  },
  {
    "type": "open",
    "question": "הסבר בקצרה את הרעיון של latent disentanglement.",
    "correctAnswerText": "כל ממד ב-z אחראי לתכונה בודדת (למשל סיבוב ראש); שינויו מניב שינוי צפוי בלי להשפיע על תכונות אחרות.",
    "explanation": "מאפשר בקרה יצירתית והעברת-סגנון נשלטת."
  },
  {
    "type": "open",
    "question": "מה תפקיד פונקציית tanh כפלט גנרטור DCGAN?",
    "correctAnswerText": "היא כופפת את ערכי הפיקסל לטווח ‎(-1, 1)‎ התואם לנתוני-אימון המנורמלים ומונעת רוויה קשה בהעברת הגרדיאנט.",
    "explanation": "התאמה לטווח הכרחית ל-Batch Norm ול-discriminator שמצפה קלט אחיד."
  },
  {
    "type": "open",
    "question": "תאר בקצרה כיצד FID מחשב \"מרחק\" בין שתי התפלגויות.",
    "correctAnswerText": "FID מעביר תמונות דרך Inception-V3, מניח כי מאפייני-feature מתפלגים נורמלית ומשווה את הממוצעים והמטריצות-הקוואריוניות בנוסחת Fréchet.",
    "explanation": "ערך נמוך מצביע על חפיפה גבוהה בין אמיתי וסינתטי."
  },
  {
    "type": "open",
    "question": "מדוע label flipping (החלפת תוויות מדי-פעם) עשוי לייצב אימון GAN?",
    "correctAnswerText": "בלבול זמני של המפלה מונע ממנו להתייצב מהר מדי ומספק גרדיאנטים משמעותיים לגנרטור לאורך זמן.",
    "explanation": "\"פיצוח אמון\" מחושב שנועד לשמר תחרות הוגנת."
  }
]

