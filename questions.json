[
  {
    "type": "mc",
    "question": "מהי המטרה העיקרית של שכבת הקונבולוציה ב-CNN?",
    "options": [
      "לצמצם ממדים על-ידי בחירת ערך בודד מכל חלון קטן",
      "לחלץ תכונות מקומיות בעזרת גרעינים קבועי-גודל המחליקים על התמונה",
      "למפות את הפלט להסתברויות סופיות עבור כל מחלקה אפשרית",
      "לאפס ערכים שליליים כדי לשמור מידע חיובי בלבד"
    ],
    "correctAnswerIndex": 1,
    "explanation": "גרעיני הקונבולוציה מבצעים מכפלה קונבולוציונית בחלונות מקומיים וכך יוצרים feature maps המציגות קצוות, טקסטורות וצורות."
  },
  {
    "type": "mc",
    "question": "באיזו שכבה ב-CNN מצמצמים את ממדי הנתונים תוך שמירה על תכונות חשובות?",
    "options": [
      "שכבת קונבולוציה עם stride גדול במיוחד",
      "שכבת הפעלה Sigmoid לא-לינארית",
      "שכבת Pooling המחלקת את המפה לחלונות ומחלצת ערך נציג",
      "שכבת Fully Connected עם כמות נוירונים מוגבלת"
    ],
    "correctAnswerIndex": 2,
    "explanation": "בשכבת Pooling מבוצע down-sampling (למשל Max או Average Pooling) ולכן המפה קטנה אך שומרת מידע קריטי."
  },
  {
    "type": "open",
    "question": "הסבר בקצרה כיצד Supervised Learning שונה מ-Unsupervised Learning.",
    "correctAnswerText": "בלמידה מונחית המודל מתאמן על נתונים עם תוויות ידועות מראש ומלמד קשר ישיר קלט-פלט; בלמידה בלתי-מונחית אין תוויות והמודל מחפש מבנים סמויים בדאטה.",
    "explanation": "ההבדל הוא קיומן של תוויות: בפיקוח יש יעד מפורש, בלא-פיקוח המודל רק מגלה קבוצות או התפלגויות בעצמו."
  },
  {
    "type": "open",
    "question": "מהי מטרת רכיב Reconstruction Loss ב-VAE, וכיצד הוא מאוזן מול KL Loss?",
    "correctAnswerText": "Reconstruction Loss דואג שה-decoder ישחזר קלט באיכות גבוהה; הוא מאוזן מול KL Divergence כדי למנוע מרחב לטנטי בלתי-רציף אך גם להימנע מטשטוש יתר.",
    "explanation": "מתן משקל יתר לשחזור פוגע ברציפות הלטנטי, ומשקל יתר ל-KL פוגע באיכות הפלט; נדרש איזון."
  },
  {
    "type": "open",
    "question": "תאר את עקרון הפעולה של Discriminator ב-GAN.",
    "correctAnswerText": "המפלה מקבל גם דוגמאות אמיתיות מדאטה וגם דוגמאות שיוצרו בידי הגנרטור, ומפיק הסתברות “אמיתי/מזויף”. הוא מתעדכן כך שהסתברותו תהיה נכונה ככל האפשר.",
    "explanation": "ככל שהמפלה משתפר, הגנרטור חייב לייצר דוגמאות מציאותיות יותר כדי להטעותו, מה שמניע את שני הצדדים."
  },
  {
    "type": "open",
    "question": "מהם שלושת השלבים העיקריים בתהליך CNN כפי שתואר בהרצאה?",
    "correctAnswerText": "קלט תמונה → חילוץ תכונות (Convolution + Activation + Pooling) → סיווג סופי (Fully Connected + Softmax).",
    "explanation": "שכבות הקונבולוציה בונות תכונות, pooling מצמצם ממד, Fully Connected ממפה למרחב המחלקות."
  },
  {
    "type": "open",
    "question": "כיצד DCGAN משפר את האימון יחסית ל-GAN בסיסי?",
    "correctAnswerText": "על-ידי שימוש בשכבות קונבולוציה עמוקות, Batch Norm, ופונקציות ReLU/LeakyReLU מתאימות, DCGAN מייצב גרדיאנטים ומפיק תמונות באיכות גבוהה יותר.",
    "explanation": "המבנה הקונבולוציוני מנצל את טבע התמונה, והנרמול מונע פיצוץ או דעיכת גרדיאנטים."
  },
  {
    "type": "mc",
    "question": "מהו תפקיד פונקציית ReLU בהקשר של CNN?",
    "options": [
      "לנרמל את פלטי השכבות לטווח (0, 1) בדיוק",
      "להחליף ערכים חיוביים בערכים שליליים כדי להדגיש ניגוד",
      "להכניס אי-לינאריות על-ידי איפוס ערכים שליליים ושמירת החיוביים",
      "לבצע דגימה אקראית המונעת התאמה-יתרה בזמן האימון"
    ],
    "correctAnswerIndex": 2,
    "explanation": "ReLU (x → max(0,x)) משאיר רק ערכים חיוביים, שובר ליניאריות ומפחית סכנת vanishing gradients."
  }
]
